{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Practical 4: Parallel and Distributed Computing Fundamentals\n",
    "\n",
    "## Goals\n",
    "\n",
    "### Understanding Parallel Computing Paradigms\n",
    "- Sequential vs parallel execution\n",
    "- Concurrency vs parallelism\n",
    "- Amdahl's Law and speedup calculations\n",
    "\n",
    "### Mastering Python's Functional Programming\n",
    "- `filter()`, `map()`, `reduce()` with lambda expressions\n",
    "- Generators and iterators for memory-efficient processing\n",
    "\n",
    "### Implementing Parallel Processing\n",
    "- `multiprocessing` module (Process, Pool, Queue, Pipe)\n",
    "- `concurrent.futures` (ThreadPoolExecutor, ProcessPoolExecutor)\n",
    "- Shared memory and synchronization primitives\n",
    "\n",
    "### Performance Measurement and Optimization\n",
    "- Benchmarking parallel code\n",
    "- Identifying bottlenecks\n",
    "- Chunking strategies\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completion of Practicals 0-3\n",
    "- Understanding of Python functions and data structures\n",
    "- Basic knowledge of computer architecture (CPU cores)\n",
    "\n",
    "## Exercises Overview\n",
    "\n",
    "| Exercise | Difficulty | Topics |\n",
    "|----------|------------|--------|\n",
    "| Exercise 1 | ★ | Functional programming: filter(), map(), reduce() |\n",
    "| Exercise 2 | ★ | Lambda expressions and higher-order functions |\n",
    "| Exercise 3 | ★★ | Generators and iterators for large data |\n",
    "| Exercise 4 | ★★ | Introduction to multiprocessing |\n",
    "| Exercise 5 | ★★ | Process communication: Queues and Pipes |\n",
    "| Exercise 6 | ★★★ | concurrent.futures and async patterns |\n",
    "| Exercise 7 | ★★★ | Performance benchmarking and optimization |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Exercise 1 [★] - Functional Programming: filter(), map(), reduce()\n",
    "\n",
    "In this exercise, we explore functional programming concepts that form the foundation of parallel and distributed data processing. These patterns are used extensively in frameworks like Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "### 1. Introduction to Functional Programming\n",
    "\n",
    "Functional programming emphasizes:\n",
    "- **Pure functions**: Functions that always produce the same output for the same input and have no side effects\n",
    "- **Immutability**: Data is not modified after creation\n",
    "- **First-class functions**: Functions can be passed as arguments and returned from other functions\n",
    "\n",
    "These principles are essential for parallel processing because they eliminate shared state and make it safe to execute operations concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### 2. The filter() Function\n",
    "\n",
    "The `filter(function, iterable)` function returns an iterator containing elements from the iterable for which the function returns `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "?filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic filter example: filter even numbers\n",
    "num = [i for i in range(1, 20)]\n",
    "print(\"Original list:\", num)\n",
    "\n",
    "def is_even(item):\n",
    "    return item % 2 == 0\n",
    "\n",
    "filtered = list(filter(is_even, num))\n",
    "print(\"Even numbers:\", filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter with None as function - removes falsy values\n",
    "mixed = [0, 1, \"\", \"hello\", None, 42, [], [1, 2]]\n",
    "print(\"Original:\", mixed)\n",
    "print(\"Filtered (truthy only):\", list(filter(None, mixed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter odd numbers\n",
    "def is_odd(item):\n",
    "    return item % 2 != 0\n",
    "\n",
    "num = [i for i in range(1, 20)]\n",
    "filtered = list(filter(is_odd, num))\n",
    "print(\"Odd numbers:\", filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "**Question 1.1**: Write a function `is_prime(n)` that returns `True` if `n` is a prime number. Then use `filter()` to extract all prime numbers from a list of 1000 random integers between 1 and 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate 1000 random integers\n",
    "random_numbers = [random.randint(1, 1000) for _ in range(1000)]\n",
    "\n",
    "# TODO: Implement is_prime function and filter prime numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "**Question 1.2**: Use `filter()` to extract numbers divisible by both 3 and 5 from the same list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Filter numbers divisible by both 3 and 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "#### Filtering with Nested Structures\n",
    "\n",
    "The `filter()` function can be applied to complex data structures like lists of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [\n",
    "    {\"name\": \"Alice\", \"age\": 28, \"department\": \"HR\", \"salary\": 55000},\n",
    "    {\"name\": \"Bob\", \"age\": 35, \"department\": \"Engineering\", \"salary\": 85000},\n",
    "    {\"name\": \"Charlie\", \"age\": 22, \"department\": \"Marketing\", \"salary\": 45000},\n",
    "    {\"name\": \"David\", \"age\": 45, \"department\": \"Engineering\", \"salary\": 95000},\n",
    "    {\"name\": \"Eve\", \"age\": 31, \"department\": \"HR\", \"salary\": 60000},\n",
    "    {\"name\": \"Frank\", \"age\": 29, \"department\": \"Engineering\", \"salary\": 78000},\n",
    "    {\"name\": \"Grace\", \"age\": 38, \"department\": \"Marketing\", \"salary\": 72000},\n",
    "    {\"name\": \"Henry\", \"age\": 42, \"department\": \"Engineering\", \"salary\": 92000}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "**Question 1.3**: Using `filter()`, complete the following tasks:\n",
    "1. Create a list of employees who work in the \"Engineering\" department\n",
    "2. Find employees whose age is between 25 and 40 (inclusive)\n",
    "3. Find employees with salary above 70000\n",
    "4. Find employees whose name starts with a vowel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "#### Advanced String Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Data science is transforming industries worldwide.\",\n",
    "    \"Python is great.\",\n",
    "    \"Machine learning algorithms require massive amounts of data.\",\n",
    "    \"Big data technologies enable processing of petabytes.\",\n",
    "    \"AI is evolving.\",\n",
    "    \"Distributed computing allows horizontal scaling across clusters.\",\n",
    "    \"The cloud provides elastic computing resources on demand.\",\n",
    "    \"Simple works.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "**Question 1.4**: Using `filter()`:\n",
    "1. Select sentences with more than 6 words\n",
    "2. Select sentences containing the word \"data\" (case-insensitive)\n",
    "3. Select sentences where the average word length is greater than 5 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### 3. The map() Function\n",
    "\n",
    "The `map(function, iterable, ...)` function applies a function to every item in an iterable and returns an iterator of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "?map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic map example: square each number\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "num = [i for i in range(1, 11)]\n",
    "squared = list(map(square, num))\n",
    "print(\"Original:\", num)\n",
    "print(\"Squared:\", squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map with multiple iterables\n",
    "def multiply(x, y):\n",
    "    return x * y\n",
    "\n",
    "list1 = [1, 2, 3, 4, 5]\n",
    "list2 = [10, 20, 30, 40, 50]\n",
    "\n",
    "products = list(map(multiply, list1, list2))\n",
    "print(\"Products:\", products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map with three iterables\n",
    "def weighted_sum(a, b, c):\n",
    "    return a + 2*b + 3*c\n",
    "\n",
    "x = [1, 2, 3]\n",
    "y = [4, 5, 6]\n",
    "z = [7, 8, 9]\n",
    "\n",
    "result = list(map(weighted_sum, x, y, z))\n",
    "print(\"Weighted sums:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "**Question 1.5**: Given a list of 100,000 file paths, use `map()` to:\n",
    "1. Extract file extensions (e.g., `.txt`, `.csv`)\n",
    "2. Extract file names without extensions\n",
    "3. Calculate path depths (number of directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Generate sample file paths\n",
    "extensions = ['.txt', '.csv', '.json', '.xml', '.py', '.md']\n",
    "directories = ['data', 'src', 'docs', 'config', 'tests', 'output']\n",
    "\n",
    "file_paths = [\n",
    "    os.path.join(\n",
    "        random.choice(directories),\n",
    "        random.choice(directories),\n",
    "        f\"file_{i}{random.choice(extensions)}\"\n",
    "    )\n",
    "    for i in range(100000)\n",
    "]\n",
    "\n",
    "print(\"Sample paths:\", file_paths[:5])\n",
    "\n",
    "# TODO: Use map() to extract extensions, names, and calculate depths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "**Question 1.6**: Use `map()` to normalize a dataset of 50,000 text strings:\n",
    "- Convert to lowercase\n",
    "- Strip leading/trailing whitespace\n",
    "- Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Generate sample text data\n",
    "sample_words = [\"Hello\", \"World\", \"Data\", \"Science\", \"Python\", \"Analysis\"]\n",
    "punctuation_marks = list(\"!.,;:?\")\n",
    "\n",
    "text_data = [\n",
    "    f\"  {random.choice(sample_words)}{random.choice(punctuation_marks)}  \".upper()\n",
    "    if random.random() > 0.5 else\n",
    "    f\"{random.choice(sample_words)} {random.choice(sample_words)}{random.choice(punctuation_marks)}\"\n",
    "    for _ in range(50000)\n",
    "]\n",
    "\n",
    "print(\"Sample data:\", text_data[:5])\n",
    "\n",
    "# TODO: Use map() to normalize the text data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "### 4. The reduce() Function\n",
    "\n",
    "The `reduce(function, iterable)` function applies a function of two arguments cumulatively to the items of an iterable, from left to right, reducing it to a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "?reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of numbers using reduce\n",
    "from functools import reduce\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "num = [i for i in range(1, 11)]\n",
    "total = reduce(add, num)\n",
    "print(f\"Sum of {num} = {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product of numbers\n",
    "def multiply(x, y):\n",
    "    return x * y\n",
    "\n",
    "num = [1, 2, 3, 4, 5]\n",
    "product = reduce(multiply, num)\n",
    "print(f\"Product of {num} = {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding maximum using reduce\n",
    "def max_of_two(x, y):\n",
    "    return x if x > y else y\n",
    "\n",
    "num = [45, 12, 89, 34, 67, 23, 90, 11]\n",
    "maximum = reduce(max_of_two, num)\n",
    "print(f\"Maximum of {num} = {maximum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "**Question 1.7**: Use `reduce()` to:\n",
    "1. Find the longest string in a list of 10,000 strings\n",
    "2. Flatten a nested list (3 levels deep)\n",
    "3. Concatenate a list of strings with a separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "words = [\"algorithm\", \"data\", \"processing\", \"distributed\", \"computing\", \n",
    "         \"parallel\", \"optimization\", \"scalability\", \"performance\"]\n",
    "\n",
    "string_list = [random.choice(words) * random.randint(1, 5) for _ in range(10000)]\n",
    "\n",
    "nested_list = [\n",
    "    [[1, 2], [3, 4]],\n",
    "    [[5, 6], [7, 8]],\n",
    "    [[9, 10], [11, 12]]\n",
    "]\n",
    "\n",
    "# TODO: Implement reduce operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "#### Matrix Operations with map() and reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = [\n",
    "    [[1, 2], [3, 4]],\n",
    "    [[5, 6], [7, 8]],\n",
    "    [[9, 10], [11, 12]],\n",
    "    [[13, 14], [15, 16]]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "**Question 1.8**: Using `map()` and `reduce()`:\n",
    "1. Compute the sum of all matrices\n",
    "2. Compute the element-wise product of two matrices\n",
    "3. Filter matrices where all elements are greater than 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement matrix operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "#### Data Transformation and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = [\n",
    "    {\"name\": \"Laptop\", \"price\": 1200, \"quantity\": 3, \"category\": \"Electronics\"},\n",
    "    {\"name\": \"Smartphone\", \"price\": 800, \"quantity\": 5, \"category\": \"Electronics\"},\n",
    "    {\"name\": \"Tablet\", \"price\": 300, \"quantity\": 10, \"category\": \"Electronics\"},\n",
    "    {\"name\": \"Desk\", \"price\": 250, \"quantity\": 8, \"category\": \"Furniture\"},\n",
    "    {\"name\": \"Chair\", \"price\": 150, \"quantity\": 20, \"category\": \"Furniture\"},\n",
    "    {\"name\": \"Monitor\", \"price\": 350, \"quantity\": 7, \"category\": \"Electronics\"},\n",
    "    {\"name\": \"Keyboard\", \"price\": 75, \"quantity\": 25, \"category\": \"Electronics\"},\n",
    "    {\"name\": \"Bookshelf\", \"price\": 180, \"quantity\": 5, \"category\": \"Furniture\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "**Question 1.9**: Using `map()`, `filter()`, and `reduce()`:\n",
    "1. Calculate the total inventory value (price * quantity for all products)\n",
    "2. Find products priced above 200\n",
    "3. Apply a 15% discount to all Electronics and return the updated list\n",
    "4. Calculate the total value by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "## Exercise 2 [★] - Lambda Expressions and Higher-Order Functions\n",
    "\n",
    "Lambda expressions are anonymous functions that can be defined inline. They are particularly useful with `filter()`, `map()`, and `reduce()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-45",
   "metadata": {},
   "source": [
    "### 1. Lambda Expression Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda with filter\n",
    "num = list(range(1, 21))\n",
    "even = list(filter(lambda x: x % 2 == 0, num))\n",
    "print(\"Even numbers:\", even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda with map\n",
    "squared = list(map(lambda x: x ** 2, num))\n",
    "print(\"Squared:\", squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda with multiple arguments\n",
    "list1 = [1, 2, 3, 4, 5]\n",
    "list2 = [10, 20, 30, 40, 50]\n",
    "products = list(map(lambda x, y: x * y, list1, list2))\n",
    "print(\"Products:\", products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda with reduce\n",
    "from functools import reduce\n",
    "\n",
    "total = reduce(lambda x, y: x + y, num)\n",
    "print(f\"Sum: {total}\")\n",
    "\n",
    "product = reduce(lambda x, y: x * y, [1, 2, 3, 4, 5])\n",
    "print(f\"Product: {product}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-50",
   "metadata": {},
   "source": [
    "### 2. Higher-Order Functions and Closures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A closure - function that remembers values from enclosing scope\n",
    "def make_multiplier(n):\n",
    "    def multiplier(x):\n",
    "        return x * n\n",
    "    return multiplier\n",
    "\n",
    "double = make_multiplier(2)\n",
    "triple = make_multiplier(3)\n",
    "\n",
    "print(\"Double 5:\", double(5))\n",
    "print(\"Triple 5:\", triple(5))\n",
    "\n",
    "# Using with map\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "print(\"Doubled:\", list(map(double, numbers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using functools.partial for partial function application\n",
    "from functools import partial\n",
    "\n",
    "def power(base, exponent):\n",
    "    return base ** exponent\n",
    "\n",
    "square = partial(power, exponent=2)\n",
    "cube = partial(power, exponent=3)\n",
    "\n",
    "print(\"Square of 5:\", square(5))\n",
    "print(\"Cube of 5:\", cube(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-53",
   "metadata": {},
   "source": [
    "**Question 2.1**: Create a filter factory function that generates filter predicates:\n",
    "1. `make_range_filter(min_val, max_val)` - returns a function that checks if a value is in range\n",
    "2. `make_threshold_filter(threshold, comparison)` - returns a function for threshold comparisons\n",
    "3. Use these with `filter()` on a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement filter factory functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-55",
   "metadata": {},
   "source": [
    "### 3. Function Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composing functions: f(g(x))\n",
    "def compose(f, g):\n",
    "    return lambda x: f(g(x))\n",
    "\n",
    "def add_one(x):\n",
    "    return x + 1\n",
    "\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "# (x + 1)^2\n",
    "add_then_square = compose(square, add_one)\n",
    "print(\"(3 + 1)^2 =\", add_then_square(3))\n",
    "\n",
    "# x^2 + 1\n",
    "square_then_add = compose(add_one, square)\n",
    "print(\"3^2 + 1 =\", square_then_add(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-57",
   "metadata": {},
   "source": [
    "**Question 2.2**: Implement a `pipeline()` function that composes multiple functions and use it to create a data cleaning pipeline:\n",
    "1. Strip whitespace\n",
    "2. Convert to lowercase\n",
    "3. Remove special characters\n",
    "4. Replace multiple spaces with single space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement pipeline function and data cleaning pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-59",
   "metadata": {},
   "source": [
    "#### Text Analysis with Lambda Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Big data analytics transforms raw data into actionable insights.\",\n",
    "    \"Cloud computing enables scalable infrastructure.\",\n",
    "    \"Machine learning models require training on large datasets.\",\n",
    "    \"Data pipelines automate the flow of information.\",\n",
    "    \"Distributed systems provide fault tolerance and high availability.\",\n",
    "    \"Real-time processing handles streaming data efficiently.\",\n",
    "    \"Data governance ensures quality and compliance.\",\n",
    "    \"APIs enable seamless integration between services.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-61",
   "metadata": {},
   "source": [
    "**Question 2.3**: Using `map()`, `filter()`, `reduce()` with lambda expressions:\n",
    "1. Count the total number of words in all sentences\n",
    "2. Find the sentence with the most words\n",
    "3. Extract all unique words across all sentences\n",
    "4. Calculate the average sentence length (in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement text analysis with lambdas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-63",
   "metadata": {},
   "source": [
    "#### Financial Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-64",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = [\n",
    "    {\"date\": \"2025-01-10\", \"type\": \"income\", \"amount\": 5000, \"category\": \"salary\"},\n",
    "    {\"date\": \"2025-01-11\", \"type\": \"expense\", \"amount\": 150, \"category\": \"utilities\"},\n",
    "    {\"date\": \"2025-01-12\", \"type\": \"expense\", \"amount\": 80, \"category\": \"food\"},\n",
    "    {\"date\": \"2025-01-13\", \"type\": \"income\", \"amount\": 200, \"category\": \"freelance\"},\n",
    "    {\"date\": \"2025-01-14\", \"type\": \"expense\", \"amount\": 500, \"category\": \"rent\"},\n",
    "    {\"date\": \"2025-01-15\", \"type\": \"expense\", \"amount\": 60, \"category\": \"transport\"},\n",
    "    {\"date\": \"2025-01-16\", \"type\": \"income\", \"amount\": 150, \"category\": \"freelance\"},\n",
    "    {\"date\": \"2025-01-17\", \"type\": \"expense\", \"amount\": 200, \"category\": \"food\"},\n",
    "    {\"date\": \"2025-01-18\", \"type\": \"expense\", \"amount\": 100, \"category\": \"entertainment\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-65",
   "metadata": {},
   "source": [
    "**Question 2.4**: Using lambda expressions:\n",
    "1. Calculate net balance (total income - total expenses)\n",
    "2. Find all expenses above 100\n",
    "3. Group transactions by category and calculate totals\n",
    "4. Find the largest expense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement financial data processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-67",
   "metadata": {},
   "source": [
    "## Exercise 3 [★★] - Generators and Iterators for Large Data\n",
    "\n",
    "When processing massive datasets, loading everything into memory is often impossible. Generators provide a memory-efficient way to process data lazily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-68",
   "metadata": {},
   "source": [
    "### 1. Generator Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List comprehension vs generator expression\n",
    "import sys\n",
    "\n",
    "# List - stores all values in memory\n",
    "list_comp = [x ** 2 for x in range(1000000)]\n",
    "print(f\"List size: {sys.getsizeof(list_comp):,} bytes\")\n",
    "\n",
    "# Generator - computes values on demand\n",
    "gen_exp = (x ** 2 for x in range(1000000))\n",
    "print(f\"Generator size: {sys.getsizeof(gen_exp):,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator function using yield\n",
    "def count_up_to(n):\n",
    "    \"\"\"Generator that yields numbers from 1 to n\"\"\"\n",
    "    i = 1\n",
    "    while i <= n:\n",
    "        yield i\n",
    "        i += 1\n",
    "\n",
    "# Using the generator\n",
    "counter = count_up_to(5)\n",
    "print(\"Type:\", type(counter))\n",
    "print(\"Values:\", list(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generators can only be iterated once\n",
    "gen = (x for x in range(5))\n",
    "print(\"First iteration:\", list(gen))\n",
    "print(\"Second iteration:\", list(gen))  # Empty!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-72",
   "metadata": {},
   "source": [
    "**Question 3.1**: Create a generator function `read_large_file(filepath, chunk_size)` that:\n",
    "1. Reads a file in chunks of `chunk_size` lines\n",
    "2. Yields each chunk as a list of lines\n",
    "3. Never loads the entire file into memory\n",
    "\n",
    "Test with a file containing millions of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a large test file\n",
    "import os\n",
    "\n",
    "test_file = \"large_test_file.txt\"\n",
    "with open(test_file, 'w') as f:\n",
    "    for i in range(100000):\n",
    "        f.write(f\"Line {i}: This is test data for generator exercise\\n\")\n",
    "\n",
    "print(f\"Created file with size: {os.path.getsize(test_file):,} bytes\")\n",
    "\n",
    "# TODO: Implement read_large_file generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-74",
   "metadata": {},
   "source": [
    "### 2. Generator Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chaining generators creates a pipeline\n",
    "def read_lines(filename):\n",
    "    \"\"\"Generator that yields lines from a file\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            yield line.strip()\n",
    "\n",
    "def filter_non_empty(lines):\n",
    "    \"\"\"Generator that yields non-empty lines\"\"\"\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            yield line\n",
    "\n",
    "def extract_numbers(lines):\n",
    "    \"\"\"Generator that extracts the line number\"\"\"\n",
    "    for line in lines:\n",
    "        parts = line.split(':')\n",
    "        if len(parts) >= 1:\n",
    "            num_part = parts[0].replace('Line ', '')\n",
    "            if num_part.isdigit():\n",
    "                yield int(num_part)\n",
    "\n",
    "# Pipeline: read -> filter -> extract -> sum\n",
    "pipeline = extract_numbers(filter_non_empty(read_lines(test_file)))\n",
    "\n",
    "# Process first 100 numbers\n",
    "from itertools import islice\n",
    "first_100 = list(islice(pipeline, 100))\n",
    "print(f\"First 10 line numbers: {first_100[:10]}\")\n",
    "print(f\"Sum of first 100: {sum(first_100)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-76",
   "metadata": {},
   "source": [
    "**Question 3.2**: Create a generator pipeline for log file analysis:\n",
    "1. `read_logs(filepath)` - yields log entries\n",
    "2. `parse_logs(logs)` - parses each log into a dictionary with timestamp, level, message\n",
    "3. `filter_errors(logs)` - yields only ERROR level logs\n",
    "4. `extract_messages(logs)` - yields only the message field\n",
    "\n",
    "Use the pipeline to process a large log file without loading it entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample log file\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "log_file = \"sample_logs.txt\"\n",
    "levels = [\"INFO\", \"DEBUG\", \"WARNING\", \"ERROR\", \"INFO\", \"INFO\", \"DEBUG\"]\n",
    "messages = [\n",
    "    \"Connection established\",\n",
    "    \"Processing request\",\n",
    "    \"Database query executed\",\n",
    "    \"Failed to connect to server\",\n",
    "    \"Timeout occurred\",\n",
    "    \"Cache miss\",\n",
    "    \"User authenticated\",\n",
    "    \"Invalid input received\"\n",
    "]\n",
    "\n",
    "base_time = datetime.now()\n",
    "with open(log_file, 'w') as f:\n",
    "    for i in range(50000):\n",
    "        timestamp = (base_time + timedelta(seconds=i)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        level = random.choice(levels)\n",
    "        message = random.choice(messages)\n",
    "        f.write(f\"{timestamp} [{level}] {message}\\n\")\n",
    "\n",
    "print(f\"Created log file with {50000} entries\")\n",
    "\n",
    "# TODO: Implement the log processing pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-78",
   "metadata": {},
   "source": [
    "### 3. The itertools Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# chain - combine multiple iterables\n",
    "list1 = [1, 2, 3]\n",
    "list2 = [4, 5, 6]\n",
    "list3 = [7, 8, 9]\n",
    "combined = list(itertools.chain(list1, list2, list3))\n",
    "print(\"Chained:\", combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# islice - slice an iterator\n",
    "infinite = itertools.count(start=0, step=2)  # 0, 2, 4, 6, ...\n",
    "first_10_even = list(itertools.islice(infinite, 10))\n",
    "print(\"First 10 even numbers:\", first_10_even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby - group consecutive elements\n",
    "data = [('A', 1), ('A', 2), ('B', 3), ('B', 4), ('A', 5)]\n",
    "for key, group in itertools.groupby(data, key=lambda x: x[0]):\n",
    "    print(f\"Key: {key}, Group: {list(group)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# product - cartesian product\n",
    "colors = ['red', 'blue']\n",
    "sizes = ['S', 'M', 'L']\n",
    "combinations = list(itertools.product(colors, sizes))\n",
    "print(\"Product combinations:\", combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-83",
   "metadata": {},
   "source": [
    "**Question 3.3**: Use `itertools` to:\n",
    "1. Generate all pairs from two lists without creating intermediate lists\n",
    "2. Create batches of 100 items from a large iterator\n",
    "3. Implement a sliding window of size 3 over an iterator\n",
    "4. Find the first 1000 numbers divisible by both 7 and 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement itertools exercises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup test files\n",
    "import os\n",
    "for f in [test_file, log_file]:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "        print(f\"Removed {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-86",
   "metadata": {},
   "source": [
    "## Exercise 4 [★★] - Introduction to Multiprocessing\n",
    "\n",
    "Python's Global Interpreter Lock (GIL) prevents true parallel execution of threads. For CPU-bound tasks, we use the `multiprocessing` module to achieve parallelism through separate processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-87",
   "metadata": {},
   "source": [
    "### 1. Understanding Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "# Get CPU count\n",
    "cpu_count = mp.cpu_count()\n",
    "print(f\"Number of CPU cores: {cpu_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A CPU-intensive function\n",
    "def is_prime(n):\n",
    "    \"\"\"Check if n is prime (CPU-intensive for large n)\"\"\"\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "    for i in range(3, int(n**0.5) + 1, 2):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def count_primes_in_range(start, end):\n",
    "    \"\"\"Count primes in a range\"\"\"\n",
    "    return sum(1 for n in range(start, end) if is_prime(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Sequential execution\n",
    "start_time = time.perf_counter()\n",
    "sequential_result = count_primes_in_range(2, 100000)\n",
    "sequential_time = time.perf_counter() - start_time\n",
    "\n",
    "print(f\"Sequential: Found {sequential_result} primes in {sequential_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-91",
   "metadata": {},
   "source": [
    "### 2. Using multiprocessing.Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_prime_wrapper(n): \n",
    "    \"\"\"Wrapper that returns (n, is_prime) tuple\"\"\" \n",
    "    return (n, is_prime(n)) \n",
    "\n",
    "# Parallel execution with Pool \n",
    "numbers = list(range(2, 100)) \n",
    "start_time = time.perf_counter() \n",
    "with mp.Pool(processes=cpu_count) as pool: \n",
    "    results = pool.map(check_prime_wrapper, numbers) \n",
    "\n",
    "parallel_result = sum(1 for _, is_p in results if is_p) \n",
    "parallel_time = time.perf_counter() - start_time \n",
    "\n",
    "print(f\"Parallel ({cpu_count} cores): Found {parallel_result} primes in {parallel_time:.2f} seconds\") \n",
    "print(f\"Speedup: {sequential_time / parallel_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-93",
   "metadata": {},
   "source": [
    "### 3. Pool Methods Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_square(x):\n",
    "    \"\"\"A slow function for demonstration\"\"\"\n",
    "    time.sleep(0.01)  # Simulate work\n",
    "    return x * x\n",
    "\n",
    "numbers = list(range(100))\n",
    "\n",
    "# pool.map - blocks until all results are ready, ordered\n",
    "start = time.perf_counter()\n",
    "with mp.Pool(4) as pool:\n",
    "    results_map = pool.map(slow_square, numbers)\n",
    "print(f\"pool.map: {time.perf_counter() - start:.2f}s, first 5: {results_map[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool.imap - returns iterator, ordered, can start processing before all done\n",
    "start = time.perf_counter()\n",
    "with mp.Pool(4) as pool:\n",
    "    results_imap = list(pool.imap(slow_square, numbers))\n",
    "print(f\"pool.imap: {time.perf_counter() - start:.2f}s, first 5: {results_imap[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool.imap_unordered - returns iterator, unordered (faster for uneven workloads)\n",
    "start = time.perf_counter()\n",
    "with mp.Pool(4) as pool:\n",
    "    results_unordered = list(pool.imap_unordered(slow_square, numbers))\n",
    "print(f\"pool.imap_unordered: {time.perf_counter() - start:.2f}s\")\n",
    "print(f\"Results are unordered: {results_unordered[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-97",
   "metadata": {},
   "source": [
    "**Question 4.1**: Compare the performance of `pool.map`, `pool.imap`, and `pool.imap_unordered` on a workload where different items take different amounts of time. When is each method most appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare pool methods with uneven workload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-99",
   "metadata": {},
   "source": [
    "### 4. Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_work(x):\n",
    "    return x * x\n",
    "\n",
    "data = list(range(100000))\n",
    "\n",
    "# Test different chunk sizes\n",
    "chunk_sizes = [1, 10, 100, 1000, 10000]\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    start = time.perf_counter()\n",
    "    with mp.Pool(4) as pool:\n",
    "        results = pool.map(simple_work, data, chunksize=chunk_size)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"Chunk size {chunk_size:5d}: {elapsed:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-101",
   "metadata": {},
   "source": [
    "**Question 4.2**: Experiment with different chunk sizes for a CPU-intensive task. Find the optimal chunk size and explain why it performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find optimal chunk size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-103",
   "metadata": {},
   "source": [
    "### 5. Practical Application: Parallel File Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "def download_wikidata_entity(entity_id):\n",
    "    \"\"\"Download a Wikidata entity and return its data\"\"\"\n",
    "    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{entity_id}.json\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return {\"id\": entity_id, \"status\": \"success\", \"size\": len(response.content)}\n",
    "        else:\n",
    "            return {\"id\": entity_id, \"status\": \"error\", \"code\": response.status_code}\n",
    "    except Exception as e:\n",
    "        return {\"id\": entity_id, \"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "# Download entities in parallel\n",
    "entities = [f\"Q{i}\" for i in range(1, 21)]  # Q1 to Q20\n",
    "\n",
    "print(\"Downloading Wikidata entities...\")\n",
    "start = time.perf_counter()\n",
    "with mp.Pool(4) as pool:\n",
    "    results = pool.map(download_wikidata_entity, entities)\n",
    "elapsed = time.perf_counter() - start\n",
    "\n",
    "print(f\"\\nDownloaded {len(results)} entities in {elapsed:.2f}s\")\n",
    "for r in results[:5]:\n",
    "    print(f\"  {r['id']}: {r['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-105",
   "metadata": {},
   "source": [
    "**Question 4.3**: Write a program that:\n",
    "1. Downloads 50 Wikipedia pages in parallel\n",
    "2. Counts the number of links on each page\n",
    "3. Returns the page with the most links\n",
    "\n",
    "Implement with proper error handling and rate limiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement parallel web scraping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-107",
   "metadata": {},
   "source": [
    "**Question 4.4**: Create a parallel image processing pipeline that:\n",
    "1. Reads images from a directory\n",
    "2. Resizes each image to 256x256\n",
    "3. Converts to grayscale\n",
    "4. Saves the processed images\n",
    "\n",
    "Compare sequential vs parallel execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement parallel image processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-109",
   "metadata": {},
   "source": [
    "## Exercise 5 [★★] - Process Communication: Queues and Pipes\n",
    "\n",
    "When processes need to share data, we use inter-process communication (IPC) mechanisms like Queues and Pipes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-110",
   "metadata": {},
   "source": [
    "### 1. multiprocessing.Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "import time\n",
    "\n",
    "def producer(queue, n_items):\n",
    "    \"\"\"Produce items and put them in the queue\"\"\"\n",
    "    for i in range(n_items):\n",
    "        item = f\"item_{i}\"\n",
    "        queue.put(item)\n",
    "        print(f\"Produced: {item}\")\n",
    "        time.sleep(0.1)\n",
    "    queue.put(None)  # Sentinel to signal completion\n",
    "\n",
    "def consumer(queue):\n",
    "    \"\"\"Consume items from the queue\"\"\"\n",
    "    while True:\n",
    "        item = queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "        print(f\"Consumed: {item}\")\n",
    "\n",
    "# Note: This pattern works best when run as a script, not in Jupyter\n",
    "# In Jupyter, we'll simulate with threading for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler Queue demonstration using threads (works in Jupyter)\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "def thread_producer(queue, n_items):\n",
    "    for i in range(n_items):\n",
    "        item = f\"item_{i}\"\n",
    "        queue.put(item)\n",
    "    queue.put(None)\n",
    "\n",
    "def thread_consumer(queue, results):\n",
    "    while True:\n",
    "        item = queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "        results.append(f\"processed_{item}\")\n",
    "\n",
    "q = Queue()\n",
    "results = []\n",
    "\n",
    "prod = Thread(target=thread_producer, args=(q, 10))\n",
    "cons = Thread(target=thread_consumer, args=(q, results))\n",
    "\n",
    "prod.start()\n",
    "cons.start()\n",
    "prod.join()\n",
    "cons.join()\n",
    "\n",
    "print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-113",
   "metadata": {},
   "source": [
    "### 2. Producer-Consumer Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "def data_producer(queue, data_source, n_workers):\n",
    "    \"\"\"Produce data items from a source\"\"\"\n",
    "    for item in data_source:\n",
    "        queue.put(item)\n",
    "    # Send termination signal for each worker\n",
    "    for _ in range(n_workers):\n",
    "        queue.put(None)\n",
    "\n",
    "def data_processor(queue, results, worker_id):\n",
    "    \"\"\"Process data items from the queue\"\"\"\n",
    "    local_results = []\n",
    "    while True:\n",
    "        item = queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "        # Simulate processing\n",
    "        processed = item ** 2\n",
    "        local_results.append(processed)\n",
    "    results[worker_id] = local_results\n",
    "\n",
    "# Create work queue and results storage\n",
    "work_queue = Queue()\n",
    "n_workers = 4\n",
    "results = {}\n",
    "data = list(range(100))\n",
    "\n",
    "# Start workers\n",
    "workers = []\n",
    "for i in range(n_workers):\n",
    "    w = Thread(target=data_processor, args=(work_queue, results, i))\n",
    "    w.start()\n",
    "    workers.append(w)\n",
    "\n",
    "# Start producer\n",
    "producer_thread = Thread(target=data_producer, args=(work_queue, data, n_workers))\n",
    "producer_thread.start()\n",
    "\n",
    "# Wait for completion\n",
    "producer_thread.join()\n",
    "for w in workers:\n",
    "    w.join()\n",
    "\n",
    "# Combine results\n",
    "all_results = []\n",
    "for worker_results in results.values():\n",
    "    all_results.extend(worker_results)\n",
    "\n",
    "print(f\"Processed {len(all_results)} items\")\n",
    "print(f\"Sample results: {sorted(all_results)[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-115",
   "metadata": {},
   "source": [
    "**Question 5.1**: Implement a producer-consumer system that:\n",
    "1. Has one producer generating random numbers\n",
    "2. Has 4 workers processing numbers (checking if prime)\n",
    "3. Has one aggregator collecting and summarizing results\n",
    "4. Uses bounded queues to prevent memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement producer-consumer with aggregator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-117",
   "metadata": {},
   "source": [
    "### 3. Shared Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Value, Array, Lock\n",
    "\n",
    "# Shared value (single value)\n",
    "shared_counter = Value('i', 0)  # 'i' = integer\n",
    "\n",
    "# Shared array\n",
    "shared_array = Array('d', [0.0] * 10)  # 'd' = double\n",
    "\n",
    "print(f\"Initial counter: {shared_counter.value}\")\n",
    "print(f\"Initial array: {list(shared_array)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of race condition (conceptual - best seen with multiprocessing)\n",
    "from threading import Thread, Lock\n",
    "\n",
    "counter = 0\n",
    "lock = Lock()\n",
    "\n",
    "def increment_unsafe():\n",
    "    global counter\n",
    "    for _ in range(100000):\n",
    "        counter += 1\n",
    "\n",
    "def increment_safe():\n",
    "    global counter\n",
    "    for _ in range(100000):\n",
    "        with lock:\n",
    "            counter += 1\n",
    "\n",
    "# Unsafe version - may give wrong result\n",
    "counter = 0\n",
    "threads = [Thread(target=increment_unsafe) for _ in range(4)]\n",
    "for t in threads:\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "print(f\"Unsafe counter (expected 400000): {counter}\")\n",
    "\n",
    "# Safe version - always correct\n",
    "counter = 0\n",
    "threads = [Thread(target=increment_safe) for _ in range(4)]\n",
    "for t in threads:\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "print(f\"Safe counter (expected 400000): {counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-120",
   "metadata": {},
   "source": [
    "**Question 5.2**: Implement a parallel word counter using shared memory:\n",
    "1. Multiple workers read different parts of a text file\n",
    "2. Each worker updates a shared dictionary of word counts\n",
    "3. Use proper locking to prevent race conditions\n",
    "4. Compare performance with and without locking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement parallel word counter with shared memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-122",
   "metadata": {},
   "source": [
    "## Exercise 6 [★★★] - concurrent.futures and Async Patterns\n",
    "\n",
    "The `concurrent.futures` module provides a high-level interface for asynchronously executing callables using threads or processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-123",
   "metadata": {},
   "source": [
    "### 1. ThreadPoolExecutor vs ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "def cpu_bound_task(n):\n",
    "    \"\"\"CPU-intensive task\"\"\"\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += i ** 2\n",
    "    return total\n",
    "\n",
    "def io_bound_task(url):\n",
    "    \"\"\"I/O-intensive task\"\"\"\n",
    "    time.sleep(0.1)  # Simulate network delay\n",
    "    return f\"Fetched {url}\"\n",
    "\n",
    "# CPU-bound: ProcessPoolExecutor is faster\n",
    "data = [1000000] * 8\n",
    "\n",
    "start = time.perf_counter()\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    results = list(executor.map(cpu_bound_task, data))\n",
    "print(f\"ThreadPool (CPU task): {time.perf_counter() - start:.2f}s\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    results = list(executor.map(cpu_bound_task, data))\n",
    "print(f\"ProcessPool (CPU task): {time.perf_counter() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O-bound: ThreadPoolExecutor is sufficient and has less overhead\n",
    "urls = [f\"http://example.com/page{i}\" for i in range(20)]\n",
    "\n",
    "start = time.perf_counter()\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    results = list(executor.map(io_bound_task, urls))\n",
    "print(f\"ThreadPool (I/O task): {time.perf_counter() - start:.2f}s\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    results = list(executor.map(io_bound_task, urls))\n",
    "print(f\"ProcessPool (I/O task): {time.perf_counter() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-126",
   "metadata": {},
   "source": [
    "### 2. Working with Futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import random\n",
    "\n",
    "def variable_time_task(task_id):\n",
    "    \"\"\"Task with variable execution time\"\"\"\n",
    "    sleep_time = random.uniform(0.1, 1.0)\n",
    "    time.sleep(sleep_time)\n",
    "    return {\"task_id\": task_id, \"duration\": sleep_time}\n",
    "\n",
    "# Process results as they complete\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    # Submit all tasks\n",
    "    futures = {executor.submit(variable_time_task, i): i for i in range(10)}\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in as_completed(futures):\n",
    "        task_id = futures[future]\n",
    "        result = future.result()\n",
    "        print(f\"Task {task_id} completed in {result['duration']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling exceptions\n",
    "def may_fail(task_id):\n",
    "    if random.random() < 0.3:  # 30% chance of failure\n",
    "        raise ValueError(f\"Task {task_id} failed!\")\n",
    "    return f\"Task {task_id} succeeded\"\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    futures = [executor.submit(may_fail, i) for i in range(10)]\n",
    "    \n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            print(result)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-129",
   "metadata": {},
   "source": [
    "**Question 6.1**: Implement a task scheduler that:\n",
    "1. Accepts tasks with different priorities\n",
    "2. Processes higher priority tasks first\n",
    "3. Implements timeout handling for slow tasks\n",
    "4. Provides progress reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement priority task scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-131",
   "metadata": {},
   "source": [
    "### 3. Introduction to asyncio (Optional Advanced Topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def async_task(task_id, delay):\n",
    "    \"\"\"An async task that simulates I/O\"\"\"\n",
    "    print(f\"Task {task_id} started\")\n",
    "    await asyncio.sleep(delay)\n",
    "    print(f\"Task {task_id} completed\")\n",
    "    return f\"Result from task {task_id}\"\n",
    "\n",
    "async def main():\n",
    "    # Run multiple tasks concurrently\n",
    "    tasks = [\n",
    "        async_task(1, 1),\n",
    "        async_task(2, 2),\n",
    "        async_task(3, 1),\n",
    "    ]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "# In Jupyter, use await directly\n",
    "results = await main()\n",
    "print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-133",
   "metadata": {},
   "source": [
    "**Question 6.2**: Compare the performance of:\n",
    "1. Sequential I/O operations\n",
    "2. ThreadPoolExecutor\n",
    "3. asyncio\n",
    "\n",
    "For downloading multiple web pages. Which approach is best for I/O-bound tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare I/O approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-135",
   "metadata": {},
   "source": [
    "## Exercise 7 [★★★] - Performance Benchmarking and Optimization\n",
    "\n",
    "Understanding how to measure and optimize parallel code is essential for effective use of these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-136",
   "metadata": {},
   "source": [
    "### 1. Measuring Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def timing_decorator(func):\n",
    "    \"\"\"Decorator to measure function execution time\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.perf_counter() - start\n",
    "        print(f\"{func.__name__} took {elapsed:.4f}s\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@timing_decorator\n",
    "def slow_function():\n",
    "    time.sleep(0.5)\n",
    "    return \"Done\"\n",
    "\n",
    "slow_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using timeit for more accurate measurements\n",
    "import timeit\n",
    "\n",
    "def test_function():\n",
    "    return sum(i**2 for i in range(1000))\n",
    "\n",
    "# Time 1000 executions\n",
    "execution_time = timeit.timeit(test_function, number=1000)\n",
    "print(f\"Average time per call: {execution_time/1000*1000:.4f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-139",
   "metadata": {},
   "source": [
    "### 2. Amdahl's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def amdahls_law(p, n):\n",
    "    \"\"\"\n",
    "    Calculate theoretical speedup using Amdahl's Law.\n",
    "    p: fraction of program that can be parallelized (0 to 1)\n",
    "    n: number of processors\n",
    "    \"\"\"\n",
    "    return 1 / ((1 - p) + p / n)\n",
    "\n",
    "# Visualize Amdahl's Law\n",
    "processors = np.arange(1, 65)\n",
    "parallel_fractions = [0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for p in parallel_fractions:\n",
    "    speedups = [amdahls_law(p, n) for n in processors]\n",
    "    plt.plot(processors, speedups, label=f'{p*100:.0f}% parallelizable')\n",
    "\n",
    "plt.xlabel('Number of Processors')\n",
    "plt.ylabel('Speedup')\n",
    "plt.title(\"Amdahl's Law: Theoretical Speedup Limits\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-141",
   "metadata": {},
   "source": [
    "### 3. Benchmarking Parallel Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "def cpu_intensive(n):\n",
    "    \"\"\"CPU-intensive computation\"\"\"\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += i ** 0.5\n",
    "    return total\n",
    "\n",
    "def benchmark_parallel(func, data, max_workers=None):\n",
    "    \"\"\"Benchmark a function with different numbers of workers\"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = mp.cpu_count()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Sequential baseline\n",
    "    start = time.perf_counter()\n",
    "    sequential_results = [func(d) for d in data]\n",
    "    sequential_time = time.perf_counter() - start\n",
    "    results.append((1, sequential_time, 1.0))\n",
    "    print(f\"Sequential: {sequential_time:.4f}s\")\n",
    "    \n",
    "    # Parallel with different worker counts\n",
    "    for n_workers in range(2, max_workers + 1):\n",
    "        start = time.perf_counter()\n",
    "        with mp.Pool(n_workers) as pool:\n",
    "            parallel_results = pool.map(func, data)\n",
    "        parallel_time = time.perf_counter() - start\n",
    "        speedup = sequential_time / parallel_time\n",
    "        efficiency = speedup / n_workers\n",
    "        results.append((n_workers, parallel_time, speedup))\n",
    "        print(f\"Workers={n_workers}: {parallel_time:.4f}s, Speedup={speedup:.2f}x, Efficiency={efficiency:.1%}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "data = [1000000] * 16\n",
    "benchmark_results = benchmark_parallel(cpu_intensive, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-143",
   "metadata": {},
   "source": [
    "**Question 7.1**: Create a comprehensive benchmark suite that:\n",
    "1. Tests different chunk sizes\n",
    "2. Tests different data sizes\n",
    "3. Generates visualizations comparing performance\n",
    "4. Identifies the optimal configuration for your workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create comprehensive benchmark suite\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-145",
   "metadata": {},
   "source": [
    "### 4. Profiling and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "\n",
    "def profile_function(func, *args, **kwargs):\n",
    "    \"\"\"Profile a function and print results\"\"\"\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    result = func(*args, **kwargs)\n",
    "    profiler.disable()\n",
    "    \n",
    "    stream = StringIO()\n",
    "    stats = pstats.Stats(profiler, stream=stream)\n",
    "    stats.sort_stats('cumulative')\n",
    "    stats.print_stats(10)\n",
    "    print(stream.getvalue())\n",
    "    \n",
    "    return result\n",
    "\n",
    "def function_to_profile():\n",
    "    \"\"\"Example function to profile\"\"\"\n",
    "    data = [i**2 for i in range(100000)]\n",
    "    filtered = list(filter(lambda x: x % 2 == 0, data))\n",
    "    total = sum(filtered)\n",
    "    return total\n",
    "\n",
    "profile_function(function_to_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-147",
   "metadata": {},
   "source": [
    "**Question 7.2**: Profile and optimize the image processing pipeline from Exercise 4:\n",
    "1. Identify the three biggest bottlenecks\n",
    "2. Apply optimizations to each\n",
    "3. Measure the improvement\n",
    "4. Document your optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Profile and optimize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-149",
   "metadata": {},
   "source": [
    "### 5. Real-World Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-150",
   "metadata": {},
   "source": [
    "**Question 7.3**: Build a complete data processing system that:\n",
    "1. Reads data from multiple CSV files (100+ files)\n",
    "2. Cleans and transforms each file\n",
    "3. Aggregates results across all files\n",
    "4. Writes the final output\n",
    "\n",
    "Implement both sequential and parallel versions, benchmark them, and document the speedup achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement real-world case study\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-152",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this practical, you learned:\n",
    "\n",
    "1. **Functional Programming**: Using `filter()`, `map()`, `reduce()` for data transformations\n",
    "2. **Lambda Expressions**: Creating concise, inline functions\n",
    "3. **Generators**: Memory-efficient processing of large datasets\n",
    "4. **Multiprocessing**: Parallel execution using `Pool` and process-based parallelism\n",
    "5. **Inter-Process Communication**: Using Queues and shared memory\n",
    "6. **concurrent.futures**: High-level interface for parallel execution\n",
    "7. **Performance Optimization**: Benchmarking, profiling, and optimizing parallel code\n",
    "\n",
    "These concepts form the foundation for distributed computing frameworks like Apache Spark, which you'll explore in Practical 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-153",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Continue to **Practical 5: Apache Spark for Massive Data Processing** to learn how these parallel processing concepts scale to distributed computing across clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
