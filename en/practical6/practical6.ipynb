{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 6: Docker for Data Processing Pipelines\n",
    "\n",
    "## Goals\n",
    "\n",
    "This practical session introduces Docker containerization for building scalable and reproducible data processing pipelines. You will learn how to package applications, manage multi-container environments, and deploy data processing workflows.\n",
    "\n",
    "### Learning Objectives\n",
    "* Understand Docker architecture and containerization concepts\n",
    "* Write Dockerfiles to package Python applications\n",
    "* Use Docker Compose for multi-container orchestration\n",
    "* Implement data pipelines with shared volumes\n",
    "* Build producer-consumer patterns with message queues\n",
    "* Connect applications to databases in containers\n",
    "* Implement frontend-backend architectures\n",
    "* Deploy and scale data processing applications\n",
    "\n",
    "### Prerequisites\n",
    "* Completion of Practical 5 (Apache Spark)\n",
    "* Docker Desktop installed ([Installation Guide](https://docs.docker.com/get-docker/))\n",
    "* Basic understanding of Linux commands\n",
    "* Python programming fundamentals\n",
    "\n",
    "### Installation\n",
    "\n",
    "Verify Docker is installed:\n",
    "```bash\n",
    "docker --version\n",
    "docker-compose --version\n",
    "```\n",
    "\n",
    "### Exercises Overview\n",
    "\n",
    "| Exercise | Topic | Difficulty |\n",
    "|----------|-------|------------|\n",
    "| 1 | Docker Fundamentals and Basic Commands | ★ |\n",
    "| 2 | Writing Dockerfiles for Python Applications | ★ |\n",
    "| 3 | Docker Compose for Multi-Container Applications | ★★ |\n",
    "| 4 | Data Pipelines with Shared Volumes | ★★ |\n",
    "| 5 | Producer-Consumer with Message Queues | ★★ |\n",
    "| 6 | Application-Database Integration | ★★ |\n",
    "| 7 | Frontend-Backend Architectures | ★★★ |\n",
    "| 8 | Scaling and Monitoring Containers | ★★★ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Docker Fundamentals and Basic Commands [★]\n",
    "\n",
    "### Docker Architecture\n",
    "\n",
    "Docker uses a client-server architecture:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                     Docker Host                              │\n",
    "│  ┌─────────────┐    ┌─────────────────────────────────────┐ │\n",
    "│  │   Docker    │    │          Docker Daemon               │ │\n",
    "│  │   Client    │◄──►│  ┌─────────┐  ┌─────────┐           │ │\n",
    "│  │   (CLI)     │    │  │Container│  │Container│           │ │\n",
    "│  └─────────────┘    │  │   1     │  │   2     │           │ │\n",
    "│                     │  └─────────┘  └─────────┘           │ │\n",
    "│                     │       │            │                 │ │\n",
    "│                     │  ┌────┴────────────┴────┐           │ │\n",
    "│                     │  │      Images          │           │ │\n",
    "│                     │  └─────────────────────┘           │ │\n",
    "│                     └─────────────────────────────────────┘ │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Image**: Read-only template with instructions for creating a container\n",
    "- **Container**: Runnable instance of an image\n",
    "- **Dockerfile**: Text file with instructions to build an image\n",
    "- **Registry**: Storage for Docker images (e.g., Docker Hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Docker Commands\n",
    "\n",
    "Run the following commands in your terminal to familiarize yourself with Docker:\n",
    "\n",
    "```bash\n",
    "# Check Docker version\n",
    "docker --version\n",
    "\n",
    "# View system-wide information\n",
    "docker info\n",
    "\n",
    "# List available images\n",
    "docker images\n",
    "\n",
    "# List running containers\n",
    "docker ps\n",
    "\n",
    "# List all containers (including stopped)\n",
    "docker ps -a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Your First Container\n",
    "\n",
    "```bash\n",
    "# Run a simple hello-world container\n",
    "docker run hello-world\n",
    "\n",
    "# Run an interactive Python container\n",
    "docker run -it python:3.10 python\n",
    "\n",
    "# Run a container with a specific command\n",
    "docker run python:3.10 python -c \"print('Hello from Docker!')\"\n",
    "\n",
    "# Run a container in the background (detached mode)\n",
    "docker run -d --name my_python python:3.10 sleep 60\n",
    "\n",
    "# Stop a running container\n",
    "docker stop my_python\n",
    "\n",
    "# Remove a container\n",
    "docker rm my_python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container Lifecycle\n",
    "\n",
    "```\n",
    "┌─────────┐   docker run   ┌─────────┐   docker stop   ┌─────────┐\n",
    "│ Created │───────────────►│ Running │────────────────►│ Stopped │\n",
    "└─────────┘                └─────────┘                 └─────────┘\n",
    "     │                          │                           │\n",
    "     │                          │ docker pause              │\n",
    "     │                          ▼                           │\n",
    "     │                    ┌─────────┐                       │\n",
    "     │                    │ Paused  │                       │\n",
    "     │                    └─────────┘                       │\n",
    "     │                                                      │\n",
    "     └──────────────────────────────────────────────────────┘\n",
    "                        docker rm\n",
    "```\n",
    "\n",
    "```bash\n",
    "# View container logs\n",
    "docker logs <container_id>\n",
    "\n",
    "# Execute command in running container\n",
    "docker exec -it <container_id> bash\n",
    "\n",
    "# Copy files to/from container\n",
    "docker cp local_file.txt <container_id>:/path/in/container/\n",
    "docker cp <container_id>:/path/in/container/file.txt ./local_file.txt\n",
    "\n",
    "# View container resource usage\n",
    "docker stats\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercise 1\n",
    "\n",
    "**Q1.1** Run a Python container that prints the system's Python version, OS name, and current date/time. Capture the output.\n",
    "\n",
    "**Q1.2** Run an Ubuntu container interactively. Inside the container:\n",
    "- Update the package list\n",
    "- Install `curl`\n",
    "- Download a web page\n",
    "- Exit the container\n",
    "\n",
    "**Q1.3** Run three containers in detached mode with different names. Use `docker ps` to verify they're running, then stop and remove all of them using a single command each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Writing Dockerfiles for Python Applications [★]\n",
    "\n",
    "### Dockerfile Basics\n",
    "\n",
    "A Dockerfile is a script containing instructions to build a Docker image.\n",
    "\n",
    "### Common Dockerfile Instructions\n",
    "\n",
    "| Instruction | Description |\n",
    "|-------------|-------------|\n",
    "| `FROM` | Base image to start from |\n",
    "| `WORKDIR` | Set working directory |\n",
    "| `COPY` | Copy files from host to image |\n",
    "| `RUN` | Execute commands during build |\n",
    "| `ENV` | Set environment variables |\n",
    "| `EXPOSE` | Document which ports the container listens on |\n",
    "| `CMD` | Default command when container starts |\n",
    "| `ENTRYPOINT` | Configure container to run as executable |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Simple Python Application\n",
    "\n",
    "Create a file `app.py`:\n",
    "\n",
    "```python\n",
    "# app.py\n",
    "import sys\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "def main():\n",
    "    print(f\"Python version: {sys.version}\")\n",
    "    print(f\"Platform: {platform.platform()}\")\n",
    "    print(f\"Current time: {datetime.now()}\")\n",
    "    print(\"Hello from Docker!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "Create a `Dockerfile`:\n",
    "\n",
    "```dockerfile\n",
    "# Use official Python image as base\n",
    "FROM python:3.10-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy application code\n",
    "COPY app.py .\n",
    "\n",
    "# Set the default command\n",
    "CMD [\"python\", \"app.py\"]\n",
    "```\n",
    "\n",
    "Build and run:\n",
    "\n",
    "```bash\n",
    "# Build the image\n",
    "docker build -t my-python-app .\n",
    "\n",
    "# Run the container\n",
    "docker run my-python-app\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Python Application with Dependencies\n",
    "\n",
    "Create `requirements.txt`:\n",
    "\n",
    "```\n",
    "pandas==2.0.0\n",
    "numpy==1.24.0\n",
    "requests==2.28.0\n",
    "```\n",
    "\n",
    "Create `data_processor.py`:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_data():\n",
    "    # Create sample data\n",
    "    data = {\n",
    "        'name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "        'value': np.random.randint(1, 100, 4)\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(\"Data Processing Results:\")\n",
    "    print(df)\n",
    "    print(f\"\\nSum: {df['value'].sum()}\")\n",
    "    print(f\"Mean: {df['value'].mean():.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_data()\n",
    "```\n",
    "\n",
    "Optimized `Dockerfile`:\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy requirements first (for better layer caching)\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY data_processor.py .\n",
    "\n",
    "CMD [\"python\", \"data_processor.py\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Stage Builds\n",
    "\n",
    "Multi-stage builds help create smaller production images:\n",
    "\n",
    "```dockerfile\n",
    "# Build stage\n",
    "FROM python:3.10 AS builder\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --user --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Production stage\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy installed packages from builder\n",
    "COPY --from=builder /root/.local /root/.local\n",
    "\n",
    "# Make sure scripts in .local are usable\n",
    "ENV PATH=/root/.local/bin:$PATH\n",
    "\n",
    "COPY app.py .\n",
    "\n",
    "CMD [\"python\", \"app.py\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Dockerfiles\n",
    "\n",
    "1. **Use specific base image tags**: `python:3.10-slim` instead of `python:latest`\n",
    "2. **Order instructions by frequency of change**: Copy requirements before code\n",
    "3. **Use `.dockerignore`**: Exclude unnecessary files\n",
    "4. **Minimize layers**: Combine related RUN commands\n",
    "5. **Don't run as root**: Create a non-root user when possible\n",
    "6. **Use multi-stage builds**: For smaller production images\n",
    "\n",
    "Example `.dockerignore`:\n",
    "\n",
    "```\n",
    "__pycache__\n",
    "*.pyc\n",
    "*.pyo\n",
    ".git\n",
    ".gitignore\n",
    "*.md\n",
    ".env\n",
    "venv/\n",
    ".pytest_cache/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercise 2\n",
    "\n",
    "**Q2.1** Create a Dockerfile for a PySpark application that:\n",
    "- Uses `bitnami/spark` as the base image\n",
    "- Installs additional Python packages (pandas, matplotlib)\n",
    "- Copies a Spark script that processes CSV data\n",
    "- Runs the script when the container starts\n",
    "\n",
    "**Q2.2** Create a Dockerfile that:\n",
    "- Uses a non-root user for security\n",
    "- Implements health checks\n",
    "- Uses environment variables for configuration\n",
    "- Includes proper labeling (maintainer, version, description)\n",
    "\n",
    "**Q2.3** Compare the image sizes of:\n",
    "- A simple Dockerfile using `python:3.10`\n",
    "- The same application using `python:3.10-slim`\n",
    "- A multi-stage build version\n",
    "\n",
    "Document the size differences and explain when each approach is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Docker Compose for Multi-Container Applications [★★]\n",
    "\n",
    "### Docker Compose Overview\n",
    "\n",
    "Docker Compose allows you to define and run multi-container applications using a YAML file.\n",
    "\n",
    "### Basic docker-compose.yml Structure\n",
    "\n",
    "```yaml\n",
    "version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  service_name:\n",
    "    image: image_name:tag\n",
    "    # OR build from Dockerfile\n",
    "    build: ./path/to/dockerfile\n",
    "    ports:\n",
    "      - \"host_port:container_port\"\n",
    "    volumes:\n",
    "      - ./local/path:/container/path\n",
    "    environment:\n",
    "      - VAR_NAME=value\n",
    "    depends_on:\n",
    "      - other_service\n",
    "\n",
    "volumes:\n",
    "  named_volume:\n",
    "\n",
    "networks:\n",
    "  custom_network:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Compose Commands\n",
    "\n",
    "```bash\n",
    "# Start all services\n",
    "docker-compose up\n",
    "\n",
    "# Start in detached mode\n",
    "docker-compose up -d\n",
    "\n",
    "# Build images before starting\n",
    "docker-compose up --build\n",
    "\n",
    "# Stop all services\n",
    "docker-compose down\n",
    "\n",
    "# Stop and remove volumes\n",
    "docker-compose down -v\n",
    "\n",
    "# View logs\n",
    "docker-compose logs\n",
    "docker-compose logs -f service_name\n",
    "\n",
    "# Scale a service\n",
    "docker-compose up --scale service_name=3\n",
    "\n",
    "# Execute command in a service\n",
    "docker-compose exec service_name command\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Web Application with Redis\n",
    "\n",
    "Create `app.py`:\n",
    "\n",
    "```python\n",
    "from flask import Flask\n",
    "import redis\n",
    "\n",
    "app = Flask(__name__)\n",
    "cache = redis.Redis(host='redis', port=6379)\n",
    "\n",
    "@app.route('/')\n",
    "def hello():\n",
    "    count = cache.incr('hits')\n",
    "    return f'Hello! This page has been viewed {count} times.'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "```\n",
    "\n",
    "Create `requirements.txt`:\n",
    "\n",
    "```\n",
    "flask\n",
    "redis\n",
    "```\n",
    "\n",
    "Create `Dockerfile`:\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY app.py .\n",
    "\n",
    "EXPOSE 5000\n",
    "\n",
    "CMD [\"python\", \"app.py\"]\n",
    "```\n",
    "\n",
    "Create `docker-compose.yml`:\n",
    "\n",
    "```yaml\n",
    "version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  web:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    depends_on:\n",
    "      - redis\n",
    "    environment:\n",
    "      - FLASK_ENV=development\n",
    "\n",
    "  redis:\n",
    "    image: redis:alpine\n",
    "    volumes:\n",
    "      - redis_data:/data\n",
    "\n",
    "volumes:\n",
    "  redis_data:\n",
    "```\n",
    "\n",
    "Run with:\n",
    "\n",
    "```bash\n",
    "docker-compose up --build\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Service Dependencies and Health Checks\n",
    "\n",
    "```yaml\n",
    "version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  web:\n",
    "    build: .\n",
    "    depends_on:\n",
    "      db:\n",
    "        condition: service_healthy\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:5000/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "\n",
    "  db:\n",
    "    image: postgres:15\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", \"pg_isready -U postgres\"]\n",
    "      interval: 5s\n",
    "      timeout: 5s\n",
    "      retries: 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercise 3\n",
    "\n",
    "**Q3.1** Create a Docker Compose configuration for a data processing pipeline with:\n",
    "- A Python data generator service\n",
    "- A Redis service for caching\n",
    "- A data processor service that reads from Redis\n",
    "- Proper service dependencies\n",
    "\n",
    "**Q3.2** Modify the previous example to use:\n",
    "- Custom networks for service isolation\n",
    "- Environment files (`.env`)\n",
    "- Volume mounts for data persistence\n",
    "\n",
    "**Q3.3** Create a Docker Compose file that starts a Jupyter Notebook server with:\n",
    "- Pre-installed data science libraries (pandas, numpy, matplotlib, sklearn)\n",
    "- Persistent notebook storage\n",
    "- Access to a shared data volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Data Pipelines with Shared Volumes [★★]\n",
    "\n",
    "### Shared Volumes for Container Communication\n",
    "\n",
    "Shared volumes allow containers to exchange data through the file system.\n",
    "\n",
    "```\n",
    "┌─────────────────┐       ┌─────────────────┐\n",
    "│    Uploader     │       │    Processor    │\n",
    "│    Container    │       │    Container    │\n",
    "│                 │       │                 │\n",
    "│   writes to     │       │   reads from    │\n",
    "│   /shared       │       │   /shared       │\n",
    "└────────┬────────┘       └────────┬────────┘\n",
    "         │                         │\n",
    "         └─────────┬───────────────┘\n",
    "                   │\n",
    "            ┌──────┴──────┐\n",
    "            │   Shared    │\n",
    "            │   Volume    │\n",
    "            └─────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: File Processing Pipeline\n",
    "\n",
    "Navigate to the `SharedVolume` folder in this practical:\n",
    "\n",
    "```bash\n",
    "cd SharedVolume\n",
    "```\n",
    "\n",
    "Examine the existing structure:\n",
    "\n",
    "**Uploader Service** (`Uploader/upload.py`):\n",
    "```python\n",
    "import time\n",
    "from shutil import copyfile\n",
    "\n",
    "def upload_file():\n",
    "    while True:\n",
    "        # Simulate uploading a new file every 5 seconds\n",
    "        print(\"Uploading new file...\")\n",
    "        copyfile(\"sample.txt\", \"/shared/sample_uploaded.txt\")\n",
    "        time.sleep(5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    upload_file()\n",
    "```\n",
    "\n",
    "**Processor Service** (`Processor/process.py`):\n",
    "```python\n",
    "import time\n",
    "import os\n",
    "\n",
    "def process_files():\n",
    "    while True:\n",
    "        if os.path.exists(\"/shared/sample_uploaded.txt\"):\n",
    "            with open(\"/shared/sample_uploaded.txt\", \"r\") as f:\n",
    "                content = f.read()\n",
    "            print(f\"Processing: {content}\")\n",
    "            # Process the file...\n",
    "            os.remove(\"/shared/sample_uploaded.txt\")\n",
    "        else:\n",
    "            print(\"Waiting for files...\")\n",
    "        time.sleep(2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_files()\n",
    "```\n",
    "\n",
    "**docker-compose.yml**:\n",
    "```yaml\n",
    "version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  uploader:\n",
    "    build:\n",
    "      context: ./uploader\n",
    "    volumes:\n",
    "      - ./shared:/shared\n",
    "    depends_on:\n",
    "      - processor\n",
    "\n",
    "  processor:\n",
    "    build:\n",
    "      context: ./processor\n",
    "    volumes:\n",
    "      - ./shared:/shared\n",
    "```\n",
    "\n",
    "Run with:\n",
    "```bash\n",
    "docker-compose up --build\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced Data Pipeline Example\n",
    "\n",
    "Create a more sophisticated data pipeline:\n",
    "\n",
    "**data_generator.py**:\n",
    "```python\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_data():\n",
    "    counter = 0\n",
    "    while True:\n",
    "        data = {\n",
    "            \"id\": counter,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"sensor_id\": f\"sensor_{random.randint(1, 10)}\",\n",
    "            \"temperature\": round(random.uniform(20, 35), 2),\n",
    "            \"humidity\": round(random.uniform(30, 80), 2)\n",
    "        }\n",
    "        \n",
    "        filename = f\"/shared/input/data_{counter}.json\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "        \n",
    "        print(f\"Generated: {filename}\")\n",
    "        counter += 1\n",
    "        time.sleep(2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    os.makedirs(\"/shared/input\", exist_ok=True)\n",
    "    generate_data()\n",
    "```\n",
    "\n",
    "**data_processor.py**:\n",
    "```python\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "def process_files():\n",
    "    os.makedirs(\"/shared/output\", exist_ok=True)\n",
    "    \n",
    "    while True:\n",
    "        input_dir = \"/shared/input\"\n",
    "        if os.path.exists(input_dir):\n",
    "            files = [f for f in os.listdir(input_dir) if f.endswith('.json')]\n",
    "            \n",
    "            for filename in files:\n",
    "                filepath = os.path.join(input_dir, filename)\n",
    "                \n",
    "                with open(filepath, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # Process the data\n",
    "                data['processed'] = True\n",
    "                data['temp_fahrenheit'] = round(data['temperature'] * 9/5 + 32, 2)\n",
    "                \n",
    "                # Write to output\n",
    "                output_path = f\"/shared/output/processed_{filename}\"\n",
    "                with open(output_path, 'w') as f:\n",
    "                    json.dump(data, f, indent=2)\n",
    "                \n",
    "                # Remove input file\n",
    "                os.remove(filepath)\n",
    "                print(f\"Processed: {filename}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_files()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercise 4\n",
    "\n",
    "**Q4.1** Extend the SharedVolume example to:\n",
    "- Add a third service that aggregates processed files\n",
    "- Generate statistics (average temperature, humidity by sensor)\n",
    "- Output a summary report every minute\n",
    "\n",
    "**Q4.2** Implement error handling in the pipeline:\n",
    "- Move failed files to an \"error\" directory\n",
    "- Log errors with timestamps\n",
    "- Add a monitoring service that reports pipeline health\n",
    "\n",
    "**Q4.3** Create a parallel processing pipeline:\n",
    "- Multiple processor containers (use `--scale`)\n",
    "- Implement file locking to prevent duplicate processing\n",
    "- Measure throughput with different numbers of processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Producer-Consumer with Message Queues [★★]\n",
    "\n",
    "### Message Queue Pattern\n",
    "\n",
    "Message queues decouple producers and consumers, enabling:\n",
    "- Asynchronous processing\n",
    "- Load balancing\n",
    "- Fault tolerance\n",
    "\n",
    "```\n",
    "┌──────────┐     ┌─────────────┐     ┌──────────┐\n",
    "│ Producer │────►│   Message   │────►│ Consumer │\n",
    "│    1     │     │    Queue    │     │    1     │\n",
    "└──────────┘     │  (RabbitMQ) │     └──────────┘\n",
    "┌──────────┐     │             │     ┌──────────┐\n",
    "│ Producer │────►│             │────►│ Consumer │\n",
    "│    2     │     └─────────────┘     │    2     │\n",
    "└──────────┘                         └──────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RabbitMQ Example\n",
    "\n",
    "Navigate to the `ProducerConsumerRabbitMQ` folder:\n",
    "\n",
    "```bash\n",
    "cd ProducerConsumerRabbitMQ\n",
    "```\n",
    "\n",
    "**producer/producer.py**:\n",
    "```python\n",
    "import pika\n",
    "import time\n",
    "\n",
    "def connect():\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            return pika.BlockingConnection(pika.ConnectionParameters('rabbitmq'))\n",
    "        except:\n",
    "            print(\"Retrying connection to RabbitMQ...\")\n",
    "            time.sleep(2)\n",
    "    raise Exception(\"Could not connect to RabbitMQ\")\n",
    "\n",
    "connection = connect()\n",
    "channel = connection.channel()\n",
    "channel.queue_declare(queue='task_queue', durable=True)\n",
    "\n",
    "for i in range(100):\n",
    "    msg = f\"Task #{i}\"\n",
    "    channel.basic_publish(\n",
    "        exchange='',\n",
    "        routing_key='task_queue',\n",
    "        body=msg,\n",
    "        properties=pika.BasicProperties(delivery_mode=2)  # Make message persistent\n",
    "    )\n",
    "    print(f\"Sent: {msg}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "connection.close()\n",
    "```\n",
    "\n",
    "**consumer/consumer.py**:\n",
    "```python\n",
    "import pika\n",
    "import time\n",
    "\n",
    "def connect():\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            return pika.BlockingConnection(pika.ConnectionParameters('rabbitmq'))\n",
    "        except:\n",
    "            print(\"Retrying connection to RabbitMQ...\")\n",
    "            time.sleep(2)\n",
    "    raise Exception(\"Could not connect to RabbitMQ\")\n",
    "\n",
    "def callback(ch, method, properties, body):\n",
    "    print(f\"Received: {body.decode()}\")\n",
    "    time.sleep(0.5)  # Simulate processing\n",
    "    print(f\"Processed: {body.decode()}\")\n",
    "    ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "\n",
    "connection = connect()\n",
    "channel = connection.channel()\n",
    "channel.queue_declare(queue='task_queue', durable=True)\n",
    "channel.basic_qos(prefetch_count=1)  # Fair dispatch\n",
    "channel.basic_consume(queue='task_queue', on_message_callback=callback)\n",
    "\n",
    "print('Waiting for messages...')\n",
    "channel.start_consuming()\n",
    "```\n",
    "\n",
    "**docker-compose.yml**:\n",
    "```yaml\n",
    "services:\n",
    "  rabbitmq:\n",
    "    image: rabbitmq:3-management\n",
    "    ports:\n",
    "      - \"5672:5672\"   # AMQP protocol\n",
    "      - \"15672:15672\" # Management UI\n",
    "    environment:\n",
    "      RABBITMQ_DEFAULT_USER: guest\n",
    "      RABBITMQ_DEFAULT_PASS: guest\n",
    "\n",
    "  producer:\n",
    "    build: ./producer\n",
    "    depends_on:\n",
    "      - rabbitmq\n",
    "\n",
    "  consumer:\n",
    "    build: ./consumer\n",
    "    depends_on:\n",
    "      - rabbitmq\n",
    "```\n",
    "\n",
    "Run with:\n",
    "```bash\n",
    "docker-compose up --build\n",
    "\n",
    "# Scale consumers\n",
    "docker-compose up --scale consumer=3\n",
    "```\n",
    "\n",
    "Access RabbitMQ management UI at: http://localhost:15672 (guest/guest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing with Message Queues\n",
    "\n",
    "Enhanced producer for data processing:\n",
    "\n",
    "```python\n",
    "# data_producer.py\n",
    "import pika\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def connect():\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            return pika.BlockingConnection(pika.ConnectionParameters('rabbitmq'))\n",
    "        except:\n",
    "            time.sleep(2)\n",
    "    raise Exception(\"Could not connect\")\n",
    "\n",
    "connection = connect()\n",
    "channel = connection.channel()\n",
    "channel.queue_declare(queue='data_queue', durable=True)\n",
    "\n",
    "sensors = ['temperature', 'humidity', 'pressure']\n",
    "\n",
    "while True:\n",
    "    data = {\n",
    "        'sensor_type': random.choice(sensors),\n",
    "        'value': round(random.uniform(0, 100), 2),\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    channel.basic_publish(\n",
    "        exchange='',\n",
    "        routing_key='data_queue',\n",
    "        body=json.dumps(data),\n",
    "        properties=pika.BasicProperties(delivery_mode=2)\n",
    "    )\n",
    "    \n",
    "    print(f\"Sent: {data}\")\n",
    "    time.sleep(0.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercise 5\n",
    "\n",
    "**Q5.1** Extend the RabbitMQ example to:\n",
    "- Use topic-based routing (different queues for different data types)\n",
    "- Implement multiple consumer types (one for each sensor type)\n",
    "- Store processed data in a shared volume\n",
    "\n",
    "**Q5.2** Implement dead letter handling:\n",
    "- Configure a dead letter queue for failed messages\n",
    "- Add a retry mechanism (max 3 retries)\n",
    "- Create a monitoring consumer that alerts on DLQ messages\n",
    "\n",
    "**Q5.3** Compare RabbitMQ with Redis Pub/Sub:\n",
    "- Implement the same producer-consumer pattern with Redis\n",
    "- Measure message throughput\n",
    "- Document the trade-offs between the two approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 6: Application-Database Integration [★★]\n",
    "\n",
    "### Connecting Applications to Databases\n",
    "\n",
    "Navigate to the `AppDB` folder:\n",
    "\n",
    "```bash\n",
    "cd AppDB\n",
    "```\n",
    "\n",
    "This example demonstrates a Flask application connected to PostgreSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**app/app.py**:\n",
    "```python\n",
    "from flask import Flask\n",
    "import psycopg2\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"bd\",  # Service name in Docker\n",
    "        database=\"livres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"postgres\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT titre FROM livres\")\n",
    "    livres = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return \"<br>\".join(title for (title,) in livres)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000)\n",
    "```\n",
    "\n",
    "**init_bd/init.sql**:\n",
    "```sql\n",
    "CREATE TABLE IF NOT EXISTS livres (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    titre VARCHAR(255) NOT NULL,\n",
    "    auteur VARCHAR(255),\n",
    "    annee INTEGER\n",
    ");\n",
    "\n",
    "INSERT INTO livres (titre, auteur, annee) VALUES\n",
    "    ('Les Misérables', 'Victor Hugo', 1862),\n",
    "    ('Le Petit Prince', 'Antoine de Saint-Exupéry', 1943),\n",
    "    ('L''Étranger', 'Albert Camus', 1942);\n",
    "```\n",
    "\n",
    "**docker-compose.yml**:\n",
    "```yaml\n",
    "services:\n",
    "  app:\n",
    "    build: ./app\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    depends_on:\n",
    "      - bd\n",
    "\n",
    "  bd:\n",
    "    image: postgres:15\n",
    "    environment:\n",
    "      POSTGRES_DB: livres\n",
    "      POSTGRES_USER: postgres\n",
    "      POSTGRES_PASSWORD: postgres\n",
    "    volumes:\n",
    "      - ./init_bd:/docker-entrypoint-initdb.d\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "```\n",
    "\n",
    "Run with:\n",
    "```bash\n",
    "docker-compose up --build\n",
    "```\n",
    "\n",
    "Access at: http://localhost:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced Example with SQLAlchemy\n",
    "\n",
    "```python\n",
    "# app_enhanced.py\n",
    "from flask import Flask, jsonify, request\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Database configuration from environment\n",
    "db_host = os.environ.get('DB_HOST', 'bd')\n",
    "db_name = os.environ.get('DB_NAME', 'livres')\n",
    "db_user = os.environ.get('DB_USER', 'postgres')\n",
    "db_pass = os.environ.get('DB_PASS', 'postgres')\n",
    "\n",
    "app.config['SQLALCHEMY_DATABASE_URI'] = f'postgresql://{db_user}:{db_pass}@{db_host}/{db_name}'\n",
    "app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n",
    "\n",
    "db = SQLAlchemy(app)\n",
    "\n",
    "class Book(db.Model):\n",
    "    __tablename__ = 'livres'\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    titre = db.Column(db.String(255), nullable=False)\n",
    "    auteur = db.Column(db.String(255))\n",
    "    annee = db.Column(db.Integer)\n",
    "\n",
    "@app.route('/books')\n",
    "def get_books():\n",
    "    books = Book.query.all()\n",
    "    return jsonify([{\n",
    "        'id': b.id,\n",
    "        'title': b.titre,\n",
    "        'author': b.auteur,\n",
    "        'year': b.annee\n",
    "    } for b in books])\n",
    "\n",
    "@app.route('/books', methods=['POST'])\n",
    "def add_book():\n",
    "    data = request.json\n",
    "    book = Book(titre=data['title'], auteur=data['author'], annee=data['year'])\n",
    "    db.session.add(book)\n",
    "    db.session.commit()\n",
    "    return jsonify({'id': book.id}), 201\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercise 6\n",
    "\n",
    "**Q6.1** Extend the AppDB example to include:\n",
    "- CRUD operations (Create, Read, Update, Delete)\n",
    "- Input validation\n",
    "- Error handling with appropriate HTTP status codes\n",
    "\n",
    "**Q6.2** Add data analytics capabilities:\n",
    "- Endpoint to get books by year range\n",
    "- Statistics endpoint (count by author, books per decade)\n",
    "- Full-text search capability\n",
    "\n",
    "**Q6.3** Implement a data import service:\n",
    "- Create a separate container that imports CSV data into the database\n",
    "- Watch a shared volume for new CSV files\n",
    "- Log import results and errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 7: Frontend-Backend Architectures [★★★]\n",
    "\n",
    "### Microservices Architecture\n",
    "\n",
    "Navigate to the `WebAppFrontBack` folder:\n",
    "\n",
    "```bash\n",
    "cd WebAppFrontBack\n",
    "```\n",
    "\n",
    "This example demonstrates a React frontend with a Flask backend.\n",
    "\n",
    "```\n",
    "┌─────────────────┐      ┌─────────────────┐\n",
    "│    Frontend     │      │    Backend      │\n",
    "│    (React)      │─────►│    (Flask)      │\n",
    "│   Port: 3000    │      │   Port: 5000    │\n",
    "└─────────────────┘      └─────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backend API (Flask)\n",
    "\n",
    "**backend/app.py**:\n",
    "```python\n",
    "from flask import Flask, jsonify, request\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Enable Cross-Origin requests\n",
    "\n",
    "# In-memory data store\n",
    "tasks = [\n",
    "    {\"id\": 1, \"title\": \"Learn Docker\", \"completed\": True},\n",
    "    {\"id\": 2, \"title\": \"Build a pipeline\", \"completed\": False}\n",
    "]\n",
    "\n",
    "@app.route('/api/tasks', methods=['GET'])\n",
    "def get_tasks():\n",
    "    return jsonify(tasks)\n",
    "\n",
    "@app.route('/api/tasks', methods=['POST'])\n",
    "def add_task():\n",
    "    data = request.json\n",
    "    new_task = {\n",
    "        \"id\": len(tasks) + 1,\n",
    "        \"title\": data['title'],\n",
    "        \"completed\": False\n",
    "    }\n",
    "    tasks.append(new_task)\n",
    "    return jsonify(new_task), 201\n",
    "\n",
    "@app.route('/api/tasks/<int:task_id>', methods=['PUT'])\n",
    "def update_task(task_id):\n",
    "    task = next((t for t in tasks if t['id'] == task_id), None)\n",
    "    if task:\n",
    "        data = request.json\n",
    "        task['completed'] = data.get('completed', task['completed'])\n",
    "        return jsonify(task)\n",
    "    return jsonify({\"error\": \"Task not found\"}), 404\n",
    "\n",
    "@app.route('/api/tasks/<int:task_id>', methods=['DELETE'])\n",
    "def delete_task(task_id):\n",
    "    global tasks\n",
    "    tasks = [t for t in tasks if t['id'] != task_id]\n",
    "    return '', 204\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Compose for Full Stack\n",
    "\n",
    "**docker-compose.yml**:\n",
    "```yaml\n",
    "version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  frontend:\n",
    "    build:\n",
    "      context: ./frontend\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    depends_on:\n",
    "      - backend\n",
    "    environment:\n",
    "      - REACT_APP_API_URL=http://localhost:5000\n",
    "\n",
    "  backend:\n",
    "    build:\n",
    "      context: ./backend\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    volumes:\n",
    "      - ./backend:/app\n",
    "    environment:\n",
    "      - FLASK_ENV=development\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Nginx as Reverse Proxy\n",
    "\n",
    "For production deployments, use Nginx as a reverse proxy:\n",
    "\n",
    "**nginx.conf**:\n",
    "```nginx\n",
    "upstream frontend {\n",
    "    server frontend:3000;\n",
    "}\n",
    "\n",
    "upstream backend {\n",
    "    server backend:5000;\n",
    "}\n",
    "\n",
    "server {\n",
    "    listen 80;\n",
    "\n",
    "    location / {\n",
    "        proxy_pass http://frontend;\n",
    "        proxy_http_version 1.1;\n",
    "        proxy_set_header Upgrade $http_upgrade;\n",
    "        proxy_set_header Connection \"upgrade\";\n",
    "    }\n",
    "\n",
    "    location /api {\n",
    "        proxy_pass http://backend;\n",
    "        proxy_set_header Host $host;\n",
    "        proxy_set_header X-Real-IP $remote_addr;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**docker-compose.prod.yml**:\n",
    "```yaml\n",
    "version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  nginx:\n",
    "    image: nginx:alpine\n",
    "    ports:\n",
    "      - \"80:80\"\n",
    "    volumes:\n",
    "      - ./nginx.conf:/etc/nginx/conf.d/default.conf\n",
    "    depends_on:\n",
    "      - frontend\n",
    "      - backend\n",
    "\n",
    "  frontend:\n",
    "    build:\n",
    "      context: ./frontend\n",
    "      dockerfile: Dockerfile.prod\n",
    "\n",
    "  backend:\n",
    "    build:\n",
    "      context: ./backend\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercise 7\n",
    "\n",
    "**Q7.1** Extend the frontend-backend example to include:\n",
    "- User authentication (login/logout)\n",
    "- Protected routes\n",
    "- JWT token handling\n",
    "\n",
    "**Q7.2** Add a database to the stack:\n",
    "- Replace in-memory storage with PostgreSQL\n",
    "- Add database migrations\n",
    "- Implement data persistence across restarts\n",
    "\n",
    "**Q7.3** Create a data visualization dashboard:\n",
    "- Backend API that serves analytics data\n",
    "- Frontend with charts (using Chart.js or similar)\n",
    "- Real-time updates using WebSockets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 8: Scaling and Monitoring Containers [★★★]\n",
    "\n",
    "### Container Scaling\n",
    "\n",
    "```bash\n",
    "# Scale a specific service\n",
    "docker-compose up --scale worker=5\n",
    "\n",
    "# View running containers\n",
    "docker-compose ps\n",
    "\n",
    "# View resource usage\n",
    "docker stats\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Balancing with Nginx\n",
    "\n",
    "**docker-compose.yml**:\n",
    "```yaml\n",
    "version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  nginx:\n",
    "    image: nginx:alpine\n",
    "    ports:\n",
    "      - \"80:80\"\n",
    "    volumes:\n",
    "      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n",
    "    depends_on:\n",
    "      - api\n",
    "\n",
    "  api:\n",
    "    build: .\n",
    "    # No ports exposed - accessed through nginx\n",
    "    deploy:\n",
    "      replicas: 3\n",
    "```\n",
    "\n",
    "**nginx.conf** for load balancing:\n",
    "```nginx\n",
    "events {\n",
    "    worker_connections 1024;\n",
    "}\n",
    "\n",
    "http {\n",
    "    upstream api_servers {\n",
    "        least_conn;  # Load balancing method\n",
    "        server api:5000;\n",
    "    }\n",
    "\n",
    "    server {\n",
    "        listen 80;\n",
    "\n",
    "        location / {\n",
    "            proxy_pass http://api_servers;\n",
    "            proxy_set_header Host $host;\n",
    "            proxy_set_header X-Real-IP $remote_addr;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring with Prometheus and Grafana\n",
    "\n",
    "**docker-compose.monitoring.yml**:\n",
    "```yaml\n",
    "version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  prometheus:\n",
    "    image: prom/prometheus\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "      - prometheus_data:/prometheus\n",
    "\n",
    "  grafana:\n",
    "    image: grafana/grafana\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    volumes:\n",
    "      - grafana_data:/var/lib/grafana\n",
    "    environment:\n",
    "      - GF_SECURITY_ADMIN_PASSWORD=admin\n",
    "\n",
    "  cadvisor:\n",
    "    image: gcr.io/cadvisor/cadvisor\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "    volumes:\n",
    "      - /:/rootfs:ro\n",
    "      - /var/run:/var/run:ro\n",
    "      - /sys:/sys:ro\n",
    "      - /var/lib/docker/:/var/lib/docker:ro\n",
    "\n",
    "volumes:\n",
    "  prometheus_data:\n",
    "  grafana_data:\n",
    "```\n",
    "\n",
    "**prometheus.yml**:\n",
    "```yaml\n",
    "global:\n",
    "  scrape_interval: 15s\n",
    "\n",
    "scrape_configs:\n",
    "  - job_name: 'prometheus'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:9090']\n",
    "\n",
    "  - job_name: 'cadvisor'\n",
    "    static_configs:\n",
    "      - targets: ['cadvisor:8080']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Limits\n",
    "\n",
    "```yaml\n",
    "version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  api:\n",
    "    build: .\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          cpus: '0.50'\n",
    "          memory: 512M\n",
    "        reservations:\n",
    "          cpus: '0.25'\n",
    "          memory: 256M\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercise 8\n",
    "\n",
    "**Q8.1** Create a scalable data processing pipeline:\n",
    "- Producer service generating data\n",
    "- Worker services that can be scaled (1-10 instances)\n",
    "- Load balancer distributing work\n",
    "- Measure throughput with different numbers of workers\n",
    "\n",
    "**Q8.2** Set up monitoring for your application:\n",
    "- Configure Prometheus to collect metrics\n",
    "- Create Grafana dashboards for:\n",
    "  - CPU and memory usage\n",
    "  - Request rates and latencies\n",
    "  - Error rates\n",
    "\n",
    "**Q8.3** Implement auto-scaling simulation:\n",
    "- Monitor CPU usage of worker containers\n",
    "- Create a script that scales workers based on load\n",
    "- Test with varying load patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this practical, you learned:\n",
    "\n",
    "1. **Docker Fundamentals**: Images, containers, and basic commands\n",
    "2. **Dockerfiles**: Writing efficient Dockerfiles for Python applications\n",
    "3. **Docker Compose**: Orchestrating multi-container applications\n",
    "4. **Shared Volumes**: Building data pipelines with file-based communication\n",
    "5. **Message Queues**: Producer-consumer patterns with RabbitMQ\n",
    "6. **Database Integration**: Connecting applications to PostgreSQL\n",
    "7. **Frontend-Backend**: Building full-stack applications\n",
    "8. **Scaling and Monitoring**: Load balancing and observability\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Use Docker Compose for development and testing\n",
    "- Implement proper health checks for service dependencies\n",
    "- Use volumes for data persistence\n",
    "- Choose the right communication pattern (files, messages, API)\n",
    "- Monitor and scale based on metrics\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In Practical 7, you will learn about Kubernetes for:\n",
    "- Production-grade container orchestration\n",
    "- Declarative configuration management\n",
    "- Automatic scaling and self-healing\n",
    "- Service discovery and load balancing\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Docker Documentation](https://docs.docker.com/)\n",
    "- [Docker Compose Documentation](https://docs.docker.com/compose/)\n",
    "- [Best practices for writing Dockerfiles](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/)\n",
    "- [Docker Security Best Practices](https://docs.docker.com/develop/security-best-practices/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
