{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 5: Apache Spark for Massive Data Processing\n",
    "\n",
    "## Goals\n",
    "\n",
    "This practical session introduces Apache Spark, a powerful distributed computing framework designed for large-scale data processing. You will learn how to leverage Spark's capabilities for handling massive datasets that exceed single-machine memory limits.\n",
    "\n",
    "### Learning Objectives\n",
    "* Understand Spark architecture: Driver, Executors, and Cluster Manager\n",
    "* Master RDD (Resilient Distributed Dataset) operations\n",
    "* Work with DataFrames and understand schema management\n",
    "* Write efficient Spark SQL queries\n",
    "* Implement joins, window functions, and complex aggregations\n",
    "* Optimize performance through partitioning and caching strategies\n",
    "* Process large-scale datasets with columnar file formats (Parquet, ORC)\n",
    "\n",
    "### Prerequisites\n",
    "* Completion of Practical 4 (Parallel and Distributed Computing)\n",
    "* Understanding of functional programming concepts (map, filter, reduce)\n",
    "* Basic SQL knowledge\n",
    "* Python programming fundamentals\n",
    "\n",
    "### Installation\n",
    "\n",
    "Install PySpark before starting:\n",
    "```bash\n",
    "!pip install pyspark==3.5.3\n",
    "```\n",
    "\n",
    "### Exercises Overview\n",
    "\n",
    "| Exercise | Topic | Difficulty |\n",
    "|----------|-------|------------|\n",
    "| 1 | Spark Architecture and RDD Basics | ★ |\n",
    "| 2 | RDD Transformations and Actions | ★ |\n",
    "| 3 | DataFrames and Schema Management | ★★ |\n",
    "| 4 | Spark SQL and Complex Queries | ★★ |\n",
    "| 5 | Joins, Window Functions, and Aggregations | ★★ |\n",
    "| 6 | Partitioning, Caching, and Optimization | ★★★ |\n",
    "| 7 | Processing Large-Scale Datasets | ★★★ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Spark Architecture and RDD Basics [★]\n",
    "\n",
    "### Understanding Spark Architecture\n",
    "\n",
    "Apache Spark uses a **master-worker architecture**:\n",
    "\n",
    "1. **Driver Program**: The main program that creates the SparkContext and coordinates execution\n",
    "2. **Cluster Manager**: Allocates resources (can be Standalone, YARN, Mesos, or Kubernetes)\n",
    "3. **Executors**: Worker processes that run tasks and store data\n",
    "\n",
    "```\n",
    "┌─────────────────┐\n",
    "│  Driver Program │\n",
    "│  (SparkContext) │\n",
    "└────────┬────────┘\n",
    "         │\n",
    "         ▼\n",
    "┌─────────────────┐\n",
    "│ Cluster Manager │\n",
    "└────────┬────────┘\n",
    "         │\n",
    "    ┌────┴────┐\n",
    "    ▼         ▼\n",
    "┌───────┐ ┌───────┐\n",
    "│Executor│ │Executor│\n",
    "│ Task   │ │ Task   │\n",
    "│ Task   │ │ Task   │\n",
    "└───────┘ └───────┘\n",
    "```\n",
    "\n",
    "### RDD (Resilient Distributed Dataset)\n",
    "\n",
    "RDDs are the fundamental data structure in Spark:\n",
    "- **Resilient**: Fault-tolerant through lineage information\n",
    "- **Distributed**: Data is partitioned across multiple nodes\n",
    "- **Dataset**: Collection of elements that can be operated on in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify PySpark installation (already installed via requirements.txt)\n",
    "!pip install pyspark==3.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's verify PySpark installation\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "print(\"PySpark imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkContext with a local configuration\n",
    "# 'local[*]' uses all available cores\n",
    "conf = SparkConf().setAppName(\"Practical5\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "# Display Spark configuration\n",
    "print(f\"Spark Version: {sc.version}\")\n",
    "print(f\"Application Name: {sc.appName}\")\n",
    "print(f\"Master: {sc.master}\")\n",
    "print(f\"Default Parallelism: {sc.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating RDDs - Method 1: From a Python collection (parallelize)\n",
    "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "numbers_rdd = sc.parallelize(numbers)\n",
    "\n",
    "print(f\"Type: {type(numbers_rdd)}\")\n",
    "print(f\"Number of partitions: {numbers_rdd.getNumPartitions()}\")\n",
    "print(f\"First element: {numbers_rdd.first()}\")\n",
    "print(f\"All elements: {numbers_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating RDDs - Method 2: From external files\n",
    "lines_rdd = sc.textFile(\"../shared_data/pl.csv\")\n",
    "\n",
    "print(f\"Number of partitions: {lines_rdd.getNumPartitions()}\")\n",
    "print(f\"Number of lines: {lines_rdd.count()}\")\n",
    "print(f\"\\nFirst 5 lines:\")\n",
    "for line in lines_rdd.take(5):\n",
    "    print(f\"  {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating RDDs with specific number of partitions\n",
    "# More partitions = more parallelism (but also more overhead)\n",
    "numbers_4partitions = sc.parallelize(range(1, 101), numSlices=4)\n",
    "numbers_8partitions = sc.parallelize(range(1, 101), numSlices=8)\n",
    "\n",
    "print(f\"RDD with 4 partitions: {numbers_4partitions.getNumPartitions()}\")\n",
    "print(f\"RDD with 8 partitions: {numbers_8partitions.getNumPartitions()}\")\n",
    "\n",
    "# View how data is distributed across partitions\n",
    "print(f\"\\nData distribution (4 partitions):\")\n",
    "print(numbers_4partitions.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercise 1\n",
    "\n",
    "**Q1.1** Create an RDD containing the squares of numbers from 1 to 1000. Experiment with different numbers of partitions (2, 4, 8, 16) and explain how partitioning affects data distribution.\n",
    "\n",
    "**Q1.2** Load multiple CSV files from the `shared_data/` directory using wildcard patterns (e.g., `*.csv`). How many total lines are there across all files?\n",
    "\n",
    "**Q1.3** Explain the difference between creating an RDD with `parallelize()` vs `textFile()`. When would you use each method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solutions here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: RDD Transformations and Actions [★]\n",
    "\n",
    "### Transformations vs Actions\n",
    "\n",
    "Spark operations are divided into two categories:\n",
    "\n",
    "**Transformations** (Lazy - create new RDDs):\n",
    "- `map()`, `filter()`, `flatMap()`, `distinct()`\n",
    "- `union()`, `intersection()`, `subtract()`\n",
    "- `groupByKey()`, `reduceByKey()`, `sortByKey()`\n",
    "\n",
    "**Actions** (Eager - return results):\n",
    "- `collect()`, `count()`, `first()`, `take()`\n",
    "- `reduce()`, `fold()`, `aggregate()`\n",
    "- `saveAsTextFile()`, `foreach()`\n",
    "\n",
    "### Lazy Evaluation\n",
    "\n",
    "Transformations are **lazy**: Spark builds a DAG (Directed Acyclic Graph) of operations but doesn't execute them until an action is called. This allows Spark to optimize the execution plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of lazy evaluation\n",
    "import time\n",
    "\n",
    "# This transformation is NOT executed immediately\n",
    "start = time.time()\n",
    "large_rdd = sc.parallelize(range(1, 1000001))\n",
    "squared = large_rdd.map(lambda x: x ** 2)\n",
    "filtered = squared.filter(lambda x: x % 2 == 0)\n",
    "print(f\"Transformations defined in: {time.time() - start:.4f} seconds\")\n",
    "\n",
    "# The action triggers execution\n",
    "start = time.time()\n",
    "result = filtered.count()\n",
    "print(f\"Action executed in: {time.time() - start:.4f} seconds\")\n",
    "print(f\"Count of even squares: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map transformation: apply a function to each element\n",
    "words = sc.parallelize([\"hello\", \"world\", \"spark\", \"python\"])\n",
    "\n",
    "# Transform to uppercase\n",
    "upper_words = words.map(lambda w: w.upper())\n",
    "print(f\"Uppercase: {upper_words.collect()}\")\n",
    "\n",
    "# Transform to (word, length) tuples\n",
    "word_lengths = words.map(lambda w: (w, len(w)))\n",
    "print(f\"Word lengths: {word_lengths.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter transformation: keep elements that satisfy a condition\n",
    "numbers = sc.parallelize(range(1, 21))\n",
    "\n",
    "# Keep only even numbers\n",
    "evens = numbers.filter(lambda x: x % 2 == 0)\n",
    "print(f\"Even numbers: {evens.collect()}\")\n",
    "\n",
    "# Keep numbers divisible by 3\n",
    "div_by_3 = numbers.filter(lambda x: x % 3 == 0)\n",
    "print(f\"Divisible by 3: {div_by_3.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FlatMap transformation: map that can return multiple elements\n",
    "sentences = sc.parallelize([\n",
    "    \"Hello World\",\n",
    "    \"Apache Spark is powerful\",\n",
    "    \"Big Data processing\"\n",
    "])\n",
    "\n",
    "# Split sentences into words\n",
    "words = sentences.flatMap(lambda s: s.split())\n",
    "print(f\"All words: {words.collect()}\")\n",
    "print(f\"Word count: {words.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce action: aggregate elements using a function\n",
    "numbers = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Sum all numbers\n",
    "total = numbers.reduce(lambda a, b: a + b)\n",
    "print(f\"Sum: {total}\")\n",
    "\n",
    "# Find maximum\n",
    "maximum = numbers.reduce(lambda a, b: a if a > b else b)\n",
    "print(f\"Maximum: {maximum}\")\n",
    "\n",
    "# Product of all numbers\n",
    "product = numbers.reduce(lambda a, b: a * b)\n",
    "print(f\"Product: {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count - The classic Spark example\n",
    "text = sc.parallelize([\n",
    "    \"Apache Spark is a unified analytics engine\",\n",
    "    \"Spark provides high-level APIs in Java Scala Python and R\",\n",
    "    \"Spark powers a stack of libraries for SQL streaming and machine learning\",\n",
    "    \"Spark runs on Hadoop YARN Mesos Kubernetes and standalone\"\n",
    "])\n",
    "\n",
    "# Word count pipeline\n",
    "word_counts = (text\n",
    "    .flatMap(lambda line: line.lower().split())  # Split into words\n",
    "    .map(lambda word: (word, 1))                  # Map to (word, 1) pairs\n",
    "    .reduceByKey(lambda a, b: a + b)              # Sum counts per word\n",
    "    .sortBy(lambda x: x[1], ascending=False))    # Sort by count\n",
    "\n",
    "print(\"Word counts (top 10):\")\n",
    "for word, count in word_counts.take(10):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set operations on RDDs\n",
    "rdd1 = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd2 = sc.parallelize([4, 5, 6, 7, 8])\n",
    "\n",
    "# Union (all elements from both)\n",
    "print(f\"Union: {rdd1.union(rdd2).collect()}\")\n",
    "\n",
    "# Intersection (common elements)\n",
    "print(f\"Intersection: {rdd1.intersection(rdd2).collect()}\")\n",
    "\n",
    "# Subtract (elements in rdd1 but not in rdd2)\n",
    "print(f\"Subtract: {rdd1.subtract(rdd2).collect()}\")\n",
    "\n",
    "# Distinct (unique elements)\n",
    "print(f\"Distinct union: {rdd1.union(rdd2).distinct().collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercise 2\n",
    "\n",
    "**Q2.1** Load the file `shared_data/pl.csv` and perform the following:\n",
    "- Count the total number of characters across all lines\n",
    "- Count the total number of tokens (comma-separated values)\n",
    "- Find the line with the maximum number of characters\n",
    "\n",
    "**Q2.2** Download 50 HTML pages from the web. Write a Spark program to:\n",
    "- Count the total number of `<div>` and `</div>` tags across all files\n",
    "- Find the page with the most `<a>` (anchor) tags\n",
    "- Extract and count all unique CSS class names\n",
    "\n",
    "**Q2.3** Implement a character frequency counter that:\n",
    "- Reads all text files from a directory\n",
    "- Counts the frequency of each character (case-insensitive)\n",
    "- Returns the top 10 most frequent characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solutions here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: DataFrames and Schema Management [★★]\n",
    "\n",
    "### DataFrames\n",
    "\n",
    "DataFrames are a higher-level abstraction built on top of RDDs:\n",
    "- Organized into named columns (like a table)\n",
    "- Optimized through Catalyst query optimizer\n",
    "- Support for structured and semi-structured data\n",
    "- Better performance than RDDs for most operations\n",
    "\n",
    "### SparkSession\n",
    "\n",
    "SparkSession is the entry point for DataFrame operations (introduced in Spark 2.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Practical5-DataFrames\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Session created: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrames - Method 1: From Python list with inferred schema\n",
    "data = [\n",
    "    (\"Python\", 1991, \"Guido van Rossum\"),\n",
    "    (\"Java\", 1995, \"James Gosling\"),\n",
    "    (\"JavaScript\", 1995, \"Brendan Eich\"),\n",
    "    (\"C\", 1972, \"Dennis Ritchie\"),\n",
    "    (\"Rust\", 2010, \"Graydon Hoare\")\n",
    "]\n",
    "\n",
    "df_inferred = spark.createDataFrame(data, [\"language\", \"year\", \"creator\"])\n",
    "df_inferred.show()\n",
    "df_inferred.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrames - Method 2: With explicit schema\n",
    "schema = StructType([\n",
    "    StructField(\"language\", StringType(), nullable=False),\n",
    "    StructField(\"year\", IntegerType(), nullable=False),\n",
    "    StructField(\"creator\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "df_explicit = spark.createDataFrame(data, schema)\n",
    "df_explicit.show()\n",
    "df_explicit.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrames - Method 3: From JSON file\n",
    "df_json = spark.read.json(\"../shared_data/pl.json\")\n",
    "df_json.show(10)\n",
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrames - Method 4: From CSV with options\n",
    "df_csv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"../shared_data/pl.csv\")\n",
    "\n",
    "df_csv.show(10)\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame basic operations\n",
    "df = df_json\n",
    "\n",
    "# Show first n rows\n",
    "print(\"First 5 rows:\")\n",
    "df.show(5)\n",
    "\n",
    "# Get column names\n",
    "print(f\"\\nColumns: {df.columns}\")\n",
    "\n",
    "# Get number of rows and columns\n",
    "print(f\"Shape: ({df.count()}, {len(df.columns)})\")\n",
    "\n",
    "# Describe statistics\n",
    "print(\"\\nStatistics:\")\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column selection and manipulation\n",
    "from pyspark.sql.functions import col, lit, when, upper, lower, length\n",
    "\n",
    "# Select specific columns\n",
    "df.select(\"languageLabel\").show(5)\n",
    "\n",
    "# Select with column expressions\n",
    "df.select(\n",
    "    col(\"languageLabel\"),\n",
    "    col(\"year\"),\n",
    "    (col(\"year\") - 1900).alias(\"years_since_1900\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding and modifying columns\n",
    "df_modified = df \\\n",
    "    .withColumn(\"century\", ((col(\"year\") / 100) + 1).cast(IntegerType())) \\\n",
    "    .withColumn(\"language_upper\", upper(col(\"languageLabel\"))) \\\n",
    "    .withColumn(\"name_length\", length(col(\"languageLabel\")))\n",
    "\n",
    "df_modified.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering rows\n",
    "# Languages created after 2000\n",
    "recent = df.filter(col(\"year\") > 2000)\n",
    "print(f\"Languages after 2000: {recent.count()}\")\n",
    "recent.show(10)\n",
    "\n",
    "# Multiple conditions\n",
    "filtered = df.filter((col(\"year\") >= 1990) & (col(\"year\") <= 2000))\n",
    "print(f\"\\nLanguages from 1990-2000: {filtered.count()}\")\n",
    "filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy and aggregations\n",
    "from pyspark.sql.functions import count, avg, min as spark_min, max as spark_max, sum as spark_sum\n",
    "\n",
    "# Count languages per year\n",
    "df.groupBy(\"year\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple aggregations\n",
    "df_modified.groupBy(\"century\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_languages\"),\n",
    "        spark_min(\"year\").alias(\"earliest_year\"),\n",
    "        spark_max(\"year\").alias(\"latest_year\"),\n",
    "        avg(\"name_length\").alias(\"avg_name_length\")\n",
    "    ) \\\n",
    "    .orderBy(\"century\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercise 3\n",
    "\n",
    "**Q3.1** Query Wikidata to download information about all software applications including: name, release date, developer, and programming language used. Load this data into a Spark DataFrame with an explicit schema.\n",
    "\n",
    "**Q3.2** Using the programming languages DataFrame:\n",
    "- Add a column categorizing languages as \"Early\" (before 1980), \"Classic\" (1980-2000), or \"Modern\" (after 2000)\n",
    "- Calculate statistics (count, min year, max year) for each category\n",
    "- Find the decade with the most language releases\n",
    "\n",
    "**Q3.3** Create a DataFrame from a CSV file with proper handling of:\n",
    "- Missing values (nulls)\n",
    "- Date parsing\n",
    "- Custom delimiters\n",
    "- Escaped characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solutions here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Spark SQL and Complex Queries [★★]\n",
    "\n",
    "### SQL Interface\n",
    "\n",
    "Spark SQL allows you to run SQL queries directly on DataFrames using temporary views. This is useful for:\n",
    "- Complex queries that are easier to express in SQL\n",
    "- Interoperability with SQL-based tools\n",
    "- Familiarity for SQL users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and create a temporary view\n",
    "df = spark.read.json(\"../shared_data/pl.json\")\n",
    "df.createOrReplaceTempView(\"languages\")\n",
    "\n",
    "# Basic SELECT query\n",
    "spark.sql(\"SELECT * FROM languages LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering with WHERE clause\n",
    "spark.sql(\"\"\"\n",
    "    SELECT languageLabel, year \n",
    "    FROM languages \n",
    "    WHERE year >= 1990 AND year <= 2000\n",
    "    ORDER BY year\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation queries\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        year,\n",
    "        COUNT(*) as language_count\n",
    "    FROM languages\n",
    "    GROUP BY year\n",
    "    HAVING COUNT(*) > 3\n",
    "    ORDER BY language_count DESC\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CASE expressions\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        languageLabel,\n",
    "        year,\n",
    "        CASE \n",
    "            WHEN year < 1980 THEN 'Pioneer Era'\n",
    "            WHEN year < 1990 THEN '1980s'\n",
    "            WHEN year < 2000 THEN '1990s'\n",
    "            WHEN year < 2010 THEN '2000s'\n",
    "            ELSE '2010s+'\n",
    "        END as era\n",
    "    FROM languages\n",
    "    ORDER BY year\n",
    "\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subqueries\n",
    "spark.sql(\"\"\"\n",
    "    SELECT languageLabel, year\n",
    "    FROM languages\n",
    "    WHERE year = (\n",
    "        SELECT MAX(year) FROM languages\n",
    "    )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Table Expressions (CTEs)\n",
    "spark.sql(\"\"\"\n",
    "    WITH decade_stats AS (\n",
    "        SELECT \n",
    "            FLOOR(year / 10) * 10 as decade,\n",
    "            COUNT(*) as count\n",
    "        FROM languages\n",
    "        GROUP BY FLOOR(year / 10) * 10\n",
    "    )\n",
    "    SELECT \n",
    "        decade,\n",
    "        count,\n",
    "        ROUND(count * 100.0 / SUM(count) OVER(), 2) as percentage\n",
    "    FROM decade_stats\n",
    "    ORDER BY decade\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional tables for join examples\n",
    "# Paradigm data\n",
    "paradigms_data = [\n",
    "    (\"Python\", \"Multi-paradigm\"),\n",
    "    (\"Java\", \"Object-oriented\"),\n",
    "    (\"JavaScript\", \"Multi-paradigm\"),\n",
    "    (\"Haskell\", \"Functional\"),\n",
    "    (\"C\", \"Procedural\"),\n",
    "    (\"Rust\", \"Multi-paradigm\"),\n",
    "    (\"Lisp\", \"Functional\"),\n",
    "    (\"Prolog\", \"Logic\")\n",
    "]\n",
    "\n",
    "paradigms_df = spark.createDataFrame(paradigms_data, [\"language\", \"paradigm\"])\n",
    "paradigms_df.createOrReplaceTempView(\"paradigms\")\n",
    "\n",
    "# Typing data\n",
    "typing_data = [\n",
    "    (\"Python\", \"Dynamic\", \"Strong\"),\n",
    "    (\"Java\", \"Static\", \"Strong\"),\n",
    "    (\"JavaScript\", \"Dynamic\", \"Weak\"),\n",
    "    (\"C\", \"Static\", \"Weak\"),\n",
    "    (\"Rust\", \"Static\", \"Strong\"),\n",
    "    (\"Haskell\", \"Static\", \"Strong\")\n",
    "]\n",
    "\n",
    "typing_df = spark.createDataFrame(typing_data, [\"language\", \"typing\", \"type_safety\"])\n",
    "typing_df.createOrReplaceTempView(\"typing\")\n",
    "\n",
    "print(\"Paradigms:\")\n",
    "paradigms_df.show()\n",
    "print(\"Typing:\")\n",
    "typing_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INNER JOIN\n",
    "spark.sql(\"\"\"\n",
    "    SELECT p.language, p.paradigm, t.typing, t.type_safety\n",
    "    FROM paradigms p\n",
    "    INNER JOIN typing t ON p.language = t.language\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEFT JOIN\n",
    "spark.sql(\"\"\"\n",
    "    SELECT p.language, p.paradigm, t.typing\n",
    "    FROM paradigms p\n",
    "    LEFT JOIN typing t ON p.language = t.language\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercise 4\n",
    "\n",
    "**Q4.1** Using the languages view, write SQL queries to:\n",
    "- Find all languages released in the same year as Python (1991)\n",
    "- Calculate the average number of languages released per decade\n",
    "- Find years where more than 5 languages were released\n",
    "\n",
    "**Q4.2** Create two new views from Wikidata:\n",
    "- Software applications with their developers\n",
    "- Developers with their countries\n",
    "Write a query joining these tables to show software grouped by country.\n",
    "\n",
    "**Q4.3** Using window functions, write queries to:\n",
    "- Rank languages by year within each decade\n",
    "- Calculate the running total of languages released over time\n",
    "- Find the first and last language released each decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solutions here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: Joins, Window Functions, and Aggregations [★★]\n",
    "\n",
    "### Advanced DataFrame Operations\n",
    "\n",
    "This exercise covers more complex operations that are essential for real-world data processing:\n",
    "- Different types of joins\n",
    "- Window functions for analytics\n",
    "- Complex aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, row_number, rank, dense_rank, lag, lead, sum as spark_sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create sample sales data\n",
    "sales_data = [\n",
    "    (\"2024-01-15\", \"Electronics\", \"Laptop\", 1200.00, 5),\n",
    "    (\"2024-01-15\", \"Electronics\", \"Phone\", 800.00, 10),\n",
    "    (\"2024-01-16\", \"Electronics\", \"Laptop\", 1200.00, 3),\n",
    "    (\"2024-01-16\", \"Clothing\", \"Shirt\", 50.00, 20),\n",
    "    (\"2024-01-17\", \"Electronics\", \"Tablet\", 500.00, 8),\n",
    "    (\"2024-01-17\", \"Clothing\", \"Pants\", 80.00, 15),\n",
    "    (\"2024-01-18\", \"Clothing\", \"Shirt\", 50.00, 25),\n",
    "    (\"2024-01-18\", \"Electronics\", \"Phone\", 800.00, 12),\n",
    "    (\"2024-01-19\", \"Books\", \"Fiction\", 25.00, 30),\n",
    "    (\"2024-01-19\", \"Books\", \"Technical\", 60.00, 10),\n",
    "]\n",
    "\n",
    "sales_schema = [\"date\", \"category\", \"product\", \"price\", \"quantity\"]\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "\n",
    "# Add calculated column\n",
    "sales_df = sales_df.withColumn(\"revenue\", col(\"price\") * col(\"quantity\"))\n",
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window function: Row number within each category\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(col(\"revenue\").desc())\n",
    "\n",
    "ranked_sales = sales_df.withColumn(\"rank_in_category\", row_number().over(window_spec))\n",
    "ranked_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different ranking functions\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(col(\"revenue\").desc())\n",
    "\n",
    "sales_df.select(\n",
    "    \"category\",\n",
    "    \"product\",\n",
    "    \"revenue\",\n",
    "    row_number().over(window_spec).alias(\"row_number\"),\n",
    "    rank().over(window_spec).alias(\"rank\"),\n",
    "    dense_rank().over(window_spec).alias(\"dense_rank\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running totals with window functions\n",
    "window_running = Window.partitionBy(\"category\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "sales_df.select(\n",
    "    \"date\",\n",
    "    \"category\",\n",
    "    \"revenue\",\n",
    "    spark_sum(\"revenue\").over(window_running).alias(\"running_total\")\n",
    ").orderBy(\"category\", \"date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag and Lead functions (previous/next row values)\n",
    "window_ordered = Window.partitionBy(\"category\").orderBy(\"date\")\n",
    "\n",
    "sales_df.select(\n",
    "    \"date\",\n",
    "    \"category\",\n",
    "    \"revenue\",\n",
    "    lag(\"revenue\", 1).over(window_ordered).alias(\"prev_revenue\"),\n",
    "    lead(\"revenue\", 1).over(window_ordered).alias(\"next_revenue\")\n",
    ").orderBy(\"category\", \"date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex aggregations with pivot\n",
    "pivot_df = sales_df.groupBy(\"date\").pivot(\"category\").sum(\"revenue\")\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple aggregations at once\n",
    "from pyspark.sql.functions import round as spark_round\n",
    "\n",
    "sales_df.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"num_transactions\"),\n",
    "    spark_round(avg(\"revenue\"), 2).alias(\"avg_revenue\"),\n",
    "    spark_round(spark_min(\"revenue\"), 2).alias(\"min_revenue\"),\n",
    "    spark_round(spark_max(\"revenue\"), 2).alias(\"max_revenue\"),\n",
    "    spark_round(spark_sum(\"revenue\"), 2).alias(\"total_revenue\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional DataFrames for join examples\n",
    "# Customer data\n",
    "customers_data = [\n",
    "    (1, \"Alice\", \"Paris\"),\n",
    "    (2, \"Bob\", \"London\"),\n",
    "    (3, \"Charlie\", \"Berlin\"),\n",
    "    (4, \"Diana\", \"Madrid\")\n",
    "]\n",
    "customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"name\", \"city\"])\n",
    "\n",
    "# Orders data\n",
    "orders_data = [\n",
    "    (101, 1, \"2024-01-15\", 150.00),\n",
    "    (102, 1, \"2024-01-16\", 200.00),\n",
    "    (103, 2, \"2024-01-15\", 300.00),\n",
    "    (104, 3, \"2024-01-17\", 450.00),\n",
    "    (105, 5, \"2024-01-18\", 100.00)  # Customer 5 doesn't exist\n",
    "]\n",
    "orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"customer_id\", \"order_date\", \"amount\"])\n",
    "\n",
    "print(\"Customers:\")\n",
    "customers_df.show()\n",
    "print(\"Orders:\")\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different types of joins\n",
    "\n",
    "# Inner join (only matching rows)\n",
    "print(\"INNER JOIN:\")\n",
    "customers_df.join(orders_df, \"customer_id\", \"inner\").show()\n",
    "\n",
    "# Left join (all customers, matching orders)\n",
    "print(\"LEFT JOIN:\")\n",
    "customers_df.join(orders_df, \"customer_id\", \"left\").show()\n",
    "\n",
    "# Right join (all orders, matching customers)\n",
    "print(\"RIGHT JOIN:\")\n",
    "customers_df.join(orders_df, \"customer_id\", \"right\").show()\n",
    "\n",
    "# Full outer join (all rows from both)\n",
    "print(\"FULL OUTER JOIN:\")\n",
    "customers_df.join(orders_df, \"customer_id\", \"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anti join (rows from left that don't match right)\n",
    "print(\"Customers without orders (LEFT ANTI):\")\n",
    "customers_df.join(orders_df, \"customer_id\", \"left_anti\").show()\n",
    "\n",
    "# Semi join (rows from left that have a match in right, but don't include right columns)\n",
    "print(\"Customers with orders (LEFT SEMI):\")\n",
    "customers_df.join(orders_df, \"customer_id\", \"left_semi\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercise 5\n",
    "\n",
    "**Q5.1** Using the sales data, calculate:\n",
    "- The percentage of total revenue contributed by each category\n",
    "- The top-selling product in each category\n",
    "- Daily revenue growth rate (percentage change from previous day)\n",
    "\n",
    "**Q5.2** Create a customer segmentation based on their total spending:\n",
    "- \"Bronze\": total spending < 200\n",
    "- \"Silver\": total spending 200-500\n",
    "- \"Gold\": total spending > 500\n",
    "Show the count of customers in each segment.\n",
    "\n",
    "**Q5.3** Download data from Wikidata about:\n",
    "- Countries and their populations\n",
    "- Cities and their countries\n",
    "- Universities and their cities\n",
    "Perform joins to find the top 10 countries by number of universities, normalized by population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solutions here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 6: Partitioning, Caching, and Optimization [★★★]\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "Spark performance depends heavily on:\n",
    "1. **Partitioning**: How data is distributed across nodes\n",
    "2. **Caching**: Keeping frequently accessed data in memory\n",
    "3. **Avoiding shuffles**: Minimizing data movement between nodes\n",
    "4. **Broadcast variables**: Efficiently sharing small data across nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding partitions\n",
    "large_df = spark.range(1000000)  # 1 million rows\n",
    "\n",
    "print(f\"Default partitions: {large_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition to specific number\n",
    "repartitioned = large_df.repartition(8)\n",
    "print(f\"After repartition(8): {repartitioned.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Coalesce (reduce partitions without full shuffle)\n",
    "coalesced = large_df.coalesce(4)\n",
    "print(f\"After coalesce(4): {coalesced.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition by column (useful for filtering)\n",
    "sales_partitioned = sales_df.repartition(4, \"category\")\n",
    "print(f\"Partitions: {sales_partitioned.rdd.getNumPartitions()}\")\n",
    "\n",
    "# View partition contents\n",
    "def show_partition_info(df):\n",
    "    partitions = df.rdd.glom().collect()\n",
    "    for i, partition in enumerate(partitions):\n",
    "        print(f\"Partition {i}: {len(partition)} rows\")\n",
    "\n",
    "show_partition_info(sales_partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching DataFrames\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# Create a large DataFrame\n",
    "large_df = spark.range(100000).withColumn(\"squared\", col(\"id\") ** 2)\n",
    "\n",
    "# Without caching - multiple computations\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "count1 = large_df.filter(col(\"squared\") > 1000).count()\n",
    "count2 = large_df.filter(col(\"squared\") > 2000).count()\n",
    "count3 = large_df.filter(col(\"squared\") > 3000).count()\n",
    "print(f\"Without caching: {time.time() - start:.4f}s\")\n",
    "\n",
    "# With caching\n",
    "large_df.cache()  # or large_df.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "# First action triggers caching\n",
    "_ = large_df.count()\n",
    "\n",
    "start = time.time()\n",
    "count1 = large_df.filter(col(\"squared\") > 1000).count()\n",
    "count2 = large_df.filter(col(\"squared\") > 2000).count()\n",
    "count3 = large_df.filter(col(\"squared\") > 3000).count()\n",
    "print(f\"With caching: {time.time() - start:.4f}s\")\n",
    "\n",
    "# Unpersist when done\n",
    "large_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage levels\n",
    "print(\"Available storage levels:\")\n",
    "print(f\"  MEMORY_ONLY: {StorageLevel.MEMORY_ONLY}\")\n",
    "print(f\"  MEMORY_AND_DISK: {StorageLevel.MEMORY_AND_DISK}\")\n",
    "print(f\"  DISK_ONLY: {StorageLevel.DISK_ONLY}\")\n",
    "print(f\"  MEMORY_ONLY_2: {StorageLevel.MEMORY_ONLY_2}\")       # 2x replication\n",
    "print(f\"  MEMORY_AND_DISK_2: {StorageLevel.MEMORY_AND_DISK_2}\") # 2x replication\n",
    "print(f\"  OFF_HEAP: {StorageLevel.OFF_HEAP}\")\n",
    "\n",
    "# Note: MEMORY_ONLY_SER / MEMORY_AND_DISK_SER exist in Scala/Java but not in PySpark.\n",
    "# In PySpark, data is already serialized (pickled), so the _SER distinction does not apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast variables for small datasets in joins\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Small lookup table\n",
    "lookup_data = [(\"A\", \"Category A\"), (\"B\", \"Category B\"), (\"C\", \"Category C\")]\n",
    "lookup_df = spark.createDataFrame(lookup_data, [\"code\", \"description\"])\n",
    "\n",
    "# Large fact table\n",
    "fact_data = [(i, [\"A\", \"B\", \"C\"][i % 3], i * 10) for i in range(10000)]\n",
    "fact_df = spark.createDataFrame(fact_data, [\"id\", \"code\", \"value\"])\n",
    "\n",
    "# Normal join\n",
    "start = time.time()\n",
    "result1 = fact_df.join(lookup_df, \"code\")\n",
    "_ = result1.count()\n",
    "print(f\"Normal join: {time.time() - start:.4f}s\")\n",
    "\n",
    "# Broadcast join (small table is broadcast to all nodes)\n",
    "start = time.time()\n",
    "result2 = fact_df.join(broadcast(lookup_df), \"code\")\n",
    "_ = result2.count()\n",
    "print(f\"Broadcast join: {time.time() - start:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain query execution plan\n",
    "df = sales_df.filter(col(\"revenue\") > 100).groupBy(\"category\").sum(\"revenue\")\n",
    "\n",
    "# Simple explanation\n",
    "print(\"=== Simple Explain ===\")\n",
    "df.explain()\n",
    "\n",
    "# Extended explanation\n",
    "print(\"\\n=== Extended Explain ===\")\n",
    "df.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicate pushdown demonstration\n",
    "# When reading from files, Spark can push filters to the data source\n",
    "\n",
    "# Write sample data to parquet\n",
    "sales_df.write.mode(\"overwrite\").parquet(\"sales_data.parquet\")\n",
    "\n",
    "# Reading with filter - Spark will only read necessary data\n",
    "filtered = spark.read.parquet(\"sales_data.parquet\").filter(col(\"category\") == \"Electronics\")\n",
    "print(\"Query plan with predicate pushdown:\")\n",
    "filtered.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column pruning - only read necessary columns\n",
    "# Select specific columns before applying transformations\n",
    "\n",
    "# Inefficient: reads all columns\n",
    "all_cols = spark.read.parquet(\"sales_data.parquet\")\n",
    "result_all = all_cols.filter(col(\"revenue\") > 1000).select(\"product\", \"revenue\")\n",
    "\n",
    "# Efficient: only reads needed columns\n",
    "selected = spark.read.parquet(\"sales_data.parquet\").select(\"product\", \"revenue\")\n",
    "result_selected = selected.filter(col(\"revenue\") > 1000)\n",
    "\n",
    "print(\"Both approaches produce the same result:\")\n",
    "result_all.show()\n",
    "result_selected.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercise 6\n",
    "\n",
    "**Q6.1** Create a DataFrame with 10 million rows. Compare the performance of:\n",
    "- Running the same aggregation 5 times without caching\n",
    "- Running it 5 times with caching\n",
    "- Using different storage levels (MEMORY_ONLY vs MEMORY_AND_DISK)\n",
    "\n",
    "**Q6.2** Demonstrate the impact of partitioning on join performance:\n",
    "- Create two large DataFrames (1 million rows each)\n",
    "- Join them with different partitioning strategies\n",
    "- Compare execution times and explain the differences\n",
    "\n",
    "**Q6.3** Analyze query plans:\n",
    "- Write a complex query with filters, joins, and aggregations\n",
    "- Use `explain()` to understand the execution plan\n",
    "- Optimize the query based on the plan analysis\n",
    "- Compare before/after performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solutions here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 7: Processing Large-Scale Datasets [★★★]\n",
    "\n",
    "### Columnar File Formats\n",
    "\n",
    "For large-scale data processing, columnar formats like Parquet and ORC offer significant advantages:\n",
    "- **Efficient compression**: Similar values stored together compress better\n",
    "- **Column pruning**: Read only needed columns\n",
    "- **Predicate pushdown**: Filter at the storage level\n",
    "- **Schema evolution**: Add/remove columns without rewriting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a larger dataset for demonstration\n",
    "import random\n",
    "from pyspark.sql.functions import rand, randn, floor, concat, lit\n",
    "\n",
    "# Create a larger sales dataset\n",
    "large_sales = spark.range(100000) \\\n",
    "    .withColumn(\"date\", concat(lit(\"2024-\"), \n",
    "                               ((floor(rand() * 12) + 1).cast(\"string\")), \n",
    "                               lit(\"-\"), \n",
    "                               ((floor(rand() * 28) + 1).cast(\"string\")))) \\\n",
    "    .withColumn(\"category\", \n",
    "                when(rand() < 0.3, \"Electronics\")\n",
    "                .when(rand() < 0.6, \"Clothing\")\n",
    "                .otherwise(\"Books\")) \\\n",
    "    .withColumn(\"price\", floor(rand() * 1000) + 10) \\\n",
    "    .withColumn(\"quantity\", floor(rand() * 50) + 1) \\\n",
    "    .withColumn(\"revenue\", col(\"price\") * col(\"quantity\"))\n",
    "\n",
    "print(f\"Generated {large_sales.count()} rows\")\n",
    "large_sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to different formats and compare sizes\n",
    "import os\n",
    "\n",
    "# Write as CSV\n",
    "large_sales.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"large_sales_csv\")\n",
    "\n",
    "# Write as Parquet (default compression: snappy)\n",
    "large_sales.write.mode(\"overwrite\").parquet(\"large_sales_parquet\")\n",
    "\n",
    "# Write as Parquet with gzip compression\n",
    "large_sales.write.mode(\"overwrite\").option(\"compression\", \"gzip\").parquet(\"large_sales_parquet_gzip\")\n",
    "\n",
    "# Write as ORC\n",
    "large_sales.write.mode(\"overwrite\").orc(\"large_sales_orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare file sizes (simplified - actual implementation depends on file system)\n",
    "def get_folder_size(path):\n",
    "    \"\"\"Calculate total size of files in a folder\"\"\"\n",
    "    total = 0\n",
    "    if os.path.exists(path):\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for f in files:\n",
    "                total += os.path.getsize(os.path.join(root, f))\n",
    "    return total\n",
    "\n",
    "csv_size = get_folder_size(\"large_sales_csv\")\n",
    "parquet_size = get_folder_size(\"large_sales_parquet\")\n",
    "parquet_gzip_size = get_folder_size(\"large_sales_parquet_gzip\")\n",
    "orc_size = get_folder_size(\"large_sales_orc\")\n",
    "\n",
    "print(f\"CSV size: {csv_size / 1024:.2f} KB\")\n",
    "print(f\"Parquet (snappy) size: {parquet_size / 1024:.2f} KB\")\n",
    "print(f\"Parquet (gzip) size: {parquet_gzip_size / 1024:.2f} KB\")\n",
    "print(f\"ORC size: {orc_size / 1024:.2f} KB\")\n",
    "\n",
    "if csv_size > 0:\n",
    "    print(f\"\\nCompression ratios vs CSV:\")\n",
    "    print(f\"  Parquet (snappy): {csv_size / parquet_size:.2f}x\")\n",
    "    print(f\"  Parquet (gzip): {csv_size / parquet_gzip_size:.2f}x\")\n",
    "    print(f\"  ORC: {csv_size / orc_size:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare read performance\n",
    "import time\n",
    "\n",
    "def benchmark_read(path, format_type):\n",
    "    start = time.time()\n",
    "    if format_type == \"csv\":\n",
    "        df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(path)\n",
    "    elif format_type == \"parquet\":\n",
    "        df = spark.read.parquet(path)\n",
    "    else:\n",
    "        df = spark.read.orc(path)\n",
    "    count = df.count()\n",
    "    return time.time() - start\n",
    "\n",
    "print(\"Read performance (seconds):\")\n",
    "print(f\"  CSV: {benchmark_read('large_sales_csv', 'csv'):.4f}s\")\n",
    "print(f\"  Parquet: {benchmark_read('large_sales_parquet', 'parquet'):.4f}s\")\n",
    "print(f\"  ORC: {benchmark_read('large_sales_orc', 'orc'):.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioned writes - partition by category\n",
    "large_sales.write.mode(\"overwrite\").partitionBy(\"category\").parquet(\"large_sales_partitioned\")\n",
    "\n",
    "# List the partition directories\n",
    "for item in os.listdir(\"large_sales_partitioned\"):\n",
    "    if os.path.isdir(os.path.join(\"large_sales_partitioned\", item)):\n",
    "        print(f\"Partition: {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading partitioned data - partition pruning\n",
    "# Only reads the Electronics partition\n",
    "electronics = spark.read.parquet(\"large_sales_partitioned\").filter(col(\"category\") == \"Electronics\")\n",
    "\n",
    "print(\"Query plan with partition pruning:\")\n",
    "electronics.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema evolution with Parquet\n",
    "# Original data\n",
    "original_data = [(1, \"A\", 100), (2, \"B\", 200)]\n",
    "original_df = spark.createDataFrame(original_data, [\"id\", \"code\", \"value\"])\n",
    "original_df.write.mode(\"overwrite\").parquet(\"schema_evolution_test\")\n",
    "\n",
    "# New data with additional column\n",
    "new_data = [(3, \"C\", 300, \"new_info\"), (4, \"D\", 400, \"more_info\")]\n",
    "new_df = spark.createDataFrame(new_data, [\"id\", \"code\", \"value\", \"extra\"])\n",
    "new_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").parquet(\"schema_evolution_test\")\n",
    "\n",
    "# Read with merged schema\n",
    "merged = spark.read.option(\"mergeSchema\", \"true\").parquet(\"schema_evolution_test\")\n",
    "print(\"Merged schema:\")\n",
    "merged.printSchema()\n",
    "merged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with multiple data sources\n",
    "# Example: Join data from different files\n",
    "\n",
    "# Write some reference data\n",
    "categories = [(\"Electronics\", \"Tech products\", 0.1), \n",
    "              (\"Clothing\", \"Apparel\", 0.05), \n",
    "              (\"Books\", \"Reading materials\", 0.0)]\n",
    "categories_df = spark.createDataFrame(categories, [\"category\", \"description\", \"tax_rate\"])\n",
    "categories_df.write.mode(\"overwrite\").parquet(\"categories_ref\")\n",
    "\n",
    "# Read and join\n",
    "sales = spark.read.parquet(\"large_sales_parquet\")\n",
    "categories = spark.read.parquet(\"categories_ref\")\n",
    "\n",
    "enriched = sales.join(broadcast(categories), \"category\")\n",
    "enriched = enriched.withColumn(\"tax\", col(\"revenue\") * col(\"tax_rate\"))\n",
    "enriched = enriched.withColumn(\"total\", col(\"revenue\") + col(\"tax\"))\n",
    "\n",
    "print(\"Enriched sales data:\")\n",
    "enriched.select(\"id\", \"category\", \"revenue\", \"tax_rate\", \"tax\", \"total\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files\n",
    "import shutil\n",
    "\n",
    "cleanup_dirs = [\n",
    "    \"large_sales_csv\", \"large_sales_parquet\", \"large_sales_parquet_gzip\",\n",
    "    \"large_sales_orc\", \"large_sales_partitioned\", \"schema_evolution_test\",\n",
    "    \"categories_ref\", \"sales_data.parquet\", \"languages.orc\", \"languages.parquet\", \"languages.csv\"\n",
    "]\n",
    "\n",
    "for d in cleanup_dirs:\n",
    "    if os.path.exists(d):\n",
    "        shutil.rmtree(d) if os.path.isdir(d) else os.remove(d)\n",
    "        print(f\"Cleaned up: {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercise 7\n",
    "\n",
    "**Q7.1** Download a large dataset (at least 1 million rows) from a public source (e.g., NYC Taxi data, Wikipedia page views):\n",
    "- Load it into Spark\n",
    "- Save in CSV, Parquet, and ORC formats\n",
    "- Compare file sizes and read/write times\n",
    "- Test query performance on each format\n",
    "\n",
    "**Q7.2** Implement an ETL (Extract, Transform, Load) pipeline:\n",
    "- Read data from multiple source files\n",
    "- Clean and transform the data (handle nulls, normalize values)\n",
    "- Join with reference data\n",
    "- Write to partitioned Parquet files\n",
    "- Measure and optimize performance\n",
    "\n",
    "**Q7.3** Create a data warehouse star schema:\n",
    "- Design fact and dimension tables\n",
    "- Generate realistic synthetic data (10 million+ rows)\n",
    "- Implement common analytical queries\n",
    "- Optimize using partitioning, caching, and broadcast joins\n",
    "- Document performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solutions here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this practical, you learned:\n",
    "\n",
    "1. **Spark Architecture**: Understanding drivers, executors, and cluster managers\n",
    "2. **RDDs**: Creating and manipulating resilient distributed datasets\n",
    "3. **Transformations vs Actions**: Lazy evaluation and optimization\n",
    "4. **DataFrames**: Structured data processing with schemas\n",
    "5. **Spark SQL**: Querying data using SQL syntax\n",
    "6. **Advanced Operations**: Joins, window functions, aggregations\n",
    "7. **Performance Optimization**: Partitioning, caching, broadcast variables\n",
    "8. **File Formats**: Working with Parquet, ORC, and CSV at scale\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Use DataFrames over RDDs when possible for better optimization\n",
    "- Cache intermediate results that are used multiple times\n",
    "- Use columnar formats (Parquet/ORC) for large datasets\n",
    "- Partition data by frequently filtered columns\n",
    "- Use broadcast joins for small lookup tables\n",
    "- Monitor query plans with `explain()` to identify bottlenecks\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)\n",
    "- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Spark: The Definitive Guide](https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/)\n",
    "- [Learning Spark, 2nd Edition](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session when done\n",
    "spark.stop()\n",
    "sc.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
