{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 5 : Apache Spark pour le Traitement de Données Massives\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "Cette séance de travaux pratiques présente Apache Spark, un framework de calcul distribué puissant conçu pour le traitement de données à grande échelle. Vous apprendrez à exploiter les capacités de Spark pour gérer des ensembles de données massifs qui dépassent les limites de mémoire d'une seule machine.\n",
    "\n",
    "### Objectifs d'apprentissage\n",
    "* Comprendre l'architecture Spark : Driver, Executors et Cluster Manager\n",
    "* Maîtriser les opérations RDD (Resilient Distributed Dataset)\n",
    "* Travailler avec les DataFrames et comprendre la gestion des schémas\n",
    "* Écrire des requêtes Spark SQL efficaces\n",
    "* Implémenter des jointures, fonctions fenêtrées et agrégations complexes\n",
    "* Optimiser les performances grâce aux stratégies de partitionnement et de mise en cache\n",
    "* Traiter des ensembles de données à grande échelle avec des formats de fichiers en colonnes (Parquet, ORC)\n",
    "\n",
    "### Prérequis\n",
    "* Complétion du TP 4 (Calcul Parallèle et Distribué)\n",
    "* Compréhension des concepts de programmation fonctionnelle (map, filter, reduce)\n",
    "* Connaissances de base en SQL\n",
    "* Fondamentaux de la programmation Python\n",
    "\n",
    "### Installation\n",
    "\n",
    "Installez PySpark avant de commencer :\n",
    "```bash\n",
    "!pip install pyspark==3.5.3\n",
    "```\n",
    "\n",
    "### Aperçu des exercices\n",
    "\n",
    "| Exercice | Sujet | Difficulté |\n",
    "|----------|-------|------------|\n",
    "| 1 | Architecture Spark et bases des RDD | ★ |\n",
    "| 2 | Transformations et actions RDD | ★ |\n",
    "| 3 | DataFrames et gestion des schémas | ★★ |\n",
    "| 4 | Spark SQL et requêtes complexes | ★★ |\n",
    "| 5 | Jointures, fonctions fenêtrées et agrégations | ★★ |\n",
    "| 6 | Partitionnement, mise en cache et optimisation | ★★★ |\n",
    "| 7 | Traitement d'ensembles de données à grande échelle | ★★★ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 1 : Architecture Spark et bases des RDD [★]\n",
    "\n",
    "### Comprendre l'architecture Spark\n",
    "\n",
    "Apache Spark utilise une **architecture maître-esclave** :\n",
    "\n",
    "1. **Programme Driver** : Le programme principal qui crée le SparkContext et coordonne l'exécution\n",
    "2. **Cluster Manager** : Alloue les ressources (peut être Standalone, YARN, Mesos ou Kubernetes)\n",
    "3. **Executors** : Processus de travail qui exécutent les tâches et stockent les données\n",
    "\n",
    "```\n",
    "┌─────────────────┐\n",
    "│  Programme Driver │\n",
    "│  (SparkContext)   │\n",
    "└────────┬────────┘\n",
    "         │\n",
    "         ▼\n",
    "┌─────────────────┐\n",
    "│ Cluster Manager   │\n",
    "└────────┬────────┘\n",
    "         │\n",
    "    ┌────┴────┐\n",
    "    ▼         ▼\n",
    "┌───────┐ ┌───────┐\n",
    "│Executor│ │Executor│\n",
    "│ Tâche   │ │ Tâche   │\n",
    "│ Tâche   │ │ Tâche   │\n",
    "└───────┘ └───────┘\n",
    "```\n",
    "\n",
    "### RDD (Resilient Distributed Dataset)\n",
    "\n",
    "Les RDD sont la structure de données fondamentale dans Spark :\n",
    "- **Résilient** : Tolérant aux pannes grâce aux informations de lignée\n",
    "- **Distribué** : Les données sont partitionnées sur plusieurs nœuds\n",
    "- **Dataset** : Collection d'éléments pouvant être traités en parallèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==3.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tout d'abord, vérifions l'installation de PySpark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "print(\"PySpark importé avec succès !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un SparkContext avec une configuration locale\n",
    "# 'local[*]' utilise tous les cœurs disponibles\n",
    "conf = SparkConf().setAppName(\"TP5\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "# Afficher la configuration Spark\n",
    "print(f\"Version Spark : {sc.version}\")\n",
    "print(f\"Nom de l'application : {sc.appName}\")\n",
    "print(f\"Master : {sc.master}\")\n",
    "print(f\"Parallélisme par défaut : {sc.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de RDD - Méthode 1 : À partir d'une collection Python (parallelize)\n",
    "nombres = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "nombres_rdd = sc.parallelize(nombres)\n",
    "\n",
    "print(f\"Type : {type(nombres_rdd)}\")\n",
    "print(f\"Nombre de partitions : {nombres_rdd.getNumPartitions()}\")\n",
    "print(f\"Premier élément : {nombres_rdd.first()}\")\n",
    "print(f\"Tous les éléments : {nombres_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de RDD - Méthode 2 : À partir de fichiers externes\n",
    "lignes_rdd = sc.textFile(\"../shared_data/pl.csv\")\n",
    "\n",
    "print(f\"Nombre de partitions : {lignes_rdd.getNumPartitions()}\")\n",
    "print(f\"Nombre de lignes : {lignes_rdd.count()}\")\n",
    "print(f\"\\n5 premières lignes :\")\n",
    "for ligne in lignes_rdd.take(5):\n",
    "    print(f\"  {ligne}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de RDD avec un nombre spécifique de partitions\n",
    "# Plus de partitions = plus de parallélisme (mais aussi plus de surcharge)\n",
    "nombres_4partitions = sc.parallelize(range(1, 101), numSlices=4)\n",
    "nombres_8partitions = sc.parallelize(range(1, 101), numSlices=8)\n",
    "\n",
    "print(f\"RDD avec 4 partitions : {nombres_4partitions.getNumPartitions()}\")\n",
    "print(f\"RDD avec 8 partitions : {nombres_8partitions.getNumPartitions()}\")\n",
    "\n",
    "# Voir comment les données sont distribuées entre les partitions\n",
    "print(f\"\\nDistribution des données (4 partitions) :\")\n",
    "print(nombres_4partitions.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercice 1\n",
    "\n",
    "**Q1.1** Créez un RDD contenant les carrés des nombres de 1 à 1000. Expérimentez avec différents nombres de partitions (2, 4, 8, 16) et expliquez comment le partitionnement affecte la distribution des données.\n",
    "\n",
    "**Q1.2** Chargez plusieurs fichiers CSV depuis le répertoire `../shared_data/` en utilisant des motifs génériques (ex: `*.csv`). Combien y a-t-il de lignes au total dans tous les fichiers ?\n",
    "\n",
    "**Q1.3** Expliquez la différence entre la création d'un RDD avec `parallelize()` et `textFile()`. Quand utiliseriez-vous chaque méthode ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vos solutions ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 2 : Transformations et actions RDD [★]\n",
    "\n",
    "### Transformations vs Actions\n",
    "\n",
    "Les opérations Spark sont divisées en deux catégories :\n",
    "\n",
    "**Transformations** (Paresseuses - créent de nouveaux RDD) :\n",
    "- `map()`, `filter()`, `flatMap()`, `distinct()`\n",
    "- `union()`, `intersection()`, `subtract()`\n",
    "- `groupByKey()`, `reduceByKey()`, `sortByKey()`\n",
    "\n",
    "**Actions** (Immédiates - retournent des résultats) :\n",
    "- `collect()`, `count()`, `first()`, `take()`\n",
    "- `reduce()`, `fold()`, `aggregate()`\n",
    "- `saveAsTextFile()`, `foreach()`\n",
    "\n",
    "### Évaluation paresseuse\n",
    "\n",
    "Les transformations sont **paresseuses** : Spark construit un DAG (Graphe Acyclique Dirigé) des opérations mais ne les exécute pas jusqu'à ce qu'une action soit appelée. Cela permet à Spark d'optimiser le plan d'exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démonstration de l'évaluation paresseuse\n",
    "import time\n",
    "\n",
    "# Cette transformation n'est PAS exécutée immédiatement\n",
    "debut = time.time()\n",
    "grand_rdd = sc.parallelize(range(1, 1000001))\n",
    "carres = grand_rdd.map(lambda x: x ** 2)\n",
    "filtres = carres.filter(lambda x: x % 2 == 0)\n",
    "print(f\"Transformations définies en : {time.time() - debut:.4f} secondes\")\n",
    "\n",
    "# L'action déclenche l'exécution\n",
    "debut = time.time()\n",
    "resultat = filtres.count()\n",
    "print(f\"Action exécutée en : {time.time() - debut:.4f} secondes\")\n",
    "print(f\"Nombre de carrés pairs : {resultat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation map : appliquer une fonction à chaque élément\n",
    "mots = sc.parallelize([\"bonjour\", \"monde\", \"spark\", \"python\"])\n",
    "\n",
    "# Transformer en majuscules\n",
    "mots_majuscules = mots.map(lambda m: m.upper())\n",
    "print(f\"Majuscules : {mots_majuscules.collect()}\")\n",
    "\n",
    "# Transformer en tuples (mot, longueur)\n",
    "longueurs_mots = mots.map(lambda m: (m, len(m)))\n",
    "print(f\"Longueurs des mots : {longueurs_mots.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation filter : garder les éléments qui satisfont une condition\n",
    "nombres = sc.parallelize(range(1, 21))\n",
    "\n",
    "# Garder uniquement les nombres pairs\n",
    "pairs = nombres.filter(lambda x: x % 2 == 0)\n",
    "print(f\"Nombres pairs : {pairs.collect()}\")\n",
    "\n",
    "# Garder les nombres divisibles par 3\n",
    "div_par_3 = nombres.filter(lambda x: x % 3 == 0)\n",
    "print(f\"Divisibles par 3 : {div_par_3.collect()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation flatMap : map qui peut retourner plusieurs éléments\n",
    "phrases = sc.parallelize([\n",
    "    \"Bonjour le Monde\",\n",
    "    \"Apache Spark est puissant\",\n",
    "    \"Traitement de données massives\"\n",
    "])\n",
    "\n",
    "# Découper les phrases en mots\n",
    "mots = phrases.flatMap(lambda s: s.split())\n",
    "print(f\"Tous les mots : {mots.collect()}\")\n",
    "print(f\"Nombre de mots : {mots.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action reduce : agréger les éléments avec une fonction\n",
    "nombres = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Somme de tous les nombres\n",
    "total = nombres.reduce(lambda a, b: a + b)\n",
    "print(f\"Somme : {total}\")\n",
    "\n",
    "# Trouver le maximum\n",
    "\n",
    "maximum = numbers.reduce(lambda a, b: a if a > b else b)\n",
    "print(f\"Maximum : {maximum}\")\n",
    "\n",
    "# Produit de tous les nombres\n",
    "produit = nombres.reduce(lambda a, b: a * b)\n",
    "print(f\"Produit : {produit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comptage de mots - L'exemple classique de Spark\n",
    "texte = sc.parallelize([\n",
    "    \"Apache Spark est un moteur d'analytique unifié\",\n",
    "    \"Spark fournit des API de haut niveau en Java Scala Python et R\",\n",
    "    \"Spark alimente une pile de bibliothèques pour SQL streaming et apprentissage automatique\",\n",
    "    \"Spark fonctionne sur Hadoop YARN Mesos Kubernetes et en mode autonome\"\n",
    "])\n",
    "\n",
    "# Pipeline de comptage de mots\n",
    "comptage_mots = (texte\n",
    "    .flatMap(lambda ligne: ligne.lower().split())  # Découper en mots\n",
    "    .map(lambda mot: (mot, 1))                      # Mapper en paires (mot, 1)\n",
    "    .reduceByKey(lambda a, b: a + b)                # Sommer les comptages par mot\n",
    "    .sortBy(lambda x: x[1], ascending=False))       # Trier par comptage\n",
    "\n",
    "print(\"Comptage de mots (top 10) :\")\n",
    "for mot, comptage in comptage_mots.take(10):\n",
    "    print(f\"  {mot}: {comptage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opérations ensemblistes sur les RDD\n",
    "rdd1 = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd2 = sc.parallelize([4, 5, 6, 7, 8])\n",
    "\n",
    "# Union (tous les éléments des deux)\n",
    "print(f\"Union : {rdd1.union(rdd2).collect()}\")\n",
    "\n",
    "# Intersection (éléments communs)\n",
    "print(f\"Intersection : {rdd1.intersection(rdd2).collect()}\")\n",
    "\n",
    "# Soustraction (éléments dans rdd1 mais pas dans rdd2)\n",
    "print(f\"Soustraction : {rdd1.subtract(rdd2).collect()}\")\n",
    "\n",
    "# Distinct (éléments uniques)\n",
    "print(f\"Union distincte : {rdd1.union(rdd2).distinct().collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercice 2\n",
    "\n",
    "**Q2.1** Chargez le fichier `../shared_data/pl.csv` et effectuez les opérations suivantes :\n",
    "- Comptez le nombre total de caractères sur toutes les lignes\n",
    "- Comptez le nombre total de tokens (valeurs séparées par des virgules)\n",
    "- Trouvez la ligne avec le nombre maximum de caractères\n",
    "\n",
    "**Q2.2** Téléchargez 50 pages HTML depuis le web. Écrivez un programme Spark pour :\n",
    "- Compter le nombre total de balises `<div>` et `</div>` dans tous les fichiers\n",
    "- Trouver la page avec le plus de balises `<a>` (ancres)\n",
    "- Extraire et compter tous les noms de classes CSS uniques\n",
    "\n",
    "**Q2.3** Implémentez un compteur de fréquence de caractères qui :\n",
    "- Lit tous les fichiers texte d'un répertoire\n",
    "- Compte la fréquence de chaque caractère (insensible à la casse)\n",
    "- Retourne les 10 caractères les plus fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vos solutions ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 3 : DataFrames et gestion des schémas [★★]\n",
    "\n",
    "### DataFrames\n",
    "\n",
    "Les DataFrames sont une abstraction de plus haut niveau construite au-dessus des RDD :\n",
    "- Organisés en colonnes nommées (comme une table)\n",
    "- Optimisés grâce à l'optimiseur de requêtes Catalyst\n",
    "- Support pour les données structurées et semi-structurées\n",
    "- Meilleures performances que les RDD pour la plupart des opérations\n",
    "\n",
    "### SparkSession\n",
    "\n",
    "SparkSession est le point d'entrée pour les opérations sur les DataFrames (introduit dans Spark 2.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "# Créer une SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TP5-DataFrames\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Session Spark créée : {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de DataFrames - Méthode 1 : À partir d'une liste Python avec schéma inféré\n",
    "donnees = [\n",
    "    (\"Python\", 1991, \"Guido van Rossum\"),\n",
    "    (\"Java\", 1995, \"James Gosling\"),\n",
    "    (\"JavaScript\", 1995, \"Brendan Eich\"),\n",
    "    (\"C\", 1972, \"Dennis Ritchie\"),\n",
    "    (\"Rust\", 2010, \"Graydon Hoare\")\n",
    "]\n",
    "\n",
    "df_infere = spark.createDataFrame(donnees, [\"langage\", \"annee\", \"createur\"])\n",
    "df_infere.show()\n",
    "df_infere.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de DataFrames - Méthode 2 : Avec schéma explicite\n",
    "schema = StructType([\n",
    "    StructField(\"langage\", StringType(), nullable=False),\n",
    "    StructField(\"annee\", IntegerType(), nullable=False),\n",
    "    StructField(\"createur\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "df_explicite = spark.createDataFrame(donnees, schema)\n",
    "df_explicite.show()\n",
    "df_explicite.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de DataFrames - Méthode 3 : À partir d'un fichier JSON\n",
    "df_json = spark.read.json(\"../shared_data/pl.json\")\n",
    "df_json.show(10)\n",
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de DataFrames - Méthode 4 : À partir d'un CSV avec options\n",
    "df_csv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"../shared_data/pl.csv\")\n",
    "\n",
    "df_csv.show(10)\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opérations de base sur les DataFrames\n",
    "df = df_json\n",
    "\n",
    "# Afficher les n premières lignes\n",
    "print(\"5 premières lignes :\")\n",
    "df.show(5)\n",
    "\n",
    "# Obtenir les noms des colonnes\n",
    "print(f\"\\nColonnes : {df.columns}\")\n",
    "\n",
    "# Obtenir le nombre de lignes et colonnes\n",
    "print(f\"Dimensions : ({df.count()}, {len(df.columns)})\")\n",
    "\n",
    "# Statistiques descriptives\n",
    "print(\"\\nStatistiques :\")\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection et manipulation de colonnes\n",
    "from pyspark.sql.functions import col, lit, when, upper, lower, length\n",
    "\n",
    "# Sélectionner des colonnes spécifiques\n",
    "df.select(\"languageLabel\").show(5)\n",
    "\n",
    "# Sélectionner avec des expressions de colonnes\n",
    "df.select(\n",
    "    col(\"languageLabel\"),\n",
    "    col(\"year\"),\n",
    "    (col(\"year\") - 1900).alias(\"annees_depuis_1900\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout et modification de colonnes\n",
    "df_modifie = df \\\n",
    "    .withColumn(\"siecle\", ((col(\"year\") / 100) + 1).cast(IntegerType())) \\\n",
    "    .withColumn(\"langage_majuscules\", upper(col(\"languageLabel\"))) \\\n",
    "    .withColumn(\"longueur_nom\", length(col(\"languageLabel\")))\n",
    "\n",
    "df_modifie.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrage des lignes\n",
    "# Langages créés après 2000\n",
    "recents = df.filter(col(\"year\") > 2000)\n",
    "print(f\"Langages après 2000 : {recents.count()}\")\n",
    "recents.show(10)\n",
    "\n",
    "# Conditions multiples\n",
    "filtres = df.filter((col(\"year\") >= 1990) & (col(\"year\") <= 2000))\n",
    "print(f\"\\nLangages de 1990-2000 : {filtres.count()}\")\n",
    "filtres.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy et agrégations\n",
    "from pyspark.sql.functions import count, avg, min, max, sum\n",
    "\n",
    "# Compter les langages par année\n",
    "df.groupBy(\"year\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrégations multiples\n",
    "df_modifie.groupBy(\"siecle\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"nb_langages\"),\n",
    "        min(\"year\").alias(\"annee_debut\"),\n",
    "        max(\"year\").alias(\"annee_fin\"),\n",
    "        avg(\"longueur_nom\").alias(\"longueur_moyenne_nom\")\n",
    "    ) \\\n",
    "    .orderBy(\"siecle\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercice 3\n",
    "\n",
    "**Q3.1** Interrogez Wikidata pour télécharger des informations sur toutes les applications logicielles incluant : nom, date de sortie, développeur et langage de programmation utilisé. Chargez ces données dans un DataFrame Spark avec un schéma explicite.\n",
    "\n",
    "**Q3.2** En utilisant le DataFrame des langages de programmation :\n",
    "- Ajoutez une colonne catégorisant les langages comme \"Pionnier\" (avant 1980), \"Classique\" (1980-2000) ou \"Moderne\" (après 2000)\n",
    "- Calculez les statistiques (comptage, année min, année max) pour chaque catégorie\n",
    "- Trouvez la décennie avec le plus de sorties de langages\n",
    "\n",
    "**Q3.3** Créez un DataFrame à partir d'un fichier CSV avec une gestion correcte de :\n",
    "- Valeurs manquantes (nulls)\n",
    "- Analyse des dates\n",
    "- Délimiteurs personnalisés\n",
    "- Caractères échappés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vos solutions ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 4 : Spark SQL et requêtes complexes [★★]\n",
    "\n",
    "### Interface SQL\n",
    "\n",
    "Spark SQL vous permet d'exécuter des requêtes SQL directement sur les DataFrames en utilisant des vues temporaires. C'est utile pour :\n",
    "- Les requêtes complexes plus faciles à exprimer en SQL\n",
    "- L'interopérabilité avec les outils basés sur SQL\n",
    "- La familiarité pour les utilisateurs SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données et créer une vue temporaire\n",
    "df = spark.read.json(\"../shared_data/pl.json\")\n",
    "df.createOrReplaceTempView(\"langages\")\n",
    "\n",
    "# Requête SELECT de base\n",
    "spark.sql(\"SELECT * FROM langages LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrage avec la clause WHERE\n",
    "spark.sql(\"\"\"\n",
    "    SELECT languageLabel, year \n",
    "    FROM langages \n",
    "    WHERE year >= 1990 AND year <= 2000\n",
    "    ORDER BY year\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requêtes d'agrégation\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        year,\n",
    "        COUNT(*) as nombre_langages\n",
    "    FROM langages\n",
    "    GROUP BY year\n",
    "    HAVING COUNT(*) > 3\n",
    "    ORDER BY nombre_langages DESC\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation des expressions CASE\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        languageLabel,\n",
    "        year,\n",
    "        CASE \n",
    "            WHEN year < 1980 THEN 'Ere Pionniere'\n",
    "            WHEN year < 1990 THEN 'Annees 1980'\n",
    "            WHEN year < 2000 THEN 'Annees 1990'\n",
    "            WHEN year < 2010 THEN 'Annees 2000'\n",
    "            ELSE 'Annees 2010+'\n",
    "        END as epoque\n",
    "    FROM langages\n",
    "    ORDER BY year\n",
    "\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sous-requêtes\n",
    "spark.sql(\"\"\"\n",
    "    SELECT languageLabel, year\n",
    "    FROM langages\n",
    "    WHERE year = (\n",
    "        SELECT MAX(year) FROM langages\n",
    "    )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expressions de table communes (CTE)\n",
    "spark.sql(\"\"\"\n",
    "    WITH stats_decennie AS (\n",
    "        SELECT \n",
    "            FLOOR(year / 10) * 10 as decennie,\n",
    "            COUNT(*) as comptage\n",
    "        FROM langages\n",
    "        GROUP BY FLOOR(year / 10) * 10\n",
    "    )\n",
    "    SELECT \n",
    "        decennie,\n",
    "        comptage,\n",
    "        ROUND(comptage * 100.0 / SUM(comptage) OVER(), 2) as pourcentage\n",
    "    FROM stats_decennie\n",
    "    ORDER BY decennie\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer des tables supplémentaires pour les exemples de jointures\n",
    "# Données de paradigmes\n",
    "donnees_paradigmes = [\n",
    "    (\"Python\", \"Multi-paradigme\"),\n",
    "    (\"Java\", \"Orienté objet\"),\n",
    "    (\"JavaScript\", \"Multi-paradigme\"),\n",
    "    (\"Haskell\", \"Fonctionnel\"),\n",
    "    (\"C\", \"Procédural\"),\n",
    "    (\"Rust\", \"Multi-paradigme\"),\n",
    "    (\"Lisp\", \"Fonctionnel\"),\n",
    "    (\"Prolog\", \"Logique\")\n",
    "]\n",
    "\n",
    "paradigmes_df = spark.createDataFrame(donnees_paradigmes, [\"langage\", \"paradigme\"])\n",
    "paradigmes_df.createOrReplaceTempView(\"paradigmes\")\n",
    "\n",
    "# Données de typage\n",
    "donnees_typage = [\n",
    "    (\"Python\", \"Dynamique\", \"Fort\"),\n",
    "    (\"Java\", \"Statique\", \"Fort\"),\n",
    "    (\"JavaScript\", \"Dynamique\", \"Faible\"),\n",
    "    (\"C\", \"Statique\", \"Faible\"),\n",
    "    (\"Rust\", \"Statique\", \"Fort\"),\n",
    "    (\"Haskell\", \"Statique\", \"Fort\")\n",
    "]\n",
    "\n",
    "typage_df = spark.createDataFrame(donnees_typage, [\"langage\", \"typage\", \"surete_type\"])\n",
    "typage_df.createOrReplaceTempView(\"typage\")\n",
    "\n",
    "print(\"Paradigmes :\")\n",
    "paradigmes_df.show()\n",
    "print(\"Typage :\")\n",
    "typage_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INNER JOIN\n",
    "spark.sql(\"\"\"\n",
    "    SELECT p.langage, p.paradigme, t.typage, t.surete_type\n",
    "    FROM paradigmes p\n",
    "    INNER JOIN typage t ON p.langage = t.langage\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEFT JOIN\n",
    "spark.sql(\"\"\"\n",
    "    SELECT p.langage, p.paradigme, t.typage\n",
    "    FROM paradigmes p\n",
    "    LEFT JOIN typage t ON p.langage = t.langage\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercice 4\n",
    "\n",
    "**Q4.1** En utilisant la vue langages, écrivez des requêtes SQL pour :\n",
    "- Trouver tous les langages sortis la même année que Python (1991)\n",
    "- Calculer le nombre moyen de langages sortis par décennie\n",
    "- Trouver les années où plus de 5 langages sont sortis\n",
    "\n",
    "**Q4.2** Créez deux nouvelles vues à partir de Wikidata :\n",
    "- Applications logicielles avec leurs développeurs\n",
    "- Développeurs avec leurs pays\n",
    "Écrivez une requête joignant ces tables pour afficher les logiciels groupés par pays.\n",
    "\n",
    "**Q4.3** En utilisant les fonctions fenêtrées, écrivez des requêtes pour :\n",
    "- Classer les langages par année au sein de chaque décennie\n",
    "- Calculer le total cumulatif des langages sortis au fil du temps\n",
    "- Trouver le premier et le dernier langage sorti chaque décennie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vos solutions ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 5 : Jointures, fonctions fenêtrées et agrégations [★★]\n",
    "\n",
    "### Opérations avancées sur les DataFrames\n",
    "\n",
    "Cet exercice couvre des opérations plus complexes essentielles pour le traitement de données réel :\n",
    "- Différents types de jointures\n",
    "- Fonctions fenêtrées pour l'analytique\n",
    "- Agrégations complexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, row_number, rank, dense_rank, lag, lead, sum as spark_sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Créer des données de ventes exemple\n",
    "donnees_ventes = [\n",
    "    (\"2024-01-15\", \"Electronique\", \"Ordinateur\", 1200.00, 5),\n",
    "    (\"2024-01-15\", \"Electronique\", \"Telephone\", 800.00, 10),\n",
    "    (\"2024-01-16\", \"Electronique\", \"Ordinateur\", 1200.00, 3),\n",
    "    (\"2024-01-16\", \"Vetements\", \"Chemise\", 50.00, 20),\n",
    "    (\"2024-01-17\", \"Electronique\", \"Tablette\", 500.00, 8),\n",
    "    (\"2024-01-17\", \"Vetements\", \"Pantalon\", 80.00, 15),\n",
    "    (\"2024-01-18\", \"Vetements\", \"Chemise\", 50.00, 25),\n",
    "    (\"2024-01-18\", \"Electronique\", \"Telephone\", 800.00, 12),\n",
    "    (\"2024-01-19\", \"Livres\", \"Fiction\", 25.00, 30),\n",
    "    (\"2024-01-19\", \"Livres\", \"Technique\", 60.00, 10),\n",
    "]\n",
    "\n",
    "schema_ventes = [\"date\", \"categorie\", \"produit\", \"prix\", \"quantite\"]\n",
    "ventes_df = spark.createDataFrame(donnees_ventes, schema_ventes)\n",
    "\n",
    "# Ajouter une colonne calculée\n",
    "ventes_df = ventes_df.withColumn(\"chiffre_affaires\", col(\"prix\") * col(\"quantite\"))\n",
    "ventes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction fenêtrée : Numéro de ligne dans chaque catégorie\n",
    "spec_fenetre = Window.partitionBy(\"categorie\").orderBy(col(\"chiffre_affaires\").desc())\n",
    "\n",
    "ventes_classees = ventes_df.withColumn(\"rang_dans_categorie\", row_number().over(spec_fenetre))\n",
    "ventes_classees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Différentes fonctions de classement\n",
    "spec_fenetre = Window.partitionBy(\"categorie\").orderBy(col(\"chiffre_affaires\").desc())\n",
    "\n",
    "ventes_df.select(\n",
    "    \"categorie\",\n",
    "    \"produit\",\n",
    "    \"chiffre_affaires\",\n",
    "    row_number().over(spec_fenetre).alias(\"numero_ligne\"),\n",
    "    rank().over(spec_fenetre).alias(\"rang\"),\n",
    "    dense_rank().over(spec_fenetre).alias(\"rang_dense\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Totaux cumulatifs avec les fonctions fenêtrées\n",
    "fenetre_cumul = Window.partitionBy(\"categorie\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "ventes_df.select(\n",
    "    \"date\",\n",
    "    \"categorie\",\n",
    "    \"chiffre_affaires\",\n",
    "    spark_sum(\"chiffre_affaires\").over(fenetre_cumul).alias(\"total_cumulatif\")\n",
    ").orderBy(\"categorie\", \"date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions lag et lead (valeurs de la ligne précédente/suivante)\n",
    "fenetre_ordonnee = Window.partitionBy(\"categorie\").orderBy(\"date\")\n",
    "\n",
    "ventes_df.select(\n",
    "    \"date\",\n",
    "    \"categorie\",\n",
    "    \"chiffre_affaires\",\n",
    "    lag(\"chiffre_affaires\", 1).over(fenetre_ordonnee).alias(\"ca_precedent\"),\n",
    "    lead(\"chiffre_affaires\", 1).over(fenetre_ordonnee).alias(\"ca_suivant\")\n",
    ").orderBy(\"categorie\", \"date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrégations complexes avec pivot\n",
    "pivot_df = ventes_df.groupBy(\"date\").pivot(\"categorie\").sum(\"chiffre_affaires\")\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrégations multiples en une fois\n",
    "from pyspark.sql.functions import avg, min, max, count, round as spark_round\n",
    "\n",
    "ventes_df.groupBy(\"categorie\").agg(\n",
    "    count(\"*\").alias(\"nb_transactions\"),\n",
    "    spark_round(avg(\"chiffre_affaires\"), 2).alias(\"ca_moyen\"),\n",
    "    spark_round(min(\"chiffre_affaires\"), 2).alias(\"ca_min\"),\n",
    "    spark_round(max(\"chiffre_affaires\"), 2).alias(\"ca_max\"),\n",
    "    spark_round(spark_sum(\"chiffre_affaires\"), 2).alias(\"ca_total\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer des DataFrames supplémentaires pour les exemples de jointures\n",
    "# Données clients\n",
    "donnees_clients = [\n",
    "    (1, \"Alice\", \"Paris\"),\n",
    "    (2, \"Bob\", \"Londres\"),\n",
    "    (3, \"Charlie\", \"Berlin\"),\n",
    "    (4, \"Diana\", \"Madrid\")\n",
    "]\n",
    "clients_df = spark.createDataFrame(donnees_clients, [\"client_id\", \"nom\", \"ville\"])\n",
    "\n",
    "# Données commandes\n",
    "donnees_commandes = [\n",
    "    (101, 1, \"2024-01-15\", 150.00),\n",
    "    (102, 1, \"2024-01-16\", 200.00),\n",
    "    (103, 2, \"2024-01-15\", 300.00),\n",
    "    (104, 3, \"2024-01-17\", 450.00),\n",
    "    (105, 5, \"2024-01-18\", 100.00)  # Le client 5 n'existe pas\n",
    "]\n",
    "commandes_df = spark.createDataFrame(donnees_commandes, [\"commande_id\", \"client_id\", \"date_commande\", \"montant\"])\n",
    "\n",
    "print(\"Clients :\")\n",
    "clients_df.show()\n",
    "print(\"Commandes :\")\n",
    "commandes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Différents types de jointures\n",
    "\n",
    "# Jointure interne (uniquement les lignes correspondantes)\n",
    "print(\"INNER JOIN :\")\n",
    "clients_df.join(commandes_df, \"client_id\", \"inner\").show()\n",
    "\n",
    "# Jointure gauche (tous les clients, commandes correspondantes)\n",
    "print(\"LEFT JOIN :\")\n",
    "clients_df.join(commandes_df, \"client_id\", \"left\").show()\n",
    "\n",
    "# Jointure droite (toutes les commandes, clients correspondants)\n",
    "print(\"RIGHT JOIN :\")\n",
    "clients_df.join(commandes_df, \"client_id\", \"right\").show()\n",
    "\n",
    "# Jointure externe complète (toutes les lignes des deux côtés)\n",
    "print(\"FULL OUTER JOIN :\")\n",
    "clients_df.join(commandes_df, \"client_id\", \"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anti-jointure (lignes de gauche sans correspondance à droite)\n",
    "print(\"Clients sans commandes (LEFT ANTI) :\")\n",
    "clients_df.join(commandes_df, \"client_id\", \"left_anti\").show()\n",
    "\n",
    "# Semi-jointure (lignes de gauche avec correspondance à droite, sans colonnes de droite)\n",
    "print(\"Clients avec commandes (LEFT SEMI) :\")\n",
    "clients_df.join(commandes_df, \"client_id\", \"left_semi\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercice 5\n",
    "\n",
    "**Q5.1** En utilisant les données de ventes, calculez :\n",
    "- Le pourcentage du chiffre d'affaires total contribué par chaque catégorie\n",
    "- Le produit le plus vendu dans chaque catégorie\n",
    "- Le taux de croissance journalier du chiffre d'affaires (variation en pourcentage par rapport au jour précédent)\n",
    "\n",
    "**Q5.2** Créez une segmentation client basée sur leurs dépenses totales :\n",
    "- \"Bronze\" : dépenses totales < 200\n",
    "- \"Argent\" : dépenses totales 200-500\n",
    "- \"Or\" : dépenses totales > 500\n",
    "Affichez le nombre de clients dans chaque segment.\n",
    "\n",
    "**Q5.3** Téléchargez des données depuis Wikidata sur :\n",
    "- Les pays et leurs populations\n",
    "- Les villes et leurs pays\n",
    "- Les universités et leurs villes\n",
    "Effectuez des jointures pour trouver les 10 pays avec le plus d'universités, normalisé par la population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vos solutions ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 6 : Partitionnement, mise en cache et optimisation [★★★]\n",
    "\n",
    "### Optimisation des performances\n",
    "\n",
    "Les performances de Spark dépendent fortement de :\n",
    "1. **Partitionnement** : Comment les données sont distribuées entre les nœuds\n",
    "2. **Mise en cache** : Garder les données fréquemment accédées en mémoire\n",
    "3. **Éviter les shuffles** : Minimiser les mouvements de données entre les nœuds\n",
    "4. **Variables broadcast** : Partager efficacement les petites données entre les nœuds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprendre les partitions\n",
    "grand_df = spark.range(1000000)  # 1 million de lignes\n",
    "\n",
    "print(f\"Partitions par défaut : {grand_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartitionner à un nombre spécifique\n",
    "repartitionne = grand_df.repartition(8)\n",
    "print(f\"Après repartition(8) : {repartitionne.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Coalesce (réduire les partitions sans shuffle complet)\n",
    "fusionne = grand_df.coalesce(4)\n",
    "print(f\"Après coalesce(4) : {fusionne.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitionner par colonne (utile pour le filtrage)\n",
    "ventes_partitionnees = ventes_df.repartition(4, \"categorie\")\n",
    "print(f\"Partitions : {ventes_partitionnees.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Voir le contenu des partitions\n",
    "def afficher_info_partitions(df):\n",
    "    partitions = df.rdd.glom().collect()\n",
    "    for i, partition in enumerate(partitions):\n",
    "        print(f\"Partition {i} : {len(partition)} lignes\")\n",
    "\n",
    "afficher_info_partitions(ventes_partitionnees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mise en cache des DataFrames\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# Créer un grand DataFrame\n",
    "grand_df = spark.range(100000).withColumn(\"carre\", col(\"id\") ** 2)\n",
    "\n",
    "# Sans mise en cache - calculs multiples\n",
    "import time\n",
    "\n",
    "debut = time.time()\n",
    "comptage1 = grand_df.filter(col(\"carre\") > 1000).count()\n",
    "comptage2 = grand_df.filter(col(\"carre\") > 2000).count()\n",
    "comptage3 = grand_df.filter(col(\"carre\") > 3000).count()\n",
    "print(f\"Sans mise en cache : {time.time() - debut:.4f}s\")\n",
    "\n",
    "# Avec mise en cache\n",
    "grand_df.cache()  # ou grand_df.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "# La première action déclenche la mise en cache\n",
    "_ = grand_df.count()\n",
    "\n",
    "debut = time.time()\n",
    "comptage1 = grand_df.filter(col(\"carre\") > 1000).count()\n",
    "comptage2 = grand_df.filter(col(\"carre\") > 2000).count()\n",
    "comptage3 = grand_df.filter(col(\"carre\") > 3000).count()\n",
    "print(f\"Avec mise en cache : {time.time() - debut:.4f}s\")\n",
    "\n",
    "# Libérer le cache quand terminé\n",
    "grand_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Niveaux de stockage\n",
    "print(\"Niveaux de stockage disponibles :\")\n",
    "print(f\"  MEMORY_ONLY : {StorageLevel.MEMORY_ONLY}\")\n",
    "print(f\"  MEMORY_AND_DISK : {StorageLevel.MEMORY_AND_DISK}\")\n",
    "print(f\"  DISK_ONLY : {StorageLevel.DISK_ONLY}\")\n",
    "print(f\"  MEMORY_ONLY_SER : {StorageLevel.MEMORY_ONLY_SER}\")\n",
    "print(f\"  MEMORY_AND_DISK_SER : {StorageLevel.MEMORY_AND_DISK_SER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables broadcast pour les petits ensembles de données dans les jointures\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Petite table de référence\n",
    "donnees_ref = [(\"A\", \"Catégorie A\"), (\"B\", \"Catégorie B\"), (\"C\", \"Catégorie C\")]\n",
    "ref_df = spark.createDataFrame(donnees_ref, [\"code\", \"description\"])\n",
    "\n",
    "# Grande table de faits\n",
    "donnees_faits = [(i, [\"A\", \"B\", \"C\"][i % 3], i * 10) for i in range(10000)]\n",
    "faits_df = spark.createDataFrame(donnees_faits, [\"id\", \"code\", \"valeur\"])\n",
    "\n",
    "# Jointure normale\n",
    "debut = time.time()\n",
    "resultat1 = faits_df.join(ref_df, \"code\")\n",
    "_ = resultat1.count()\n",
    "print(f\"Jointure normale : {time.time() - debut:.4f}s\")\n",
    "\n",
    "# Jointure broadcast (la petite table est diffusée à tous les nœuds)\n",
    "debut = time.time()\n",
    "resultat2 = faits_df.join(broadcast(ref_df), \"code\")\n",
    "_ = resultat2.count()\n",
    "print(f\"Jointure broadcast : {time.time() - debut:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expliquer le plan d'exécution de la requête\n",
    "df = ventes_df.filter(col(\"chiffre_affaires\") > 100).groupBy(\"categorie\").sum(\"chiffre_affaires\")\n",
    "\n",
    "# Explication simple\n",
    "print(\"=== Explication simple ===\")\n",
    "df.explain()\n",
    "\n",
    "# Explication étendue\n",
    "print(\"\\n=== Explication étendue ===\")\n",
    "df.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démonstration du predicate pushdown\n",
    "# Lors de la lecture de fichiers, Spark peut pousser les filtres vers la source de données\n",
    "\n",
    "# Écrire les données exemple en parquet\n",
    "ventes_df.write.mode(\"overwrite\").parquet(\"donnees_ventes.parquet\")\n",
    "\n",
    "# Lecture avec filtre - Spark ne lira que les données nécessaires\n",
    "filtre = spark.read.parquet(\"donnees_ventes.parquet\").filter(col(\"categorie\") == \"Electronique\")\n",
    "print(\"Plan de requête avec predicate pushdown :\")\n",
    "filtre.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Élagage de colonnes - ne lire que les colonnes nécessaires\n",
    "# Sélectionner des colonnes spécifiques avant d'appliquer des transformations\n",
    "\n",
    "# Inefficace : lit toutes les colonnes\n",
    "toutes_cols = spark.read.parquet(\"donnees_ventes.parquet\")\n",
    "resultat_tout = toutes_cols.filter(col(\"chiffre_affaires\") > 1000).select(\"produit\", \"chiffre_affaires\")\n",
    "\n",
    "# Efficace : ne lit que les colonnes nécessaires\n",
    "selectionnees = spark.read.parquet(\"donnees_ventes.parquet\").select(\"produit\", \"chiffre_affaires\")\n",
    "resultat_select = selectionnees.filter(col(\"chiffre_affaires\") > 1000)\n",
    "\n",
    "print(\"Les deux approches produisent le même résultat :\")\n",
    "resultat_tout.show()\n",
    "resultat_select.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercice 6\n",
    "\n",
    "**Q6.1** Créez un DataFrame avec 10 millions de lignes. Comparez les performances de :\n",
    "- Exécuter la même agrégation 5 fois sans mise en cache\n",
    "- L'exécuter 5 fois avec mise en cache\n",
    "- Utiliser différents niveaux de stockage (MEMORY_ONLY vs MEMORY_AND_DISK)\n",
    "\n",
    "**Q6.2** Démontrez l'impact du partitionnement sur les performances des jointures :\n",
    "- Créez deux grands DataFrames (1 million de lignes chacun)\n",
    "- Joignez-les avec différentes stratégies de partitionnement\n",
    "- Comparez les temps d'exécution et expliquez les différences\n",
    "\n",
    "**Q6.3** Analysez les plans de requêtes :\n",
    "- Écrivez une requête complexe avec filtres, jointures et agrégations\n",
    "- Utilisez `explain()` pour comprendre le plan d'exécution\n",
    "- Optimisez la requête en fonction de l'analyse du plan\n",
    "- Comparez les performances avant/après"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vos solutions ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 7 : Traitement d'ensembles de données à grande échelle [★★★]\n",
    "\n",
    "### Formats de fichiers en colonnes\n",
    "\n",
    "Pour le traitement de données à grande échelle, les formats en colonnes comme Parquet et ORC offrent des avantages significatifs :\n",
    "- **Compression efficace** : Les valeurs similaires stockées ensemble se compressent mieux\n",
    "- **Élagage de colonnes** : Ne lire que les colonnes nécessaires\n",
    "- **Predicate pushdown** : Filtrer au niveau du stockage\n",
    "- **Évolution du schéma** : Ajouter/supprimer des colonnes sans réécrire les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer un plus grand ensemble de données pour la démonstration\n",
    "import random\n",
    "from pyspark.sql.functions import rand, randn, floor, concat, lit\n",
    "\n",
    "# Créer un plus grand ensemble de données de ventes\n",
    "grandes_ventes = spark.range(100000) \\\n",
    "    .withColumn(\"date\", concat(lit(\"2024-\"), \n",
    "                               ((floor(rand() * 12) + 1).cast(\"string\")), \n",
    "                               lit(\"-\"), \n",
    "                               ((floor(rand() * 28) + 1).cast(\"string\")))) \\\n",
    "    .withColumn(\"categorie\", \n",
    "                when(rand() < 0.3, \"Electronique\")\n",
    "                .when(rand() < 0.6, \"Vetements\")\n",
    "                .otherwise(\"Livres\")) \\\n",
    "    .withColumn(\"prix\", floor(rand() * 1000) + 10) \\\n",
    "    .withColumn(\"quantite\", floor(rand() * 50) + 1) \\\n",
    "    .withColumn(\"chiffre_affaires\", col(\"prix\") * col(\"quantite\"))\n",
    "\n",
    "print(f\"{grandes_ventes.count()} lignes générées\")\n",
    "grandes_ventes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Écrire dans différents formats et comparer les tailles\n",
    "import os\n",
    "\n",
    "# Écrire en CSV\n",
    "grandes_ventes.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"grandes_ventes_csv\")\n",
    "\n",
    "# Écrire en Parquet (compression par défaut : snappy)\n",
    "grandes_ventes.write.mode(\"overwrite\").parquet(\"grandes_ventes_parquet\")\n",
    "\n",
    "# Écrire en Parquet avec compression gzip\n",
    "grandes_ventes.write.mode(\"overwrite\").option(\"compression\", \"gzip\").parquet(\"grandes_ventes_parquet_gzip\")\n",
    "\n",
    "# Écrire en ORC\n",
    "grandes_ventes.write.mode(\"overwrite\").orc(\"grandes_ventes_orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer les tailles de fichiers (simplifié - l'implémentation réelle dépend du système de fichiers)\n",
    "def obtenir_taille_dossier(chemin):\n",
    "    \"\"\"Calculer la taille totale des fichiers dans un dossier\"\"\"\n",
    "    total = 0\n",
    "    if os.path.exists(chemin):\n",
    "        for racine, dossiers, fichiers in os.walk(chemin):\n",
    "            for f in fichiers:\n",
    "                total += os.path.getsize(os.path.join(racine, f))\n",
    "    return total\n",
    "\n",
    "taille_csv = obtenir_taille_dossier(\"grandes_ventes_csv\")\n",
    "taille_parquet = obtenir_taille_dossier(\"grandes_ventes_parquet\")\n",
    "taille_parquet_gzip = obtenir_taille_dossier(\"grandes_ventes_parquet_gzip\")\n",
    "taille_orc = obtenir_taille_dossier(\"grandes_ventes_orc\")\n",
    "\n",
    "print(f\"Taille CSV : {taille_csv / 1024:.2f} Ko\")\n",
    "print(f\"Taille Parquet (snappy) : {taille_parquet / 1024:.2f} Ko\")\n",
    "print(f\"Taille Parquet (gzip) : {taille_parquet_gzip / 1024:.2f} Ko\")\n",
    "print(f\"Taille ORC : {taille_orc / 1024:.2f} Ko\")\n",
    "\n",
    "if taille_csv > 0:\n",
    "    print(f\"\\nRatios de compression vs CSV :\")\n",
    "    print(f\"  Parquet (snappy) : {taille_csv / taille_parquet:.2f}x\")\n",
    "    print(f\"  Parquet (gzip) : {taille_csv / taille_parquet_gzip:.2f}x\")\n",
    "    print(f\"  ORC : {taille_csv / taille_orc:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer les performances de lecture\n",
    "import time\n",
    "\n",
    "def benchmark_lecture(chemin, type_format):\n",
    "    debut = time.time()\n",
    "    if type_format == \"csv\":\n",
    "        df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(chemin)\n",
    "    elif type_format == \"parquet\":\n",
    "        df = spark.read.parquet(chemin)\n",
    "    else:\n",
    "        df = spark.read.orc(chemin)\n",
    "    comptage = df.count()\n",
    "    return time.time() - debut\n",
    "\n",
    "print(\"Performances de lecture (secondes) :\")\n",
    "print(f\"  CSV : {benchmark_lecture('grandes_ventes_csv', 'csv'):.4f}s\")\n",
    "print(f\"  Parquet : {benchmark_lecture('grandes_ventes_parquet', 'parquet'):.4f}s\")\n",
    "print(f\"  ORC : {benchmark_lecture('grandes_ventes_orc', 'orc'):.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Écritures partitionnées - partitionner par catégorie\n",
    "grandes_ventes.write.mode(\"overwrite\").partitionBy(\"categorie\").parquet(\"grandes_ventes_partitionne\")\n",
    "\n",
    "# Lister les répertoires de partition\n",
    "for element in os.listdir(\"grandes_ventes_partitionne\"):\n",
    "    if os.path.isdir(os.path.join(\"grandes_ventes_partitionne\", element)):\n",
    "        print(f\"Partition : {element}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture de données partitionnées - élagage de partition\n",
    "# Ne lit que la partition Electronique\n",
    "electronique = spark.read.parquet(\"grandes_ventes_partitionne\").filter(col(\"categorie\") == \"Electronique\")\n",
    "\n",
    "print(\"Plan de requête avec élagage de partition :\")\n",
    "electronique.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évolution du schéma avec Parquet\n",
    "# Données originales\n",
    "donnees_originales = [(1, \"A\", 100), (2, \"B\", 200)]\n",
    "df_original = spark.createDataFrame(donnees_originales, [\"id\", \"code\", \"valeur\"])\n",
    "df_original.write.mode(\"overwrite\").parquet(\"test_evolution_schema\")\n",
    "\n",
    "# Nouvelles données avec colonne supplémentaire\n",
    "nouvelles_donnees = [(3, \"C\", 300, \"nouvelle_info\"), (4, \"D\", 400, \"plus_info\")]\n",
    "nouveau_df = spark.createDataFrame(nouvelles_donnees, [\"id\", \"code\", \"valeur\", \"extra\"])\n",
    "nouveau_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").parquet(\"test_evolution_schema\")\n",
    "\n",
    "# Lire avec schéma fusionné\n",
    "fusionne = spark.read.option(\"mergeSchema\", \"true\").parquet(\"test_evolution_schema\")\n",
    "print(\"Schéma fusionné :\")\n",
    "fusionne.printSchema()\n",
    "fusionne.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Travailler avec plusieurs sources de données\n",
    "# Exemple : Joindre des données de différents fichiers\n",
    "\n",
    "# Écrire des données de référence\n",
    "categories = [(\"Electronique\", \"Produits technologiques\", 0.1), \n",
    "              (\"Vetements\", \"Habillement\", 0.05), \n",
    "              (\"Livres\", \"Matériel de lecture\", 0.0)]\n",
    "categories_df = spark.createDataFrame(categories, [\"categorie\", \"description\", \"taux_taxe\"])\n",
    "categories_df.write.mode(\"overwrite\").parquet(\"categories_ref\")\n",
    "\n",
    "# Lire et joindre\n",
    "ventes = spark.read.parquet(\"grandes_ventes_parquet\")\n",
    "categories = spark.read.parquet(\"categories_ref\")\n",
    "\n",
    "enrichi = ventes.join(broadcast(categories), \"categorie\")\n",
    "enrichi = enrichi.withColumn(\"taxe\", col(\"chiffre_affaires\") * col(\"taux_taxe\"))\n",
    "enrichi = enrichi.withColumn(\"total\", col(\"chiffre_affaires\") + col(\"taxe\"))\n",
    "\n",
    "print(\"Données de ventes enrichies :\")\n",
    "enrichi.select(\"id\", \"categorie\", \"chiffre_affaires\", \"taux_taxe\", \"taxe\", \"total\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyer les fichiers temporaires\n",
    "import shutil\n",
    "\n",
    "dossiers_nettoyage = [\n",
    "    \"grandes_ventes_csv\", \"grandes_ventes_parquet\", \"grandes_ventes_parquet_gzip\",\n",
    "    \"grandes_ventes_orc\", \"grandes_ventes_partitionne\", \"test_evolution_schema\",\n",
    "    \"categories_ref\", \"donnees_ventes.parquet\", \"languages.orc\", \"languages.parquet\", \"languages.csv\"\n",
    "]\n",
    "\n",
    "for d in dossiers_nettoyage:\n",
    "    if os.path.exists(d):\n",
    "        shutil.rmtree(d) if os.path.isdir(d) else os.remove(d)\n",
    "        print(f\"Nettoyé : {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercice 7\n",
    "\n",
    "**Q7.1** Téléchargez un grand ensemble de données (au moins 1 million de lignes) depuis une source publique (ex: données des taxis de NYC, vues des pages Wikipedia) :\n",
    "- Chargez-le dans Spark\n",
    "- Sauvegardez en formats CSV, Parquet et ORC\n",
    "- Comparez les tailles de fichiers et les temps de lecture/écriture\n",
    "- Testez les performances des requêtes sur chaque format\n",
    "\n",
    "**Q7.2** Implémentez un pipeline ETL (Extract, Transform, Load) :\n",
    "- Lisez les données depuis plusieurs fichiers sources\n",
    "- Nettoyez et transformez les données (gérez les valeurs nulles, normalisez les valeurs)\n",
    "- Joignez avec des données de référence\n",
    "- Écrivez dans des fichiers Parquet partitionnés\n",
    "- Mesurez et optimisez les performances\n",
    "\n",
    "**Q7.3** Créez un schéma en étoile d'entrepôt de données :\n",
    "- Concevez des tables de faits et de dimensions\n",
    "- Générez des données synthétiques réalistes (10 millions+ de lignes)\n",
    "- Implémentez des requêtes analytiques courantes\n",
    "- Optimisez en utilisant le partitionnement, la mise en cache et les jointures broadcast\n",
    "- Documentez les métriques de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vos solutions ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Résumé\n",
    "\n",
    "Dans ce TP, vous avez appris :\n",
    "\n",
    "1. **Architecture Spark** : Comprendre les drivers, executors et cluster managers\n",
    "2. **RDD** : Créer et manipuler des ensembles de données distribués résilients\n",
    "3. **Transformations vs Actions** : Évaluation paresseuse et optimisation\n",
    "4. **DataFrames** : Traitement de données structurées avec schémas\n",
    "5. **Spark SQL** : Interroger les données avec la syntaxe SQL\n",
    "6. **Opérations avancées** : Jointures, fonctions fenêtrées, agrégations\n",
    "7. **Optimisation des performances** : Partitionnement, mise en cache, variables broadcast\n",
    "8. **Formats de fichiers** : Travailler avec Parquet, ORC et CSV à grande échelle\n",
    "\n",
    "### Points clés à retenir\n",
    "\n",
    "- Utilisez les DataFrames plutôt que les RDD quand c'est possible pour une meilleure optimisation\n",
    "- Mettez en cache les résultats intermédiaires utilisés plusieurs fois\n",
    "- Utilisez les formats en colonnes (Parquet/ORC) pour les grands ensembles de données\n",
    "- Partitionnez les données par les colonnes fréquemment filtrées\n",
    "- Utilisez les jointures broadcast pour les petites tables de référence\n",
    "- Surveillez les plans de requête avec `explain()` pour identifier les goulots d'étranglement\n",
    "\n",
    "### Lectures complémentaires\n",
    "\n",
    "- [Documentation Apache Spark](https://spark.apache.org/docs/latest/)\n",
    "- [Référence API PySpark](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Spark: The Definitive Guide](https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/)\n",
    "- [Learning Spark, 2nd Edition](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrêter la session Spark quand terminé\n",
    "spark.stop()\n",
    "sc.stop()\n",
    "print(\"Session Spark arrêtée.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
