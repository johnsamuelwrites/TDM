{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 7 : Kubernetes pour les Pipelines de Données Évolutifs\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "Ce TP introduit Kubernetes (K8s), la plateforme d'orchestration de conteneurs de référence dans l'industrie. Vous apprendrez à déployer, mettre à l'échelle et gérer des applications de traitement de données conteneurisées dans un environnement proche de la production.\n",
    "\n",
    "### Objectifs d'apprentissage\n",
    "* Comprendre l'architecture Kubernetes et ses concepts fondamentaux\n",
    "* Déployer des applications avec des Pods, Deployments et Services\n",
    "* Gérer la configuration avec des ConfigMaps et des Secrets\n",
    "* Mettre en œuvre le stockage persistant avec des PersistentVolumes\n",
    "* Exécuter des tâches par lots et planifiées avec des Jobs et CronJobs\n",
    "* Mettre à l'échelle automatiquement les applications avec le Horizontal Pod Autoscaler\n",
    "* Déployer des applications Spark sur Kubernetes\n",
    "* Surveiller et dépanner les charges de travail Kubernetes\n",
    "\n",
    "### Prérequis\n",
    "* Achèvement du TP 6 (Docker)\n",
    "* Docker Desktop avec Kubernetes activé, OU\n",
    "* Minikube installé ([Guide d'installation](https://minikube.sigs.k8s.io/docs/start/))\n",
    "* CLI kubectl installé ([Guide d'installation](https://kubernetes.io/docs/tasks/tools/))\n",
    "\n",
    "### Vérification de l'installation\n",
    "\n",
    "```bash\n",
    "# Vérifier la version de kubectl\n",
    "kubectl version --client\n",
    "\n",
    "# Vérifier l'état du cluster\n",
    "kubectl cluster-info\n",
    "\n",
    "# Lister les nœuds\n",
    "kubectl get nodes\n",
    "```\n",
    "\n",
    "### Aperçu des exercices\n",
    "\n",
    "| Exercice | Sujet | Difficulté |\n",
    "|----------|-------|------------|\n",
    "| 1 | Architecture Kubernetes et bases de kubectl | ★ |\n",
    "| 2 | Pods et Deployments | ★ |\n",
    "| 3 | Services et mise en réseau | ★★ |\n",
    "| 4 | ConfigMaps et Secrets | ★★ |\n",
    "| 5 | Stockage persistant | ★★ |\n",
    "| 6 | Jobs et CronJobs pour le traitement par lots | ★★ |\n",
    "| 7 | Mise à l'échelle automatique horizontale des Pods | ★★★ |\n",
    "| 8 | Déploiement de pipelines de traitement de données | ★★★ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 1 : Architecture Kubernetes et bases de kubectl [★]\n",
    "\n",
    "### Architecture Kubernetes\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│                        Plan de contrôle                            │\n",
    "│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐             │\n",
    "│  │  API Server  │  │  Scheduler   │  │  Controller  │             │\n",
    "│  │              │  │              │  │   Manager    │             │\n",
    "│  └──────────────┘  └──────────────┘  └──────────────┘             │\n",
    "│                           │                                        │\n",
    "│                    ┌──────┴──────┐                                │\n",
    "│                    │    etcd     │                                │\n",
    "│                    │  (Stockage) │                                │\n",
    "│                    └─────────────┘                                │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "         ┌────────────────────┼────────────────────┐\n",
    "         │                    │                    │\n",
    "         ▼                    ▼                    ▼\n",
    "┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐\n",
    "│  Nœud Worker    │  │  Nœud Worker    │  │  Nœud Worker    │\n",
    "│  ┌───────────┐  │  │  ┌───────────┐  │  │  ┌───────────┐  │\n",
    "│  │  kubelet  │  │  │  │  kubelet  │  │  │  │  kubelet  │  │\n",
    "│  └───────────┘  │  │  └───────────┘  │  │  └───────────┘  │\n",
    "│  ┌───────────┐  │  │  ┌───────────┐  │  │  ┌───────────┐  │\n",
    "│  │kube-proxy │  │  │  │kube-proxy │  │  │  │kube-proxy │  │\n",
    "│  └───────────┘  │  │  └───────────┘  │  │  └───────────┘  │\n",
    "│  ┌───┐ ┌───┐    │  │  ┌───┐ ┌───┐    │  │  ┌───┐ ┌───┐    │\n",
    "│  │Pod│ │Pod│    │  │  │Pod│ │Pod│    │  │  │Pod│ │Pod│    │\n",
    "│  └───┘ └───┘    │  │  └───┘ └───┘    │  │  └───┘ └───┘    │\n",
    "└─────────────────┘  └─────────────────┘  └─────────────────┘\n",
    "```\n",
    "\n",
    "### Composants clés\n",
    "\n",
    "**Plan de contrôle :**\n",
    "- **API Server** : Point d'entrée pour toutes les commandes REST\n",
    "- **etcd** : Stockage clé-valeur distribué pour l'état du cluster\n",
    "- **Scheduler** : Assigne les Pods aux nœuds\n",
    "- **Controller Manager** : Exécute les boucles de contrôle (ReplicaSet, Deployment, etc.)\n",
    "\n",
    "**Nœuds Worker :**\n",
    "- **kubelet** : Agent qui s'exécute sur chaque nœud\n",
    "- **kube-proxy** : Proxy réseau pour la mise en réseau des services\n",
    "- **Container Runtime** : Docker, containerd ou CRI-O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commandes kubectl de base\n",
    "\n",
    "```bash\n",
    "# Obtenir les informations du cluster\n",
    "kubectl cluster-info\n",
    "\n",
    "# Lister tous les nœuds\n",
    "kubectl get nodes\n",
    "\n",
    "# Lister tous les namespaces\n",
    "kubectl get namespaces\n",
    "\n",
    "# Lister toutes les ressources dans le namespace courant\n",
    "kubectl get all\n",
    "\n",
    "# Lister les pods avec plus de détails\n",
    "kubectl get pods -o wide\n",
    "\n",
    "# Décrire une ressource\n",
    "kubectl describe pod <nom-du-pod>\n",
    "\n",
    "# Afficher les logs\n",
    "kubectl logs <nom-du-pod>\n",
    "\n",
    "# Exécuter une commande dans un pod\n",
    "kubectl exec -it <nom-du-pod> -- /bin/bash\n",
    "\n",
    "# Appliquer une configuration\n",
    "kubectl apply -f <fichier.yaml>\n",
    "\n",
    "# Supprimer une ressource\n",
    "kubectl delete -f <fichier.yaml>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travailler avec les Namespaces\n",
    "\n",
    "Les namespaces fournissent une isolation et une organisation pour les ressources.\n",
    "\n",
    "```bash\n",
    "# Créer un namespace\n",
    "kubectl create namespace data-processing\n",
    "\n",
    "# Lister les pods dans un namespace spécifique\n",
    "kubectl get pods -n data-processing\n",
    "\n",
    "# Définir le namespace par défaut pour le contexte courant\n",
    "kubectl config set-context --current --namespace=data-processing\n",
    "\n",
    "# Lister toutes les ressources dans tous les namespaces\n",
    "kubectl get pods --all-namespaces\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifestes YAML\n",
    "\n",
    "Kubernetes utilise des fichiers YAML pour définir les ressources. Structure de base :\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1              # Version de l'API\n",
    "kind: Pod                   # Type de ressource\n",
    "metadata:\n",
    "  name: my-pod              # Nom de la ressource\n",
    "  namespace: default        # Namespace\n",
    "  labels:                   # Labels clé-valeur\n",
    "    app: myapp\n",
    "spec:                       # Spécification de la ressource\n",
    "  # ... champs spécifiques à la ressource\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercice 1\n",
    "\n",
    "**Q1.1** Explorez votre cluster Kubernetes :\n",
    "- Listez tous les nœuds et leur état\n",
    "- Décrivez un nœud pour voir sa capacité et ses ressources allouables\n",
    "- Listez tous les namespaces et pods du cluster\n",
    "\n",
    "**Q1.2** Créez un namespace appelé `tdm-practicals` et définissez-le comme votre namespace par défaut.\n",
    "\n",
    "**Q1.3** Utilisez `kubectl explain` pour explorer la ressource Pod :\n",
    "- Quels champs sont disponibles dans `spec.containers` ?\n",
    "- Quelle est la différence entre `resources.limits` et `resources.requests` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 2 : Pods et Deployments [★]\n",
    "\n",
    "### Pods\n",
    "\n",
    "Un Pod est la plus petite unité déployable dans Kubernetes. Il peut contenir un ou plusieurs conteneurs qui partagent le stockage et le réseau.\n",
    "\n",
    "**Pod simple (pod.yaml) :**\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: python-pod\n",
    "  labels:\n",
    "    app: python-demo\n",
    "spec:\n",
    "  containers:\n",
    "  - name: python\n",
    "    image: python:3.10-slim\n",
    "    command: [\"python\", \"-c\"]\n",
    "    args:\n",
    "    - |\n",
    "      import time\n",
    "      while True:\n",
    "          print(f\"Bonjour depuis Kubernetes à {time.ctime()}\")\n",
    "          time.sleep(5)\n",
    "    resources:\n",
    "      requests:\n",
    "        memory: \"64Mi\"\n",
    "        cpu: \"100m\"\n",
    "      limits:\n",
    "        memory: \"128Mi\"\n",
    "        cpu: \"200m\"\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Créer le pod\n",
    "kubectl apply -f pod.yaml\n",
    "\n",
    "# Afficher les logs\n",
    "kubectl logs python-pod -f\n",
    "\n",
    "# Supprimer le pod\n",
    "kubectl delete pod python-pod\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployments\n",
    "\n",
    "Les Deployments gèrent les ReplicaSets et fournissent des mises à jour déclaratives pour les Pods.\n",
    "\n",
    "**Deployment (deployment.yaml) :**\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: data-processor\n",
    "  labels:\n",
    "    app: data-processor\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: data-processor\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: data-processor\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: processor\n",
    "        image: python:3.10-slim\n",
    "        command: [\"python\", \"-c\"]\n",
    "        args:\n",
    "        - |\n",
    "          import socket\n",
    "          import time\n",
    "          hostname = socket.gethostname()\n",
    "          while True:\n",
    "              print(f\"Traitement sur {hostname}\")\n",
    "              time.sleep(10)\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"64Mi\"\n",
    "            cpu: \"100m\"\n",
    "          limits:\n",
    "            memory: \"128Mi\"\n",
    "            cpu: \"200m\"\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Créer le deployment\n",
    "kubectl apply -f deployment.yaml\n",
    "\n",
    "# Afficher l'état du deployment\n",
    "kubectl get deployments\n",
    "kubectl get pods\n",
    "\n",
    "# Mettre à l'échelle le deployment\n",
    "kubectl scale deployment data-processor --replicas=5\n",
    "\n",
    "# Afficher l'historique du deployment\n",
    "kubectl rollout history deployment/data-processor\n",
    "\n",
    "# Mettre à jour le deployment (changer l'image, etc.)\n",
    "kubectl set image deployment/data-processor processor=python:3.11-slim\n",
    "\n",
    "# Revenir à la version précédente\n",
    "kubectl rollout undo deployment/data-processor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycle de vie des Pods et vérifications de santé\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: web-app\n",
    "spec:\n",
    "  replicas: 2\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: web-app\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: web-app\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: web\n",
    "        image: python:3.10-slim\n",
    "        command: [\"python\", \"-c\"]\n",
    "        args:\n",
    "        - |\n",
    "          from http.server import HTTPServer, SimpleHTTPRequestHandler\n",
    "          print('Démarrage du serveur sur le port 8080')\n",
    "          HTTPServer(('0.0.0.0', 8080), SimpleHTTPRequestHandler).serve_forever()\n",
    "        ports:\n",
    "        - containerPort: 8080\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /\n",
    "            port: 8080\n",
    "          initialDelaySeconds: 10\n",
    "          periodSeconds: 5\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /\n",
    "            port: 8080\n",
    "          initialDelaySeconds: 5\n",
    "          periodSeconds: 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercice 2\n",
    "\n",
    "**Q2.1** Créez un Deployment pour une application de traitement de données qui :\n",
    "- Exécute 3 réplicas\n",
    "- Utilise l'image Python\n",
    "- Traite des données en boucle\n",
    "- Possède des limites de ressources appropriées\n",
    "- Inclut des sondes de vivacité et de disponibilité (liveness et readiness probes)\n",
    "\n",
    "**Q2.2** Expérimentez avec la mise à l'échelle :\n",
    "- Mettez à l'échelle le deployment à 5 réplicas\n",
    "- Observez comment Kubernetes distribue les pods sur les nœuds\n",
    "- Réduisez à 2 réplicas et observez la terminaison des pods\n",
    "\n",
    "**Q2.3** Effectuez une mise à jour progressive (rolling update) :\n",
    "- Mettez à jour la version de l'image\n",
    "- Observez la progression du déploiement\n",
    "- Simulez un déploiement échoué et effectuez un rollback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 3 : Services et mise en réseau [★★]\n",
    "\n",
    "### Types de Services\n",
    "\n",
    "Les Services exposent les pods et fournissent une mise en réseau stable.\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────┐\n",
    "│                      Types de Services                     │\n",
    "├────────────────┬───────────────────────────────────────────┤\n",
    "│ ClusterIP      │ IP interne au cluster (par défaut)        │\n",
    "│ NodePort       │ Expose sur l'IP de chaque nœud à un port  │\n",
    "│                │ statique                                  │\n",
    "│ LoadBalancer   │ Répartiteur de charge externe (cloud)     │\n",
    "│ ExternalName   │ Mappe vers un nom DNS externe             │\n",
    "└────────────────┴───────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Service ClusterIP\n",
    "\n",
    "**service-clusterip.yaml :**\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: data-processor-service\n",
    "spec:\n",
    "  type: ClusterIP\n",
    "  selector:\n",
    "    app: data-processor\n",
    "  ports:\n",
    "  - port: 80          # Port du service\n",
    "    targetPort: 8080  # Port du conteneur\n",
    "    protocol: TCP\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Service NodePort\n",
    "\n",
    "**service-nodeport.yaml :**\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: web-app-nodeport\n",
    "spec:\n",
    "  type: NodePort\n",
    "  selector:\n",
    "    app: web-app\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8080\n",
    "    nodePort: 30080   # Optionnel : 30000-32767\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Accéder au service\n",
    "# http://<ip-du-noeud>:30080\n",
    "\n",
    "# Avec minikube\n",
    "minikube service web-app-nodeport --url\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple complet d'application web\n",
    "\n",
    "**webapp.yaml :**\n",
    "\n",
    "```yaml\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: flask-app\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: flask-app\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: flask-app\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: flask\n",
    "        image: python:3.10-slim\n",
    "        command: [\"/bin/bash\", \"-c\"]\n",
    "        args:\n",
    "        - |\n",
    "          pip install flask && python -c \"\n",
    "          from flask import Flask\n",
    "          import socket\n",
    "          app = Flask(__name__)\n",
    "          @app.route('/')\n",
    "          def hello():\n",
    "              return f'Bonjour depuis {socket.gethostname()}'\n",
    "          app.run(host='0.0.0.0', port=5000)\n",
    "          \"\n",
    "        ports:\n",
    "        - containerPort: 5000\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"128Mi\"\n",
    "            cpu: \"100m\"\n",
    "          limits:\n",
    "            memory: \"256Mi\"\n",
    "            cpu: \"200m\"\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: flask-app-service\n",
    "spec:\n",
    "  type: NodePort\n",
    "  selector:\n",
    "    app: flask-app\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 5000\n",
    "    nodePort: 30500\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Appliquer les deux ressources\n",
    "kubectl apply -f webapp.yaml\n",
    "\n",
    "# Tester la répartition de charge (plusieurs requêtes montrent différents noms d'hôte)\n",
    "for i in {1..10}; do curl http://localhost:30500; done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNS et découverte de services\n",
    "\n",
    "Kubernetes fournit une découverte de services basée sur DNS. Les services peuvent être accédés par :\n",
    "- `<nom-du-service>` (même namespace)\n",
    "- `<nom-du-service>.<namespace>` (entre namespaces)\n",
    "- `<nom-du-service>.<namespace>.svc.cluster.local` (FQDN)\n",
    "\n",
    "```bash\n",
    "# Tester le DNS depuis un pod\n",
    "kubectl run -it --rm debug --image=busybox -- nslookup flask-app-service\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercice 3\n",
    "\n",
    "**Q3.1** Créez une application multi-niveaux :\n",
    "- Deployment et Service Frontend (NodePort)\n",
    "- Deployment et Service Backend (ClusterIP)\n",
    "- Le frontend communique avec le backend via le nom du service\n",
    "\n",
    "**Q3.2** Testez la découverte de services :\n",
    "- Créez un pod de débogage\n",
    "- Utilisez `nslookup` et `curl` pour vérifier la connectivité des services\n",
    "- Documentez le processus de résolution DNS\n",
    "\n",
    "**Q3.3** Implémentez la répartition de charge :\n",
    "- Déployez 5 réplicas d'une application web\n",
    "- Faites plusieurs requêtes et identifiez quel pod traite chacune\n",
    "- Analysez la distribution de la charge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 4 : ConfigMaps et Secrets [★★]\n",
    "\n",
    "### ConfigMaps\n",
    "\n",
    "Les ConfigMaps stockent des données de configuration non sensibles.\n",
    "\n",
    "**configmap.yaml :**\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: app-config\n",
    "data:\n",
    "  # Paires clé-valeur\n",
    "  DATABASE_HOST: \"postgres-service\"\n",
    "  DATABASE_PORT: \"5432\"\n",
    "  LOG_LEVEL: \"INFO\"\n",
    "  \n",
    "  # Clés de type fichier\n",
    "  config.json: |\n",
    "    {\n",
    "      \"processing\": {\n",
    "        \"batch_size\": 100,\n",
    "        \"timeout\": 30\n",
    "      }\n",
    "    }\n",
    "```\n",
    "\n",
    "**Utilisation des ConfigMaps dans les Pods :**\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: config-demo\n",
    "spec:\n",
    "  containers:\n",
    "  - name: demo\n",
    "    image: python:3.10-slim\n",
    "    \n",
    "    # Méthode 1 : Variables d'environnement à partir de clés spécifiques\n",
    "    env:\n",
    "    - name: DB_HOST\n",
    "      valueFrom:\n",
    "        configMapKeyRef:\n",
    "          name: app-config\n",
    "          key: DATABASE_HOST\n",
    "    \n",
    "    # Méthode 2 : Toutes les clés comme variables d'environnement\n",
    "    envFrom:\n",
    "    - configMapRef:\n",
    "        name: app-config\n",
    "    \n",
    "    # Méthode 3 : Montage comme volume\n",
    "    volumeMounts:\n",
    "    - name: config-volume\n",
    "      mountPath: /etc/config\n",
    "      \n",
    "  volumes:\n",
    "  - name: config-volume\n",
    "    configMap:\n",
    "      name: app-config\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secrets\n",
    "\n",
    "Les Secrets stockent des données sensibles comme les mots de passe et les clés API.\n",
    "\n",
    "```bash\n",
    "# Créer un secret à partir de valeurs littérales\n",
    "kubectl create secret generic db-credentials \\\n",
    "  --from-literal=username=admin \\\n",
    "  --from-literal=password=secretpass123\n",
    "\n",
    "# Créer un secret à partir d'un fichier\n",
    "kubectl create secret generic tls-certs \\\n",
    "  --from-file=cert.pem \\\n",
    "  --from-file=key.pem\n",
    "\n",
    "# Afficher le secret (encodé en base64)\n",
    "kubectl get secret db-credentials -o yaml\n",
    "```\n",
    "\n",
    "**secret.yaml :**\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: db-credentials\n",
    "type: Opaque\n",
    "data:\n",
    "  # Les valeurs doivent être encodées en base64\n",
    "  # echo -n 'admin' | base64\n",
    "  username: YWRtaW4=\n",
    "  password: c2VjcmV0cGFzczEyMw==\n",
    "```\n",
    "\n",
    "**Utilisation des Secrets dans les Pods :**\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: secret-demo\n",
    "spec:\n",
    "  containers:\n",
    "  - name: demo\n",
    "    image: python:3.10-slim\n",
    "    env:\n",
    "    - name: DB_USERNAME\n",
    "      valueFrom:\n",
    "        secretKeyRef:\n",
    "          name: db-credentials\n",
    "          key: username\n",
    "    - name: DB_PASSWORD\n",
    "      valueFrom:\n",
    "        secretKeyRef:\n",
    "          name: db-credentials\n",
    "          key: password\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple complet : Application avec configuration\n",
    "\n",
    "**data-processor-config.yaml :**\n",
    "\n",
    "```yaml\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: processor-config\n",
    "data:\n",
    "  BATCH_SIZE: \"100\"\n",
    "  PROCESSING_MODE: \"parallel\"\n",
    "  LOG_LEVEL: \"DEBUG\"\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: processor-secrets\n",
    "type: Opaque\n",
    "stringData:  # Utilisez stringData pour les valeurs non encodées\n",
    "  API_KEY: \"your-secret-api-key\"\n",
    "  DATABASE_URL: \"postgresql://user:pass@host:5432/db\"\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: data-processor\n",
    "spec:\n",
    "  replicas: 2\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: data-processor\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: data-processor\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: processor\n",
    "        image: python:3.10-slim\n",
    "        command: [\"python\", \"-c\"]\n",
    "        args:\n",
    "        - |\n",
    "          import os\n",
    "          import time\n",
    "          print(f\"Taille du lot : {os.environ.get('BATCH_SIZE')}\")\n",
    "          print(f\"Mode : {os.environ.get('PROCESSING_MODE')}\")\n",
    "          print(f\"Clé API : {os.environ.get('API_KEY')[:5]}...\")\n",
    "          while True:\n",
    "              print(\"Traitement en cours...\")\n",
    "              time.sleep(10)\n",
    "        envFrom:\n",
    "        - configMapRef:\n",
    "            name: processor-config\n",
    "        - secretRef:\n",
    "            name: processor-secrets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercice 4\n",
    "\n",
    "**Q4.1** Créez une configuration pour une application de traitement de données :\n",
    "- ConfigMap avec les paramètres de traitement (taille du lot, timeout, chemins d'entrée/sortie)\n",
    "- Secret avec les identifiants de base de données\n",
    "- Deployment qui utilise les deux\n",
    "\n",
    "**Q4.2** Implémentez le rechargement à chaud de la configuration :\n",
    "- Montez le ConfigMap comme volume\n",
    "- Écrivez un script Python qui surveille les changements du fichier de configuration\n",
    "- Mettez à jour le ConfigMap et vérifiez que l'application détecte les changements\n",
    "\n",
    "**Q4.3** Créez une configuration prête pour la production :\n",
    "- Configurations séparées pour les environnements dev/staging/prod\n",
    "- Utilisez Kustomize pour gérer les surcharges spécifiques à chaque environnement\n",
    "- Documentez la stratégie de gestion de configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 5 : Stockage persistant [★★]\n",
    "\n",
    "### Concepts de stockage\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                   Architecture de stockage                   │\n",
    "│                                                              │\n",
    "│  ┌──────────────────┐                                       │\n",
    "│  │       Pod        │                                       │\n",
    "│  │  ┌────────────┐  │                                       │\n",
    "│  │  │  Montage   │  │◄──── PersistentVolumeClaim (PVC)     │\n",
    "│  │  │  de volume │  │            │                          │\n",
    "│  │  └────────────┘  │            │ liaison                  │\n",
    "│  └──────────────────┘            ▼                          │\n",
    "│                         ┌──────────────────┐                │\n",
    "│                         │ PersistentVolume │                │\n",
    "│                         │      (PV)        │                │\n",
    "│                         └────────┬─────────┘                │\n",
    "│                                  │                          │\n",
    "│                                  ▼                          │\n",
    "│                         ┌──────────────────┐                │\n",
    "│                         │Backend de stockage│               │\n",
    "│                         │ (NFS, EBS, etc.)  │               │\n",
    "│                         └──────────────────┘                │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PersistentVolume et PersistentVolumeClaim\n",
    "\n",
    "**pv-pvc.yaml :**\n",
    "\n",
    "```yaml\n",
    "---\n",
    "# PersistentVolume (généralement créé par l'administrateur)\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: data-pv\n",
    "spec:\n",
    "  capacity:\n",
    "    storage: 5Gi\n",
    "  accessModes:\n",
    "    - ReadWriteOnce      # RWO : nœud unique\n",
    "    # - ReadWriteMany    # RWX : plusieurs nœuds\n",
    "    # - ReadOnlyMany     # ROX : lecture seule sur plusieurs nœuds\n",
    "  persistentVolumeReclaimPolicy: Retain  # ou Delete\n",
    "  storageClassName: manual\n",
    "  hostPath:\n",
    "    path: /data/pv-data\n",
    "---\n",
    "# PersistentVolumeClaim (créé par l'utilisateur)\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: data-pvc\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 2Gi\n",
    "  storageClassName: manual\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilisation d'un PVC dans les Pods\n",
    "\n",
    "**pod-with-storage.yaml :**\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: data-writer\n",
    "spec:\n",
    "  containers:\n",
    "  - name: writer\n",
    "    image: python:3.10-slim\n",
    "    command: [\"python\", \"-c\"]\n",
    "    args:\n",
    "    - |\n",
    "      import time\n",
    "      from datetime import datetime\n",
    "      \n",
    "      counter = 0\n",
    "      while True:\n",
    "          with open('/data/output.txt', 'a') as f:\n",
    "              f.write(f\"{datetime.now()}: Enregistrement {counter}\\n\")\n",
    "          print(f\"Enregistrement {counter} écrit\")\n",
    "          counter += 1\n",
    "          time.sleep(5)\n",
    "    volumeMounts:\n",
    "    - name: data-volume\n",
    "      mountPath: /data\n",
    "  volumes:\n",
    "  - name: data-volume\n",
    "    persistentVolumeClaim:\n",
    "      claimName: data-pvc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StorageClass pour le provisionnement dynamique\n",
    "\n",
    "**storageclass.yaml :**\n",
    "\n",
    "```yaml\n",
    "apiVersion: storage.k8s.io/v1\n",
    "kind: StorageClass\n",
    "metadata:\n",
    "  name: fast-storage\n",
    "provisioner: kubernetes.io/gce-pd  # ou aws-ebs, azure-disk\n",
    "parameters:\n",
    "  type: pd-ssd\n",
    "reclaimPolicy: Delete\n",
    "volumeBindingMode: Immediate\n",
    "```\n",
    "\n",
    "**PVC dynamique :**\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: dynamic-pvc\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 10Gi\n",
    "  storageClassName: fast-storage  # Utilise la StorageClass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercice 5\n",
    "\n",
    "**Q5.1** Créez une configuration de stockage persistant pour un pipeline de données :\n",
    "- PersistentVolume pour les données d'entrée\n",
    "- PersistentVolume pour les données de sortie\n",
    "- Pod qui lit depuis l'entrée, traite, et écrit vers la sortie\n",
    "\n",
    "**Q5.2** Implémentez le partage de données entre pods :\n",
    "- Créez un PVC avec le mode d'accès ReadWriteMany\n",
    "- Déployez un pod écrivain et plusieurs pods lecteurs\n",
    "- Vérifiez que tous les pods peuvent accéder aux données partagées\n",
    "\n",
    "**Q5.3** Testez la persistance des données :\n",
    "- Déployez une base de données (PostgreSQL) avec stockage persistant\n",
    "- Insérez des données, supprimez le pod, vérifiez que les données persistent\n",
    "- Documentez le processus de sauvegarde et de restauration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 6 : Jobs et CronJobs pour le traitement par lots [★★]\n",
    "\n",
    "### Jobs\n",
    "\n",
    "Les Jobs exécutent un ou plusieurs pods jusqu'à leur achèvement.\n",
    "\n",
    "**job.yaml :**\n",
    "\n",
    "```yaml\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: data-processing-job\n",
    "spec:\n",
    "  completions: 5       # Nombre d'achèvements réussis requis\n",
    "  parallelism: 2       # Pods s'exécutant en parallèle\n",
    "  backoffLimit: 3      # Tentatives avant de marquer comme échec\n",
    "  activeDeadlineSeconds: 600  # Timeout\n",
    "  template:\n",
    "    spec:\n",
    "      restartPolicy: Never  # ou OnFailure\n",
    "      containers:\n",
    "      - name: processor\n",
    "        image: python:3.10-slim\n",
    "        command: [\"python\", \"-c\"]\n",
    "        args:\n",
    "        - |\n",
    "          import random\n",
    "          import time\n",
    "          import socket\n",
    "          \n",
    "          hostname = socket.gethostname()\n",
    "          work_time = random.randint(5, 15)\n",
    "          \n",
    "          print(f\"Job {hostname} démarré, s'exécutera pendant {work_time} secondes\")\n",
    "          time.sleep(work_time)\n",
    "          print(f\"Job {hostname} terminé avec succès\")\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Créer le job\n",
    "kubectl apply -f job.yaml\n",
    "\n",
    "# Surveiller la progression du job\n",
    "kubectl get jobs -w\n",
    "\n",
    "# Afficher les logs des pods\n",
    "kubectl logs job/data-processing-job\n",
    "\n",
    "# Supprimer le job et ses pods\n",
    "kubectl delete job data-processing-job\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CronJobs\n",
    "\n",
    "Les CronJobs exécutent des jobs selon un calendrier.\n",
    "\n",
    "**cronjob.yaml :**\n",
    "\n",
    "```yaml\n",
    "apiVersion: batch/v1\n",
    "kind: CronJob\n",
    "metadata:\n",
    "  name: data-aggregator\n",
    "spec:\n",
    "  schedule: \"*/5 * * * *\"  # Toutes les 5 minutes\n",
    "  # Format cron : minute heure jour-du-mois mois jour-de-la-semaine\n",
    "  # \"0 * * * *\"     - Toutes les heures\n",
    "  # \"0 0 * * *\"     - Tous les jours à minuit\n",
    "  # \"0 0 * * 0\"     - Tous les dimanches à minuit\n",
    "  \n",
    "  concurrencyPolicy: Forbid  # Allow, Forbid, Replace\n",
    "  successfulJobsHistoryLimit: 3\n",
    "  failedJobsHistoryLimit: 1\n",
    "  startingDeadlineSeconds: 200\n",
    "  \n",
    "  jobTemplate:\n",
    "    spec:\n",
    "      template:\n",
    "        spec:\n",
    "          restartPolicy: OnFailure\n",
    "          containers:\n",
    "          - name: aggregator\n",
    "            image: python:3.10-slim\n",
    "            command: [\"python\", \"-c\"]\n",
    "            args:\n",
    "            - |\n",
    "              from datetime import datetime\n",
    "              print(f\"Exécution de l'agrégation à {datetime.now()}\")\n",
    "              # Simuler le travail d'agrégation\n",
    "              import time\n",
    "              time.sleep(30)\n",
    "              print(\"Agrégation terminée\")\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Créer le cronjob\n",
    "kubectl apply -f cronjob.yaml\n",
    "\n",
    "# Lister les cronjobs\n",
    "kubectl get cronjobs\n",
    "\n",
    "# Afficher les jobs créés par le cronjob\n",
    "kubectl get jobs\n",
    "\n",
    "# Déclencher manuellement un job depuis le cronjob\n",
    "kubectl create job --from=cronjob/data-aggregator manual-run\n",
    "\n",
    "# Suspendre un cronjob\n",
    "kubectl patch cronjob data-aggregator -p '{\"spec\": {\"suspend\": true}}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline de traitement de données avec Jobs\n",
    "\n",
    "**etl-pipeline.yaml :**\n",
    "\n",
    "```yaml\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: etl-config\n",
    "data:\n",
    "  INPUT_PATH: \"/data/input\"\n",
    "  OUTPUT_PATH: \"/data/output\"\n",
    "---\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: etl-extract\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      restartPolicy: OnFailure\n",
    "      containers:\n",
    "      - name: extract\n",
    "        image: python:3.10-slim\n",
    "        command: [\"python\", \"-c\"]\n",
    "        args:\n",
    "        - |\n",
    "          import os\n",
    "          import json\n",
    "          \n",
    "          output_path = os.environ.get('OUTPUT_PATH', '/data/output')\n",
    "          os.makedirs(output_path, exist_ok=True)\n",
    "          \n",
    "          # Simuler l'extraction\n",
    "          data = [{'id': i, 'value': i * 10} for i in range(100)]\n",
    "          \n",
    "          with open(f'{output_path}/extracted.json', 'w') as f:\n",
    "              json.dump(data, f)\n",
    "          \n",
    "          print(f\"{len(data)} enregistrements extraits\")\n",
    "        envFrom:\n",
    "        - configMapRef:\n",
    "            name: etl-config\n",
    "        volumeMounts:\n",
    "        - name: data-volume\n",
    "          mountPath: /data\n",
    "      volumes:\n",
    "      - name: data-volume\n",
    "        persistentVolumeClaim:\n",
    "          claimName: etl-data-pvc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercice 6\n",
    "\n",
    "**Q6.1** Créez un job de traitement par lots qui :\n",
    "- Lit des données depuis un ConfigMap\n",
    "- Traite les données en parallèle (3 pods)\n",
    "- Écrit les résultats sur un PersistentVolume\n",
    "- Gère les échecs avec des tentatives de reprise\n",
    "\n",
    "**Q6.2** Implémentez un pipeline de données planifié :\n",
    "- CronJob qui s'exécute toutes les heures\n",
    "- Récupère des données depuis une API externe (simulée)\n",
    "- Traite et stocke les résultats\n",
    "- Envoie une notification à la fin (simulée)\n",
    "\n",
    "**Q6.3** Créez un pipeline ETL avec plusieurs jobs :\n",
    "- Job d'extraction qui récupère les données brutes\n",
    "- Job de transformation qui nettoie et enrichit les données\n",
    "- Job de chargement qui écrit vers la destination finale\n",
    "- Utilisez des initContainers pour assurer le bon séquencement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 7 : Mise à l'échelle automatique horizontale des Pods [★★★]\n",
    "\n",
    "### Bases du HPA\n",
    "\n",
    "Le Horizontal Pod Autoscaler met automatiquement à l'échelle le nombre de pods en fonction de l'utilisation observée du CPU/mémoire ou de métriques personnalisées.\n",
    "\n",
    "```bash\n",
    "# Activer metrics-server (requis pour le HPA)\n",
    "# Pour minikube :\n",
    "minikube addons enable metrics-server\n",
    "\n",
    "# Vérifier que les métriques sont disponibles\n",
    "kubectl top nodes\n",
    "kubectl top pods\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration du HPA\n",
    "\n",
    "**deployment-for-hpa.yaml :**\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: cpu-intensive-app\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: cpu-intensive\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: cpu-intensive\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: app\n",
    "        image: python:3.10-slim\n",
    "        command: [\"/bin/bash\", \"-c\"]\n",
    "        args:\n",
    "        - |\n",
    "          pip install flask && python -c \"\n",
    "          from flask import Flask\n",
    "          import math\n",
    "          app = Flask(__name__)\n",
    "          @app.route('/')\n",
    "          def compute():\n",
    "              x = 0\n",
    "              for i in range(1000000):\n",
    "                  x += math.sqrt(i)\n",
    "              return f'Calcul terminé : {x}'\n",
    "          app.run(host='0.0.0.0', port=5000)\n",
    "          \"\n",
    "        ports:\n",
    "        - containerPort: 5000\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: \"100m\"\n",
    "            memory: \"128Mi\"\n",
    "          limits:\n",
    "            cpu: \"500m\"\n",
    "            memory: \"256Mi\"\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: cpu-intensive-service\n",
    "spec:\n",
    "  type: NodePort\n",
    "  selector:\n",
    "    app: cpu-intensive\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 5000\n",
    "    nodePort: 30600\n",
    "```\n",
    "\n",
    "**hpa.yaml :**\n",
    "\n",
    "```yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: cpu-intensive-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: cpu-intensive-app\n",
    "  minReplicas: 1\n",
    "  maxReplicas: 10\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 50\n",
    "  behavior:\n",
    "    scaleDown:\n",
    "      stabilizationWindowSeconds: 300\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 100\n",
    "        periodSeconds: 15\n",
    "    scaleUp:\n",
    "      stabilizationWindowSeconds: 0\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 100\n",
    "        periodSeconds: 15\n",
    "      - type: Pods\n",
    "        value: 4\n",
    "        periodSeconds: 15\n",
    "      selectPolicy: Max\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Appliquer les configurations\n",
    "kubectl apply -f deployment-for-hpa.yaml\n",
    "kubectl apply -f hpa.yaml\n",
    "\n",
    "# Ou créer le HPA via la ligne de commande\n",
    "kubectl autoscale deployment cpu-intensive-app --cpu-percent=50 --min=1 --max=10\n",
    "\n",
    "# Surveiller l'état du HPA\n",
    "kubectl get hpa -w\n",
    "\n",
    "# Générer de la charge\n",
    "# Dans un autre terminal :\n",
    "kubectl run -it load-generator --rm --image=busybox -- /bin/sh -c \\\n",
    "  \"while true; do wget -q -O- http://cpu-intensive-service; done\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPA basé sur la mémoire\n",
    "\n",
    "```yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: memory-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: memory-intensive-app\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 8\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: 500Mi\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercice 7\n",
    "\n",
    "**Q7.1** Configurez la mise à l'échelle automatique pour un service de traitement de données :\n",
    "- Déployez une application de traitement intensive en CPU\n",
    "- Configurez le HPA avec min=2, max=10 réplicas\n",
    "- Ciblez une utilisation CPU à 60%\n",
    "- Testez avec différents niveaux de charge\n",
    "\n",
    "**Q7.2** Implémentez la mise à l'échelle multi-métriques :\n",
    "- Mise à l'échelle basée sur le CPU et la mémoire\n",
    "- Ajoutez des métriques personnalisées (si vous utilisez Prometheus)\n",
    "- Documentez le comportement de mise à l'échelle sous différentes conditions\n",
    "\n",
    "**Q7.3** Créez une démonstration complète de mise à l'échelle automatique :\n",
    "- Déployez une application avec HPA\n",
    "- Créez un générateur de charge\n",
    "- Visualisez les événements de mise à l'échelle\n",
    "- Mesurez les temps de réponse pendant la mise à l'échelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercice 8 : Déploiement de pipelines de traitement de données [★★★]\n",
    "\n",
    "### Architecture complète du pipeline de données\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    Cluster Kubernetes                           │\n",
    "│                                                                 │\n",
    "│  ┌─────────────┐     ┌─────────────┐     ┌─────────────┐       │\n",
    "│  │  Ingestion  │     │   File de   │     │    Pods     │       │\n",
    "│  │  de données │────►│  messages   │────►│   Worker    │       │\n",
    "│  │  (Deploy)   │     │ (RabbitMQ)  │     │  (Deploy)   │       │\n",
    "│  └─────────────┘     └─────────────┘     └──────┬──────┘       │\n",
    "│                                                  │              │\n",
    "│                                                  ▼              │\n",
    "│                                           ┌─────────────┐       │\n",
    "│                                           │ Base de     │       │\n",
    "│                                           │ données     │       │\n",
    "│                                           │ (PostgreSQL)│       │\n",
    "│                                           └─────────────┘       │\n",
    "│                                                  │              │\n",
    "│                                                  ▼              │\n",
    "│                                           ┌─────────────┐       │\n",
    "│                                           │    API      │       │\n",
    "│                                           │  (Deploy)   │       │\n",
    "│                                           └─────────────┘       │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Déploiement RabbitMQ\n",
    "\n",
    "**rabbitmq.yaml :**\n",
    "\n",
    "```yaml\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: rabbitmq-config\n",
    "data:\n",
    "  RABBITMQ_DEFAULT_USER: \"admin\"\n",
    "  RABBITMQ_DEFAULT_PASS: \"rabbitmq123\"\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: rabbitmq\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: rabbitmq\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: rabbitmq\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: rabbitmq\n",
    "        image: rabbitmq:3-management\n",
    "        ports:\n",
    "        - containerPort: 5672\n",
    "        - containerPort: 15672\n",
    "        envFrom:\n",
    "        - configMapRef:\n",
    "            name: rabbitmq-config\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"256Mi\"\n",
    "            cpu: \"200m\"\n",
    "          limits:\n",
    "            memory: \"512Mi\"\n",
    "            cpu: \"500m\"\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: rabbitmq\n",
    "spec:\n",
    "  selector:\n",
    "    app: rabbitmq\n",
    "  ports:\n",
    "  - name: amqp\n",
    "    port: 5672\n",
    "  - name: management\n",
    "    port: 15672\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Déploiement PostgreSQL\n",
    "\n",
    "**postgres.yaml :**\n",
    "\n",
    "```yaml\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: postgres-secret\n",
    "type: Opaque\n",
    "stringData:\n",
    "  POSTGRES_USER: \"datauser\"\n",
    "  POSTGRES_PASSWORD: \"datapass123\"\n",
    "  POSTGRES_DB: \"dataprocessing\"\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: postgres-pvc\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 5Gi\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: postgres\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: postgres\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: postgres\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: postgres\n",
    "        image: postgres:15\n",
    "        ports:\n",
    "        - containerPort: 5432\n",
    "        envFrom:\n",
    "        - secretRef:\n",
    "            name: postgres-secret\n",
    "        volumeMounts:\n",
    "        - name: postgres-storage\n",
    "          mountPath: /var/lib/postgresql/data\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"256Mi\"\n",
    "            cpu: \"200m\"\n",
    "          limits:\n",
    "            memory: \"512Mi\"\n",
    "            cpu: \"500m\"\n",
    "      volumes:\n",
    "      - name: postgres-storage\n",
    "        persistentVolumeClaim:\n",
    "          claimName: postgres-pvc\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: postgres\n",
    "spec:\n",
    "  selector:\n",
    "    app: postgres\n",
    "  ports:\n",
    "  - port: 5432\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Déploiement des Workers avec HPA\n",
    "\n",
    "**worker.yaml :**\n",
    "\n",
    "```yaml\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: data-worker\n",
    "spec:\n",
    "  replicas: 2\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: data-worker\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: data-worker\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: worker\n",
    "        image: python:3.10-slim\n",
    "        command: [\"/bin/bash\", \"-c\"]\n",
    "        args:\n",
    "        - |\n",
    "          pip install pika psycopg2-binary && python -c \"\n",
    "          import pika\n",
    "          import psycopg2\n",
    "          import time\n",
    "          import os\n",
    "          import json\n",
    "          \n",
    "          # Connexion à RabbitMQ\n",
    "          for i in range(10):\n",
    "              try:\n",
    "                  connection = pika.BlockingConnection(\n",
    "                      pika.ConnectionParameters('rabbitmq', credentials=pika.PlainCredentials('admin', 'rabbitmq123'))\n",
    "                  )\n",
    "                  break\n",
    "              except:\n",
    "                  print('Attente de RabbitMQ...')\n",
    "                  time.sleep(5)\n",
    "          \n",
    "          channel = connection.channel()\n",
    "          channel.queue_declare(queue='data_queue', durable=True)\n",
    "          \n",
    "          def callback(ch, method, props, body):\n",
    "              data = json.loads(body)\n",
    "              print(f'Traitement : {data}')\n",
    "              # Traiter les données et sauvegarder en base de données\n",
    "              time.sleep(1)\n",
    "              ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "          \n",
    "          channel.basic_qos(prefetch_count=1)\n",
    "          channel.basic_consume(queue='data_queue', on_message_callback=callback)\n",
    "          print('Worker démarré, en attente de messages...')\n",
    "          channel.start_consuming()\n",
    "          \"\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"128Mi\"\n",
    "            cpu: \"100m\"\n",
    "          limits:\n",
    "            memory: \"256Mi\"\n",
    "            cpu: \"300m\"\n",
    "---\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: data-worker-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: data-worker\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 10\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 60\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Déploiement du pipeline complet\n",
    "\n",
    "```bash\n",
    "# Créer le namespace\n",
    "kubectl create namespace data-pipeline\n",
    "\n",
    "# Déployer les composants\n",
    "kubectl apply -f rabbitmq.yaml -n data-pipeline\n",
    "kubectl apply -f postgres.yaml -n data-pipeline\n",
    "kubectl apply -f worker.yaml -n data-pipeline\n",
    "\n",
    "# Vérifier les déploiements\n",
    "kubectl get all -n data-pipeline\n",
    "\n",
    "# Afficher les logs\n",
    "kubectl logs -f deployment/data-worker -n data-pipeline\n",
    "\n",
    "# Redirection de port pour le débogage\n",
    "kubectl port-forward svc/rabbitmq 15672:15672 -n data-pipeline\n",
    "# Accéder à l'interface RabbitMQ : http://localhost:15672\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Exercice 8\n",
    "\n",
    "**Q8.1** Déployez un pipeline complet de traitement de données :\n",
    "- RabbitMQ pour la mise en file d'attente des messages\n",
    "- PostgreSQL pour le stockage des données\n",
    "- Service producteur qui génère des données\n",
    "- Service worker avec mise à l'échelle automatique\n",
    "- Service API pour interroger les résultats\n",
    "\n",
    "**Q8.2** Ajoutez la surveillance et l'observabilité :\n",
    "- Déployez Prometheus pour la collecte de métriques\n",
    "- Configurez le scraping pour tous les services\n",
    "- Créez des tableaux de bord Grafana\n",
    "- Configurez des alertes pour les métriques clés\n",
    "\n",
    "**Q8.3** Implémentez un déploiement Spark sur Kubernetes :\n",
    "- Déployez l'opérateur Spark (ou utilisez spark-submit avec Kubernetes)\n",
    "- Soumettez un job Spark pour traiter des données depuis PostgreSQL\n",
    "- Configurez la mise à l'échelle des executors\n",
    "- Surveillez la progression du job et l'utilisation des ressources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Résumé\n",
    "\n",
    "Dans ce TP, vous avez appris :\n",
    "\n",
    "1. **Architecture Kubernetes** : Plan de contrôle, nœuds worker et composants principaux\n",
    "2. **Pods et Deployments** : Création et gestion d'applications conteneurisées\n",
    "3. **Services** : Exposition des applications et découverte de services\n",
    "4. **ConfigMaps et Secrets** : Gestion de la configuration des applications\n",
    "5. **Stockage persistant** : Implémentation de la persistance des données avec PVs et PVCs\n",
    "6. **Jobs et CronJobs** : Exécution de charges de travail par lots et planifiées\n",
    "7. **Mise à l'échelle automatique horizontale des Pods** : Mise à l'échelle automatique basée sur les métriques\n",
    "8. **Pipelines complets** : Déploiement de systèmes de traitement de données prêts pour la production\n",
    "\n",
    "### Points clés à retenir\n",
    "\n",
    "- Utilisez les Deployments pour les applications sans état, les StatefulSets pour celles avec état\n",
    "- Définissez toujours les demandes et limites de ressources\n",
    "- Utilisez les ConfigMaps pour la configuration, les Secrets pour les données sensibles\n",
    "- Implémentez des vérifications de santé (sondes liveness et readiness)\n",
    "- Concevez pour la mise à l'échelle horizontale dès le départ\n",
    "- Utilisez les namespaces pour l'isolation et l'organisation des ressources\n",
    "\n",
    "### Considérations pour la production\n",
    "\n",
    "- **Sécurité** : Utilisez RBAC, les Network Policies, les Pod Security Policies\n",
    "- **Surveillance** : Implémentez une observabilité complète\n",
    "- **Haute disponibilité** : Déployez sur plusieurs zones de disponibilité\n",
    "- **Reprise après sinistre** : Sauvegardes régulières et procédures de récupération testées\n",
    "- **GitOps** : Utilisez des outils comme ArgoCD ou Flux pour les déploiements déclaratifs\n",
    "\n",
    "### Lectures complémentaires\n",
    "\n",
    "- [Documentation Kubernetes](https://kubernetes.io/docs/home/)\n",
    "- [Kubernetes Patterns](https://www.oreilly.com/library/view/kubernetes-patterns/9781492050278/)\n",
    "- [The Kubernetes Book](https://nigelpoulton.com/books/)\n",
    "- [Spark sur Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
